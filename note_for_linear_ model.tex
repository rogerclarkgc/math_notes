\documentclass{article}
\usepackage{amsmath}
\usepackage{ctex}
\usepackage{pgfplots}
\usepackage{enumerate}
\usepackage{amssymb}
\begin{titlepage}
\author{rogerclark}
\date{2017.10.23}
\title{\LARGE Notes for Linear model}
\end{titlepage}
\begin{document}
\maketitle
\numberwithin{equation}{section}
\newtheorem{Definition}{\textsf{Definition}}[subsection]
\newtheorem{Theorem}{\textsf{Theorem}}[subsection]
\section{Basic concept of Linear model}
The basic linear model is define by:
\begin{equation}
y = f(X_1, \ldots, X_K, \beta _1, \ldots, \beta _K) + e
\end{equation}
\par Where $f$ is some well defined function and $\beta_1, \ldots, \beta_K$ are the parameters which characterize the role and contribution of $X_1, \ldots, X_K$\\
The term $e$ reflects the stochastic nature of the relationship between $y$ and $X_1, \ldots, X_K$ and indicates that such a relationship is not exact in nature.When $e=0$, then the model is called the mathematical model otherwise the statistical model.($e$ 是误差项，同随机变量组$X_1, \ldots, X_K$ 一样会影响$y$ 的结果，相当于在研究中没被考虑的变量以及系统本身的内在干扰，在模型中成为了误差项)\\
\\
PAY ATTENTION:The `linear` relationship of linear model is described by the parameter terms which is not influenced by the explanatory variables($X_1, \ldots, X_K$) in the model.Concretely, if all the partial derivatives(偏导数) of $y$ with respect to each of parameters $\beta_1, \ldots, \beta_K$ are independent of the parameters, the eq(1.1) is called as a linear model, if any of the partial derivatives of y with respect to any of the $\beta_1, \ldots, \beta_K$ is not independent of the parameters, the model is called as nonlinear.
\par For example
\begin{displaymath}
  y = \beta_1 X^2_1 + \beta_2 \sqrt{X_2} + \beta_3 \log{X_3} + e
\end{displaymath}
is a linear model, because:
\begin{align*}
  \frac{\partial{y}}{\partial{\beta_1}} &= X_1 ^ 2\\
  \frac{\partial{y}}{\partial{\beta_2}} &= \sqrt{X_2}\\
  \frac{\partial{y}}{\partial{\beta_3}} &= \log{X_3}
\end{align*}
are independent of parameters $\beta_i, (i=1, 2, 3)$.(可以看到，三个偏导数都和$\beta_i$无关)\\
On the other hand,
\begin{displaymath}
  y = \beta_1^2 X_1 + \beta_2 X_2 + \beta_3 \log{X_3} + e
\end{displaymath}
is a nonlinear model, because:
\begin{align*}
  \frac{\partial{y}}{\partial{\beta_1}} &= 2\beta_1X_1\\
  \frac{\partial{y}}{\partial{\beta_2}} &= X_2\\
  \frac{\partial{y}}{\partial{\beta_3}} &= \log{X_3}
\end{align*}
we can see $\frac{\partial{y}}{\partial{\beta_1}} = 2\beta_1 X_1$ depends on $\beta_1$.(对$\beta_1$求偏导，结果与$\beta_1$相关)
\par Basically, the statistical linear modeling essential consists of developing approaches and tools to determine $\beta_1, \ldots, \beta_K$ in the linear model:
\begin{displaymath}
  y = \beta_1 X_1 + \ldots + \beta_K X_K + e
\end{displaymath}
given the observations on $y$ and $X_1, \ldots, X_K$.

\par The regression analysis is a tool to determine the values of the parameters given the data on $y$ and $X_1,\ldots,X_K$. It can be broadly thought that the model exists in nature but is unknown to the experimenter, When some values to the explanatory variables($X1,\ldots,X_K$) are provided, then the values for the output($y$) or study variable are generated accordingly.
\\
\section{The simple Linear Regression Model}
\subsection{The Linear Model}
Consider a linear regression model which has only one independent(or explanatory) variable:
\begin{equation}
  y = \beta_0 + \beta_1 X + e
\end{equation}
Where $y$ is the dependent(or study) variable and $X$ is an independent variable. The $\beta_0$ and $\beta_1$ are the intercept term and slope parameter, respectively, which are usually called as regression coefficients.The unobservable error component $e$ accounts for the failure of data to lie on the straight line and represents the difference between the true and observed realizations of $y$.We assume that $e$ is observed as independent and identically distributed random variable with $\mu = 0$ and $\sigma$ is a constant.
\par The independent variable is viewed as controlled by the experimenter, so it is considered as non-stochastic whereas $y$ is viewed as a random variable with:
\begin{align*}
  &\textrm{E}(y) = \beta_0 + \beta_1 X\\
  &\textrm{var}(y) = \sigma ^2
\end{align*}
Actually, $X$ can also be a random variable, we consider conditional mean of y and conditional variance of y when given $X=x$ as:
\begin{align*}
  &\textrm{E(y$\mid$x)} = \beta_0 + \beta_1 x\\
  &\textrm{var(y$\mid$x)} = \sigma ^2
\end{align*}
 \par Now, Only $T$ pairs of observations $(x_t, y_t)(t=1, \ldots, T)$ on $(X, y)$ are observed which are used to determine the unknown parameters $\beta_0, \beta_1, \sigma^2$
 (已知t对观测数据，我们的任务就是估计$\beta_0, \beta_1, \sigma^2$的值)

\subsection{Least Squares Estimation}
Least Squares Estimation(最小二乘估计) aims estimating $\beta_0$ and $\beta_1$ so that the sum of squares of difference between the observations and the line in the scatter diagrams is minimum.
\par According to different perspectives, there are four methods: \textbf{direct regression method}, \textbf{reverse regression method}, \textbf{major axis regression method}(or orthogonal regression method) and \textbf{reduced major axis method}.Both method 's goal is found the minimum value of sum of squares of difference between observations and the regression line(or regression model).\\
For \textbf{direct regression method}, we consider:
\begin{align*}
  &y_t = \beta_0 + \beta_1 x_t + e_t \qquad(t = 1, \ldots, T) \\
  &\textmd{(is natural model in experiment)}\\
  &Y_t = \hat{\beta_0} + \hat{\beta_1} X_t \qquad(t = 1, \ldots, T) \\
  &\textmd{(is the model we finally estimated)}\\
  &S(\hat{\beta_0}, \hat{\beta_1}) = \sum_{t=1}^{T}e_t^2 = \sum_{t=1}^{T}(y_t - Y_t) = \sum_{t=1}^{T}(y_t - \hat{\beta_0} - \hat{\beta_1}x_t)^2\\
  &\rightarrow minimize(S(\hat{\beta_0}, \hat{\beta_1}))
\end{align*}
\par Among them the direct regression approach is more popular. Generally, the direct regression estimates are referred as the least squares estimates.
\subsection{Direct Regression Method}
\par The direct regression approach minimizes the sum of squares:
\begin{align}
  S(\beta_0, \beta_1) = \sum_{t=1}^T e_t^2 = \sum_{t=1}^T (y_t - \beta_0 - \beta_1 x_1)^2
\end{align}
In order to find $\beta_0$ and $\beta_1$ to minimize the $S$, we can let partial derivatives with respect to $\beta_1$ and $\beta_0$ equal to zero:
\begin{align}
  &\frac{\partial{S(\beta_0 ,\beta_1)}}{\partial{\beta_0}} = -2 \sum_{t=1}^{T} (y_t - \beta_0 - \beta_1 x_t) = 0\\
  &\frac{\partial{S(\beta_0 ,\beta_1)}}{\partial{\beta_1}} = -2 \sum_{t=1}^{T} (y_t - \beta_0 - \beta_1 x_t)x_t = 0
\end{align}
respectively, The solution of eq(2.3) and eq(2.4) are called the direct regression estimators, or usually called as Ordinary Least Squares(OLS) estimators of $\beta_0$ and $\beta_1$.
\par This gives the \textbf{OLS estimates} $b_0$ of $\beta_0$ and $b_1$ of $\beta_1$ as:
\begin{align}
  &b_0 = \bar{y} - b_1 \bar{x}\\
  &b_1 = \frac{SXY}{SXX}
\end{align}
where $SXY = \frac{1}{T}\sum_{t=1}{T}(x_t - \bar{x})(y_t - \bar{y})$, $SXX = \frac{1}{T}\sum_{t=1}{T}(x_t - \bar{x})^2$.
Further, using eq(2.3) and eq(2.4), we can get the Hessian matrix which is the matrix of second order partial derivatives as:
\begin{equation}
\begin{split}
  H &=\begin{pmatrix}
    \frac{\partial ^2 S(\beta_0, \beta_1)}{\partial \beta_0^2} & \frac{\partial ^2 S(\beta_0, \beta_1)}{\partial \beta_0 \partial \beta_1} \\
    \frac{\partial ^2 S(\beta_0, \beta_1)}{\partial \beta_0 \partial \beta_1} & \frac{\partial ^2 S(\beta_0, \beta_1)}{\partial \beta_1^2}
  \end{pmatrix}\\
  &= 2 \begin{pmatrix}
    T & T\bar{x} \\
    T\bar{x} & \sum_{t=1}^{T} x_{t}^{2}
  \end{pmatrix}
\end{split}
\end{equation}
Then, the determinant(行列式) of $H$ is:
\begin{equation}
  |H| = 2T\sum_{t=1}{T}(x_t - \bar{x}) ^ 2 \geq 0
\end{equation}
When $|H|=0$ is not suitable for linear model because then all the observations are identical. It is means that there is no relationship between $x$ and $y$ in the context of regression analysis. Basically, we thought $|H|$ is always bigger than zero. Therefore $H$ is a positive definite matrix for any $(\beta_0, \beta_1)$; that means $S(\beta_0, \beta_1)$ always has a global minimum at $(b_0, b_1)$.
\par The fitted line or the fitted linear regression models is(可以理解成真实的模型)
\begin{equation}
  y = b_0 + b_1 X
\end{equation}
and the predicted values are（这是我们通过回归分析得到的模型）
\begin{equation}
  \hat{y_t} = b_0 + b_1 x_t \quad (t = 1, \ldots, T)
\end{equation}
The difference between the observed value $y_t$ and the predicted value $\hat{y_t}$ is called as \textbf{residual}, The $t^th$ residual is :
\begin{equation}
  \begin{split}
    \hat{\epsilon_t} &= y_t - \hat{y_t} \quad (t=1, \ldots, T)\\
                     &= y_t - (b_0 + b_1 x_t)
  \end{split}
\end{equation}
\subsection{Properties of the direct regression estimators}
\par Using formula (2.5) and (2.6), we have:
\begin{equation}
\begin{split}
  \textbf{E}(b_1) &= \sum_{t=1}^{T} k_t \textbf{E}(y_t) = \sum_{t=1}^{T} k_t (\beta_0+\beta_1 x_t)\\
  &=\beta_1
\end{split}
\end{equation}
Similarly,
\begin{equation}
  \textbf{E}(b_0) = \beta_0
\end{equation}
Thus \textbf{$b_0$ and $b_1$ are unbiased estimators of $\beta_0$ and $\beta_1$}.
\par The variance of $b_1$ is:
\begin{equation}
  \begin{split}
    \textmd{var}(b1) &= \sum_{t=1}^T k_t^2 \textmd{var}(y_t) + \sum_t \sum_{t^* \neq t} k_tk_{t^*} \textmd{cov}(y_t, y_{t^*})\\
            &= \sigma^2 \sum_t^T k_t^2\\
            &= \frac{\sigma^2}{SXX}
  \end{split}
\end{equation}
Similarly, the variance of $b_0$ is:
\begin{equation}
\begin{split}
  \textmd{var}(b_0) &= \textmd{var}(\bar{y}) + \bar{x}^2\textmd{var}(b_1) - 2\bar{x}\textmd{cov}(\bar{y}, b_1)\\
           &= \sigma^2\left(\frac{1}{T} + \frac{\bar{x}^2}{SXX}\right)
\end{split}
\end{equation}
Finally, the covariance between $b_0$ and $b_1$ is:
\begin{equation}
  \begin{split}
    \textmd{cov}(b_0, b_1) &= \textmd{cov}(\bar{y}, b_1) - \bar{x}\textmd{var}(b_1)\\
                  &= -\frac{\sigma^2\bar{x}}{SXX}
  \end{split}
\end{equation}
\par It can further be shown that the ordinary least squares estimators $b_0$ and $b_1$ possess the minimum variance in the class of linear and unbiased estimators.So they are termed as the \textbf{Best Linear Unbiased Estimators(BLUE)}.
(OLS方法得出的$b_0$和$b_1$是具有最小方差的无偏估计量)
\par Further, note that $\sum_{t=1}^{T}x_t\hat{\epsilon_t}=0$, $\sum_{t=1}^{T}\hat{y_t}\hat{\epsilon_t}=0$, $\sum_{t=1}^{T}y_t=\sum_{t=1}^{T}\hat{y_t}$ and the fitted line always passes through $(\bar{x}, \bar{y})$.
\subsection{Testing of Hypotheses and Confidence Interval Estimation}
\par For the simple linear regression model under the assumption that $\epsilon_t$'s are independent and identically distributed with $N(0, \sigma^2)$.
\par We set a null hypothesis related to the slope parameter $\beta_1$:
\begin{displaymath}
  H_0 : \beta_1 = \beta_{10}
\end{displaymath}
where $\beta_{10}$ is some given constant.
\par Assuming $\sigma^2$ to be known, we have:
\begin{equation}
  b_1 \sim N(\beta, \frac{\sigma^2}{SXX})
\end{equation}
So $b_1$ is a random variable which has normal distribution and it's easy to conclude the statistic parameter $Z$ has:
\begin{equation}
  Z_{01} = \frac{b_1 - \beta_{10}}{\sqrt{\frac{\sigma^2}{SXX}}} \sim N(0, 1)
\end{equation}
when null hypothesis $H_0$ is \textbf{TRUE}
(零假设为斜率为已知常数，我们已知$b_1$的分布是正态分布，如果零假设是真实的，那么$b_1$可以归一化成具有标准正态分布统计量$Z$)\\
\\
%distribution for b1
%image start
\begin{tikzpicture}
\begin{axis}[xmin=-4, xmax=22, title=the distribution of $b_1$, small]
\newcommand \MU{4}
\newcommand \SIGMA{1}
\addplot[red, domain=-5:20,samples=100]{exp(-(x-\MU)^2 /2/\SIGMA^2)/(\SIGMA * sqrt(2*pi))};
\draw[help lines](axis cs:\MU,0)--(axis cs:\MU,1);
\end{axis}
\end{tikzpicture}
%image end
%distribution of z01
%image start
\begin{tikzpicture}
\begin{axis}[xmin=-6, xmax=6, title=the distribution of $Z$, small]
\newcommand \MU{0}
\newcommand \SIGMA{1}
\addplot[red, domain=-7:7,samples=100]{exp(-(x-\MU)^2 /2/\SIGMA^2)/(\SIGMA * sqrt(2*pi))};
\draw[help lines](axis cs:\MU,0)--(axis cs:\MU,1);
\end{axis}
\end{tikzpicture}
%image end
\par The $100(1-\alpha)\%$ confidence interval for $\beta_1$ can be obtained from equation(2.18):
\begin{equation}
  b_1 \pm z_{1-\frac{\alpha}{2}}\sqrt{\frac{\sigma^2}{SXX}}
\end{equation}
where $z_{1-\alpha /2}$ is the $(1-\alpha /2)$ percentage point of the $N(0, 1)$ distribution\\
%the image of confidence interval
%image start
\\
\begin{tikzpicture}
\begin{axis}[xmin=-6, xmax=6, title=the $100(1-\alpha)\%$ confidence interval for $\beta_1$]
\newcommand \MU{0}
\newcommand \SIGMA{1}
\newcommand \ALPHA{4.65}
\addplot[red, domain=-7:7,samples=100]{exp(-(x-\MU)^2 /2/\SIGMA^2)/(\SIGMA * sqrt(2*pi))};
\draw[help lines](axis cs:\MU,-0.1)--(axis cs:\MU,1);
\draw[help lines](axis cs:-\ALPHA, -0.1)--(axis cs:-\ALPHA, 0);
\draw[help lines](axis cs:\ALPHA, -0.1)--(axis cs:\ALPHA, 0);
\end{axis}
\end{tikzpicture}
%image end
\\
(通过统计量Z的分布，我们可以得到$\beta_1$的$100(1-\alpha)\%$)置信区间，如上图所示，它等价于两个$\alpha /2$分位数所包围的概率区间所对应的$x$轴区间)
\par Usually, we don't know $\sigma^2$ of $b_1$, then we proceed as follows.We know:
\begin{displaymath}
  E(\frac{RSS}{T-2}) = \sigma^2
\end{displaymath}
(OSS方法中方差的无偏估计量的期望是实际方差)
and we can find:
\begin{displaymath}
  \frac{RSS}{\sigma^2} \sim \chi^2_{T-2}
\end{displaymath}
Further, $RSS/\sigma^2$ and $b_1$ are independently distributed, so the statistic t has:
\begin{equation}
  \begin{split}
    t_{01} &= \frac{b_1 - \beta_{10}}{\sqrt{\frac{\hat{\sigma^2}}{SXX}}}\\
           & = \frac{b1 - \beta_{10}}{\sqrt{\frac{RSS}{(T-2)SXX}}} \sim t_{T-2}
  \end{split}
\end{equation}
when $H_0$ is true\\
($b_1$可以归一化成一个具有t分布的统计量)
\par The corresponding $100(1-\alpha)\%$ confidence interval of $\beta_1$ can be obtained from eq(2.20) as :
\begin{equation}
  b_1 \pm t_{T-2,1-\alpha/2} \sqrt{\frac{RSS}{(T-2)SXX}}
\end{equation}
\par Then, we can use confidence interval to decide the null hypothesis $H_0$ is \textbf{TRUE} or \textbf{FALSE}.
Basically, a decision rule to test $H_0$ can be frame from eq(2.18) or eq(2.20) under the condition when $\sigma^2$ is known or unknown.\\
When $\sigma^2$ is \textbf{unknown}, the decision rule is to \textbf{reject} $H_0$ if :
\begin{displaymath}
  |t_{01}| > t_{T-2, 1-\alpha/2}
\end{displaymath}
then, the decision rule is to \textbf{accept} $H_0$ if:
\begin{displaymath}
 |t_{01}| < t_{T-2, 1-\alpha/2}
\end{displaymath}
where $t_{T-2, 1-\alpha/2}$ is the $1-\alpha/2$ percentage point of the $t$-distribution whith $(T-2)$ degree of freedom.\\
When $sigma^2$ is \textbf{known}, the decision rule is to \textbf{reject} $H_0$ if :
\begin{displaymath}
  |z_{01}| > z_{1-\alpha / 2}
\end{displaymath}
then, the decision rule is to \textbf{accept} $H_0$ if:
\begin{displaymath}
  |z_{01}| < z_{1-\alpha/2}
\end{displaymath}
where $z_{1-\alpha / 2}$ is the $1-\alpha / 2$ percentage point of the normal distribution.\\
(假设检验就是看相关统计量是否落在对应的$(1-\alpha/2)$分位数点区间内，如果在区间外，那么则拒绝该假设下的统计量，反之则接受该假设下的统计量。换成$b_1$就变成了看是否落在我们计算出的置信区间内。
对于方差已知的情况，统计量符合正态分布，方差未知则符合$t$分布)
\par A similar test statistic and test procedure can be developed for testing the hypothesis related to the intercept term $\beta_0$:
\begin{displaymath}
  H_0 : \beta_0 = \beta_{00}
\end{displaymath}
\par When $\sigma^2$ is \textbf{known}, then the $100(1-\alpha)\%$ confidence intervals for $\beta_0$ is:
\begin{equation}
  b_0 \pm z_{1-\alpha / 2}\sqrt{\sigma^2 \left(\frac{1}{T} + \frac{\hat{x}}{SXX}\right)}
\end{equation}
when $\sigma^2$ is \textbf{unknown}, then the $100(1-\alpha)\%$ confidence intervals for $\beta_0$ is:
\begin{equation}
  b_0 \pm t_{T-2, 1-\alpha / 2}\sqrt{\frac{RSS}{T-2}\left(\frac{1}{T} + \frac{\hat{x}}{SXX}\right)}
\end{equation}
\par A confidence interval for $\sigma^2$ can also be derived as follows. Since $RSS/\sigma^2 \sim \chi^2_{T-2}$, thus
\begin{equation}
  P\left[\chi ^2 _{T-2, \alpha/2} \leq \frac{RSS}{\sigma^2} \leq \chi ^2 _{T-2, 1 - \alpha/2}  \right] = 1 - \alpha
\end{equation}
The corresponding $100(1-\alpha)\%$ confidence interval for $\sigma^2$ is:
\begin{equation}
  \frac{RSS}{\chi^2_{T-2, 1-\alpha/2}} \leq \sigma^2 \leq \frac{RSS}{\chi^2_{T-2, \alpha/2}}
\end{equation}
\subsection{Analysis of Variance}
A test statistic for testing $H_0: \beta_1 = 0$ can also be formulated using the analysis of variance technique as follows.
\par on the basis of the identity:
\begin{displaymath}
  y_t - \bar{y_t} = (y_t - \bar{y}) - (\hat{y_t} - \bar{y})
\end{displaymath}
then, the sum of squared residuals is:
\begin{equation}
  \begin{split}
    S(b) &= \sum_{t=1}^T (y_t - \hat{y_t})^2\\
         &= \sum_{t=1}^T (y_t - \bar{y_t})^2 + \sum_{t=1}^T (\hat{y_t} - \bar{y})^2 - 2\sum_{t=1}^T(y_t - \bar{y})(\hat{y_t} - \bar{y})
  \end{split}
\end{equation}
Finally we have:
\begin{equation}
  \sum_{t=1}^T (y_t - \bar{y})^2 = \sum_{t=1}^T (y_t - \hat{y_t})^2 + \sum_{t=1}^T(\hat{y_t} - \bar{y})^2
\end{equation}
The left hand side of eq(2.27) is called the sum of squares about the mean or corrected sum of squares of $y$ or $SYY$.\\
The first term on right hand side is the \textbf{residual sum of squares}, $i.e$: \textbf{RSS}:
\begin{equation}
  SS\ Residual: RSS = \sum_{t=1}^T (y_t - \hat{y_t})^2.
\end{equation}
The second term on right hand side describe the proportion of variability explained by regression:
\begin{equation}
  SS\ Regression: SS_{Reg} = \sum_{t=1}^T (\hat{y_t} - \bar{y})^2
\end{equation}
(式(2.27)表示了SYY的组成，它由残差平方和($RSS$)和回归项与均值之差平方和($SS_{Reg}$))组成)\\
Furthermore, we can define mean square due to regression and mean square due to residual:
\begin{equation}
  MS_{Reg} = \frac{SS_{Reg}}{1}
\end{equation}
\begin{equation}
  MSE = \frac{RSS}{T-2}
\end{equation}
\par The test statistic for testing $H_0:\beta_1 = 0$ is:
\begin{equation}
  F_0 = \frac{MS_{Reg}}{MSE}
\end{equation}
If $H_0 : \beta_1 = 0$ is \textbf{TRUE}, then the $MS_{Reg}$ and $MSE$ are \textbf{independently distributed}, and
\begin{displaymath}
  F_0 \sim F_{1, T-2}.
\end{displaymath}
(当$\beta_1 = 0$成立时，线性模型变为$y = \beta_0$， 此时可以认为回归解释真实值变异性部分$MS_{Reg}$和描述回归结果和真实值之差异的残差平方和即$MSE$之间是独立的关系)\\
The decision rule for $H_1:\beta_1 \neq 0$ is to reject $H_0$ if
\begin{displaymath}
  F_0 > F_{1, T-2;1-\alpha}
\end{displaymath}
at $(1-\alpha)\%$ significance.
The sample correlation coefficient then may be written as:
\begin{equation}
  r_{xy} = \frac{SXY}{\sqrt{SXX}\sqrt{SYYP}}
\end{equation}
\subsection{Goodness of Fit of Regression}
It is very clear that a good fitted model is obtained when residuals are small.So a measure of quality of model can be obtained with $RSS$.
When intercept term is present in the model, a measure of goodness of fit is given by:
\begin{equation}
\begin{split}
  R^2 &= 1 - \frac{RSS}{SYY}\\
      &= \frac{SS_{Reg}}{SYY}
\end{split}
\end{equation}
On the other hand, the ratio $RSS/SYY$ describes the proportion of variability that is not covered by the regression
(可以看到R平方和实际就是回归值解释真实值变异性占据$SYY$的比例)\\
\par It can be seem that:
\begin{displaymath}
  R^2 = r^2_{xy}
\end{displaymath}
\par It may be noted that when $\beta_0 = 0$, the regression line passes through origin, then the $R^2$ can not be used to judge the goodness of fitted model.
In such a case, a possible measure of goodness of fit can be defined as:
\begin{equation}
  R_0^2 = 1 - \frac{\sum_{t=1}^T \hat{\epsilon _t^2}}{\sum_{t=1}^T y_t^2}
\end{equation}
\\
\section{The Multiple Linear Regression Model and its Extensions}
The main topic of this chapter is the linear regression model with more than one independent variables. The principles of \textbf{least squares} and \textbf{maximum likelihood}
are used for the estimation of parameters.
\subsection{The linear Model}
We write:
\begin{equation}
  y = X_1 \beta_1 + \ldots + X_K \beta_K + e
\end{equation}
This is called as the multiple linear regression model. The parameters $\beta_1, \ldots, \beta_K$ are the regression coefficients associated with $X_1, \ldots, X_K$.
\par We have $T$ sets of observations on $y$ and $(X_1, \ldots, X_K)$, which we represents as follows:
\begin{equation}
  (y, X) = \begin{pmatrix}
    y_1 & x_{11} & \cdots & x_{K1}\\
    \vdots & \vdots & \ddots & \vdots\\
    y_T & x_{1T} & \cdots & x_{KT}
  \end{pmatrix}
  = (y, x_{(1)}, \ldots, x_{(K)})
  = \begin{pmatrix}
    y_1, x^{'}_1\\
    \vdots\\
    y_T, x^{'}_T
  \end{pmatrix}
\end{equation}
where $y = (y_1, \ldots, y_T)^{'}$ is a $T$-vector, $x_i = (x_{1i}, \ldots, x_{Ki})^{'}$ is a $K$-vector and $x_{(j)}=(x_{j1}, \ldots, x_{jT})^{'}$ is a $T$-vector.
\par So, there are $T$ observational equations of the form (3.1):
\begin{equation}
\begin{split}
  y_t &= x^{'}_t \beta + e_t\\
      &= \left(x_{1t} \cdots x_{Kt}\right) \begin{pmatrix}
        \beta_1\\
        \vdots\\
        \beta_K
      \end{pmatrix} + e_t \quad t=1,\ldots, T
\end{split}
\end{equation}
which can be written using matrix notation.
\begin{equation}
\begin{split}
  y &= X \beta + e\\
    &= \begin{pmatrix}
      x_{11} & \cdots & x_{K1}\\
      \vdots & \ddots & \vdots\\
      x_{1T} & \cdots & x_{KT}
    \end{pmatrix}
    \begin{pmatrix}
      \beta_1\\
      \vdots\\
      \beta_K
    \end{pmatrix} +
    \begin{pmatrix}
      e_1\\
      \vdots\\
      e_T
    \end{pmatrix}
\end{split}
\end{equation}
where $X$ is $T \times K$ design matrix of $T$ observations on each of the $K$ explanatory variables.
\par We consider the problems of estimation and testing of hypotheses on $\beta$ under some assumptions.
A general procedure for estimation of $\beta$ is minimize:
\begin{equation}
  \sum_{t=1}^T M(e_t) = \sum_{t=1}^T M(y - x_t^{'}\beta)
\end{equation}
for a suitably chosen function $M$, some examples of which are $M(x) = |x|$ and $M(x) = x^2$ and more generally, $M(x) = |x|^p$.\\
($M(x)$可以理解成优化函数，OLS中最常用的就是$M(x)=x^2$)
\subsection{Assumptions in Multiple Linear Regression Model}
We assume that $e$ is observed as a random variable $\epsilon$ with following assumptions:
\begin{enumerate}[(\romannumeral1)]
  \item $E(\epsilon)=0$,
  \item $E(\epsilon \epsilon^{'})=\sigma^2 I_T$,
  \item $\textrm{Rank}(X)=K$,
  \item $X$ is a non-stochastic matrix
  \item $\epsilon \sim N(0, \sigma^2 I_T)$\\
  The following assumption is required to study particularly the large sample properties of the estimators:
  \item $\textmd{lim}_{T\rightarrow \infty}(X_{'}X/T) = \Delta$ exists and is a non-stochastic and non-singular matrix(with finite elements)
\end{enumerate}
\subsection{The Principle of Ordinary Least Squares (OLS)}
Our object is to find a vector $b^{'}=(b_1, \ldots, b_K)$ from $B = \mathbb{R} ^{K}$ that minimizes the sum of squared residuals
\begin{equation}
  S(\beta) = \sum^T_{t=1}e^2_t = e^{'}e = (y-x\beta)^{'}(y-x\beta)
\end{equation}
If we rewrite $S(\beta)$ as
\begin{equation}
  S(\beta)=y^{'}y + \beta^{'}X^{'}X\beta - 2\beta^{'}X^{'}y
\end{equation}
we have:
\begin{align}
  \frac{\partial S(\beta)}{\partial \beta} &= 2 X^{'}X\beta - 2 X^{'}y\\
  \frac{\partial^2 S(\beta)}{\partial \beta^2} &= 2 X^{'}X
\end{align}
Equating the first derivative to zero yields what are called the normal equations
\begin{equation}
  X^{'}Xb = X^{'}y
\end{equation}
If $X$ \textbf{is} of full rank $K$, then $X^{'}X$ is positive definite and the unique solution of (3.10) is
\begin{equation}
  b = (X^{'}X)^{-1}X^{'}y
\end{equation}
If $X$ \textbf{is not} full rank, then equation(3.10) has a set of solutions:
\begin{equation}
  b = (X^{'}X)^{-}X^{'}y + (I - (X^{'}X)^{-}X^{'}X)w
\end{equation}
(类似与一元线性模型，$\beta$此时为一个$K$维向量，如果设计矩阵$X$满秩，则可通过矩阵逆运算解(3.10)直接求出最小值$b$，即$\beta$的无偏估计量。如果矩阵非满秩矩阵，则需要求g-inverse)
\subsection{Best Linear Unbiased Estimation}
\subsubsection{Linear Estimator}
It is common to choose an estimator $\hat{\beta}$ that is \textbf{linear} in y, that is,
\begin{equation}
  \hat{\beta} = Cy + d
\end{equation}
where $C:K\times T$ and $d:K\times 1$ are nonstochastic matrices to be determined by minimizing a suitably chosen risk function.
\par First, some definitions are introduced as we need:\\
\textsf{Definition 1:} \textit{$\hat{\beta}$ is called a homogeneous estimator of $\beta$ if $d=0$; otherwise $\hat{\beta}$ is called heterogeneous}
\par We have measured the model's goodness of fit by the sum of squared errors $S(\beta)$. Analogously we define, for the random variable $\hat{\beta}$,
the \textbf{quadratic loss function}
\begin{equation}
  L(\hat{\beta}, \beta, A) = (\hat{\beta} - \beta)^{'} A (\hat{\beta} - \beta)
\end{equation}
where $A$ is a symmetric and $\geq 0$ $K \times K$-matrix.
\par Now, we can define the \textbf{quadratic risk} of an estimator $\hat{\beta}$ of $\beta$ is:\\
\textsf{Definition 2:} \textit{The quadratic risk of an estimator:}
\begin{equation}
  R(\hat{\beta}, \beta, A) = E(\hat{\beta} - \beta)^{'} A (\hat{\beta} - \beta)
\end{equation}
\par The next step now consists of finding an estimator $\beta$ that minimizes the quadratic risk function. Therefore we have to define a criterion to compare estimators:\\
\textsf{Definition 3:} \textit{An estimator $\hat{\beta_2}$ of $\beta$ is called $R(A)$ superior or an $R(A)$-improvement over another estimator $\hat{\beta_1}$ of $\beta$ if}
\begin{equation}
  R(\hat{\beta_1}, \beta, A) - R(\hat{\beta_2}, \beta, A) \geq 0.
\end{equation}
(不同于OLS方法中$S(\beta)$定义了一个最小残差平方和来求得$\beta$的OLS无偏估计量，这里基于$\hat{\beta}$是$y$的线性组合为基准定理来推出一个二次损失函数来计算$\beta$的线性估计量)
\subsubsection{Mean Dispersion Error}
The quadratic risk is closely related to the matrix-valued criterion of the mean dispersion error(MDE) of an estimator.The MDE is defined as the matrix
\begin{equation}
  M(\hat{\beta}, \beta) = E(\hat{\beta} - \beta)(\hat{\beta} - \beta)^{'}
\end{equation}
The covariance matrix of an estimator $\hat{\beta}$ by $V(\hat{\beta})$:
\begin{equation}
  V(\hat{\beta}) = E(\hat{\beta} - E(\hat{\beta}))(\hat{\beta} - E(\hat{\beta}))^{'}
\end{equation}
If $E(\hat{\beta}) = \beta$ then $\hat{\beta}$ will be called \textbf{unbiased} (for $\beta$). Otherwise, then $\hat{\beta}$ is called \textbf{biased}.
The difference between $E(\hat{\beta})$ and $\beta$ is
\begin{equation}
  \textmd{Bias}(\hat{\beta}, \beta) = E(\hat{p\beta}) - \beta
\end{equation}
\par The following decomposition of the mean dispersion error often proves to be useful:
\begin{equation}
  M(\hat{\beta}, \beta) = V(\hat{\beta}) + (\textmd{Bias}(\hat{\beta}, \beta))(\textmd{Bias}(\hat{\beta}, \beta))^{'}
\end{equation}
We can define a criterion to compare MDE as we define in (3.16):\\
\textsf{Definition 4:} \textit{Let $\hat{\beta_1}$ and $\hat{\beta_2}$ be two estimator of $\beta$. Then $\hat{\beta_2}$ is called MDE-superior to $\hat{\beta_1}$ if the difference of their MDE matrices is nonnegative definite,
that is, if}
\begin{equation}
  \Delta(\hat{\beta_1}, \hat{\beta_2}) = M(\hat{\beta_1}, \beta) - M(\hat{\beta_2}, \beta) \geq 0
\end{equation}
\par One important connection between $R(A)$ superiority and MDE superiority is:
\begin{equation}
  \Delta (\hat{\beta_1}, \hat{\beta_2}) \geq 0 \leftrightarrows R(\hat{\beta_1}, \beta, A) - R(\hat{\beta_2}, \beta, A) = \textmd{tr}\{A\Delta(\hat{\beta_1}, \hat{\beta_2})\} \geq 0
\end{equation}
\subsection{Estimation of the Error Term $\epsilon$ and $\sigma^2$}
We have estimated $X\beta$ by $X\hat{\beta}$, we may consider the residual
\begin{equation}
  \hat{\epsilon} = y - X\hat{\beta} = (I - P_X)y
\end{equation}
where $P_X=X(X^{'}X)^{-}X^{'}$ is the projection operator on $\Re(X)$, as an estimator of $\epsilon$, with the mean prediction error
\begin{equation}
  D(\hat{\epsilon}) = D(y - X\hat{\beta}) = D(I-P_X)y = \sigma^2(I-P_X)(I-P_X)
\end{equation}
Using the estimate $\hat{\epsilon}$ of $\epsilon$ we can obtain an \textbf{unbiased} estimator of $\sigma^2$ as
\begin{equation}
  s^2 = \frac{1}{T-r}y^{'}(I-P_X)y
\end{equation}
with $\textmd{rank}(X) = r$.
\subsection{Classical Regression under Normal Errors}
The classic linear regression model under normal errors is given by
\begin{equation}
\begin{split}
  &y=X\beta + \epsilon\\
  &\epsilon \sim N(0, \sigma^2 I)\\
  &X \quad \textmd{nonstochastic}, \textmd{rank}(X)=K
\end{split}
\end{equation}
\par Now we use maximum likelihood method to estimate parameters in eq(3.25)
\subsubsection{The maximum-Likelihood (ML) Principle}
\begin{Definition}
Let $\xi = (\xi_1, \ldots, \xi_n)^{'}$ be a random variable with density function $f(\xi;\Theta)$, where the parameter vector $\Theta = (\Theta_1, \ldots, \Theta_m)^{'}$ is an element of the parameter space $\Omega$ comprising
all values that are a priori admissible
\end{Definition}
The basic idea of ML principle is to consider the density $f(\xi;\Theta)$ for a specific realization of the sample $\xi_0$ of $\xi$ as a function of $\Theta$:
\begin{displaymath}
  L(\Theta) = L(\Theta_1, \ldots, \Theta_m) = f(\xi_0;\Theta)
\end{displaymath}
$L(\Theta)$ will be referred to as the \textbf{likelihood function of $\Theta$ given $\xi_0$}
\par We want to find a value $\hat{\Theta}\in\Omega$ that maximizes the $L(\Theta)$:
\begin{displaymath}
  L(\hat{\Theta}) \geq L(\Theta) \quad for \ all \ \Theta \in \Omega
\end{displaymath}
(极大似然法就是首先建立要估计量的似然函数$f(\xi;\Theta)$，这个函数的自变量是我们估计量$\Theta$，参数是观测变量$\xi$，目的就是找到在某个（或某些）$\xi$下，使得似然函数最大化的$\hat{\Theta}$)
\subsubsection{Maximum Likelihood Estimation in Classical Normal Regression}
We have y follow the normal distribution from eq(3.26):
\begin{equation}
  y=X\beta + \epsilon \sim N(X\beta, \sigma^2 I)
\end{equation}
so the likelihood function of $y$ is given by
\begin{equation}
  \begin{split}
    L(\beta, \sigma^2)=(2\pi \sigma^2)^{-T/2} e ^{-\frac{1}{2\sigma^2} (y-X\beta)^{'} (y-X\beta)}
  \end{split}
\end{equation}
(这里的似然函数中的$X$是设计矩阵，已经包括了所有T个观测，所以是$T\times K$维的)
It is appropriate to maximize $\ln L(\beta, \sigma^2)$ instead of $L(\beta, \sigma^2)$, as the maximizing argument remains unchanged:
\begin{equation}
  \ln L(\beta, \sigma^2) = - \frac{T}{2} \ln(2\pi \sigma^2) - \frac{1}{2\sigma^2}(y-X\beta)^{'}(y-X\beta)
\end{equation}
the parameter space is given by $\Omega = \{\beta;\sigma^2 : \beta \in \mathbb{R}^K; \sigma^2 > 0 \}$. We can solve the ML estimators of $\beta$ and $\sigma^2$ from:\\
\begin{equation}
  \begin{split}
    \frac{\partial \ln L}{\partial \beta} &= \frac{1}{2 \sigma^2} 2X^{'}(y-X\beta)=0\\
    \frac{\partial \ln L}{\partial \sigma^2} &= \frac{T}{2\sigma^2} + \frac{1}{2(\sigma^2)^2}(y-X\beta)^{'}(y-X\beta)=0.
  \end{split}
\end{equation}
Then, the \textbf{likelihood equations} are given by:
\begin{equation}
  \begin{split}
    & X^{'}X\hat{\beta} = X^{'}y\\
    & \hat{\sigma^2}=\frac{1}{T}(y-X\hat{\beta})^{'}(y-X\hat{\beta})
  \end{split}
\end{equation}
It is obviously that when $\textmd{Rank}(X)=K$ we get the unique ML estimator of $\beta$
\begin{equation}
  \hat{\beta} = b = (X^{'}X)^{-1}X^{'}y
\end{equation}
the ML estimator of $\sigma^2$ is \textbf{biased}:
\begin{equation}
  \hat{\sigma^2} = \frac{T-K}{T}s^2
\end{equation}
The asymptotic expectation is given by:
\begin{equation}
  \begin{split}
    \lim_{T\rightarrow\inf} \textmd{E}(\hat{\sigma^2}) &= \bar{\textmd{E}}(\hat{\sigma^2})\\
                                                       &= \textmd{E}(s^2)\\
                                                       &= \sigma^2
  \end{split}
\end{equation}
\par Thus, we can state the following result
\begin{Theorem}
  The maximum-likelihood estimator and OLS estimator of $\beta$ are identical in the classical normal regression model.The estimator $\hat{\sigma^2}$ of $\sigma^2$ is asymptotically unbiased.
\end{Theorem}
(可以看到极大似然估计量对斜率参数$\beta$的估计和OLS方法得出的是等价的无偏估计量，但对方差的估计量则是一个有偏估计量，但是当观测足够多时，趋近于OLS方法得出的无偏估计量)
\end{document}
