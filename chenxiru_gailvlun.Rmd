---
title: "陈希孺概率论与数理统计读书笔记"
author:
  - RogerClark
documentclass: ctexart
output:
  rticles::ctex:
    fig_caption: yes
    number_sections: yes
    toc: yes
classoption: "hyperref,"
---
```{r eval=T, echo=F,warning=F,message=F}
library(tidyverse)
library(latex2exp)
library(grid)
vplayout <- function(x, y){
  viewport(layout.pos.row=x, layout.pos.col = y)
}
```
\newtheorem{Definition}{\hspace{2em}定义}
\newtheorem{theorem}{\hspace{2em}定理}
\newtheorem{lemma}{\hspace{2em}例子}
\newtheorem{Proof}{证明}
\section{事件的概率}

（暂空）

\section{随机变量及概率分布}

（暂空）

\section{随机变量的数字特征}

（暂空）

\section{参数估计}

\subsection{基本概念}

\subsubsection{总体、样本、统计量}

*总体*：所研究问题有关对象的全体构成的集合

\begin{lemma}
\begin{enumerate}
\item 研究试验田中水稻的根系长度，那么试验田中所有水稻根系长度构成问题总体，其中一株水稻的根系长度为其个体
\item 某厂生产的一批螺母的质量，那么这一批螺母的质量构成总体，每个螺母的质量则为个体
\end{enumerate}
\end{lemma}

总体可以理解为研究对象所有可能取值组成的一个空间，这个空间符合某种概率分布，那么总体就叫符合某种分布的总体。如前面
提到的水稻根系长度，一般符合正态分布，那么这个总体属于正态分布的总体

*样本*：按照一定规定从总体中抽出的一部分个体，这个规定指得是每个个体都有同等被抽出的机会。

*统计量*：完全由样本决定的量叫做统计量，统计量的只依赖样本，不能依赖其他任何未知的量。

\subsubsection{样本矩}

一类重要的统计量称为样本矩，它分为*样本原点矩*和*样本中心矩*，定义如下

\begin{Definition}
样本原点矩：设$X_1,\cdots,X_n$位样本，$k$为正整数，则
\begin{equation}
a_k = \frac{(X_1^k + \cdots + X_n^k)}{n}
\end{equation}
称为$k$阶原点矩.
\end{Definition}
一阶原点矩，也就是$a_1 = \bar{X}$就是常见的样本均值。

\begin{Definition}
样本中心矩:设$X_1,\cdots,X_n$位样本，$k$为正整数，则
\begin{equation}
m_k = \frac{\sum_{i=1}^n(X_i - \bar{X})}{n}
\end{equation}
称为$k$阶中心矩阵
\end{Definition}
值得注意的是样本二阶中心矩为$m2 = \sum_{i=1}^n(X_i - \bar{X})^2/n$，样本的方差为$S^2 = \sum_{i=1}^n(X_i - \bar{X})^2/(n-1)$，
它们之间相差一个常数$m_2 = \frac{n-1}{n}S^2$
\subsection{矩估计、极大似然估计和贝叶斯估计}

\subsubsection{参数点估计问题}
当已知总体是某种分布，但不知道其概率密度函数的参数时，这时需要根据从总体中抽出的独立随机样本来估计总体分布的某个或几个参数。即有概率
密度函数$g(\theta_1, \cdots, \theta_k)$，其中$\theta_1$是未知的，我们便构造一个统计量$\hat{\theta_1}=\hat{\theta_1}(X_1,\cdots,X_n)$，
当有样本$X_1, \cdots, X_n$就带入$\hat{\theta_1}(X_1, \cdots, X_n)$得到$theta_1$的估计值。这样用一个点$\hat{\theta_1}$来估计未知参数$\theta_1$
的行为称为点估计。

\subsubsection{矩估计法}
设总体分布为$f(x;\theta_1,\cdots,\theta_k)$，则它的矩（此处以原点矩为例）为：
\begin{displaymath}
a_m = \int_{-\infty}^{\infty}x^mf(x;\theta_1,\cdots,\theta_k)
\end{displaymath}
对离散型的分布则有：
\begin{displaymath}
a_m = \sum_i x_i^m f(x_i;\theta_1,\cdots,\theta_k)
\end{displaymath}
当在样本大小$n$较大时，它接近样本的原点矩，于是有
\begin{displaymath}
a_m = a_m(\theta_1,\cdots,\theta_k) \simeq a_m = \sum_{i=1}^{n}X_i^m/n
\end{displaymath}
这样，式子左边是包含$\theta_1,\cdots, \theta_k$的表达式，右边是一个实数的样本矩，于是让$m=1,\cdots,k$，得到一系列方程组：
\begin{equation}
a_m(\theta_1,\cdots, \theta_k) = a_m \quad (m=1, \cdots, k)
\end{equation}
解这个方程组，其根$\hat{\theta_i}=\hat{\theta_i}(X_1,\cdots,X_n)$就是$\theta_i$的估计。

\subsubsection{极大似然估计法}
设总体分布为$f(x;\theta_1,\cdots,\theta_k)$，$X_1,\cdots,X_k$为取自总体的样本（各个样本之间独立同分布），则样本的分布为：
\begin{displaymath}
f(x_1;\theta_1,\cdots,\theta_k)f(x_2;\theta_1,\cdots,\theta_k)\cdots f(x_n;\theta_1,\cdots,\theta_k)
\end{displaymath}
把这分布记为$L(x_1,\cdots,x_n;\theta_1,\cdots,\theta_k)$，极大似然法就是把$L(x_1,\cdots,x_n;\theta_1,\cdots,\theta_k)$中的样本
$x_1,\cdots,x_n$固定，把$L$看成$\theta_1,\cdots,\theta_k$的函数，这样只要找到一组$\theta_1^*,\cdots,\theta_k^*$使得$L$取最大，
这组值就可以作为$\theta_1,\cdots,\theta_k$的估计了。即满足如下：
\begin{equation}
L(X_1, \cdots,X_n;\theta_1^{*},\cdots,\theta_k^{*}) = \max \limits_{\theta_1,\cdots,\theta_k} L(X_1,\cdots,X_n;\theta_1,\cdots,\theta_k)
\end{equation}
的$(\theta_1^*,\cdots,\theta_k^*)$作为$(\theta_1,\cdots,\theta_k)$的估计值。
根据$L$的定义容易得到:
\begin{equation}
\begin{split}
\ln L &= \ln f(x_1;\theta_1,\cdots,\theta_k)f(x_2;\theta_1,\cdots,\theta_k)\cdots f(x_n;\theta_1,\cdots,\theta_k)\\
& = \sum_{i=1}^n \ln f(X_i;\theta_1,\cdots, \theta_k)
\end{split}
\end{equation}
要让$L$在$(\theta_1, \cdots,\theta_k)$上有最大，只需要让$L$在$\theta_k$上的一阶偏导为0，这样就有：
\begin{equation}
\frac{\partial \ln L}{\partial \theta_i} = 0 \quad (i = 1, \cdots, k)
\end{equation}
由此得到$k$个关于$\theta_k$的方程组，如果该方程组有唯一解且能证明这个解是极大值点，则可得出极大似然估计。

值得注意的是，极大似然估计法和矩估计法在一些特殊的情况下导出的对总体的估计量是等价的，例如，当总体是正态分布、指数分布时。当然更多的情况是两者并不相同。

\subsubsection{贝叶斯法}
贝叶斯法的核心思想是“先验分布”的设定和“后验分布”的计算，假定要估计的一个参数是$\theta$，在抽样或实验之前，我们可以根据以前的经验或仅仅是直观感受把这个参数的分布定为一个$h(\theta)$，这个分布就称为*先验概率密度分布*。

有了先验分布，我们来一步步推导$\theta$的*后验概率密度分布*，首先我们开始抽样，得到一系列样本$X_1,\cdots,X_n$，设总体的分布是$f(X,\theta)$，那么，这个样本的概率密度函数为：
\begin{equation}
\label{sample_condition}
f(X_1,\theta)\cdots f(X_n, \theta)
\end{equation}
这个概率密度相当于在给定了$\theta$下样本$(X_1,\cdots,X_n)$的条件概率密度。根据联合概率密度的公式：
\begin{equation}
f(X_1,X_2) = f_1(X_1)f(X_2|X_1)
\end{equation}
这样我们写出$(\theta, X_1,\cdots,X_n)$的联合概率密度，注意，这里应该把系列样本$(X_1,\cdots,X_n)$作为一个整体看，这样\eqref{sample_condition}就相当于是样本在分布参数为$\theta$时的条件概率密度了。这样就有:
\begin{equation}
f(\theta, X_1,\cdots,X_n) = h(\theta)f(X_1,\theta)\cdots f(X_n, \theta)
\end{equation}
有了联合密度函数$f(\theta,X_1,\cdots,X_n)$，对它在$\theta$方向积分，得到了样本$(X_1,\cdots,X_n)$的一个所谓“普遍性”的边缘密度分布，如下：
\begin{equation}
\label{sample_border}
p(X_1,\cdots,X_n) = \int h(\theta) f(X_1,\theta)\cdots f(X_n, \theta) \textrm{d} \theta
\end{equation}
由此，根据条件概率公式，很容易写出$\theta$在抽样后的后验概率密度函数$h(\theta|X_1,\cdots,X_n)$为：
\begin{equation}
\label{theta_post}
\begin{split}
h(\theta |X_1,\cdots,X_n) &= \frac{f(\theta, X_1,\cdots,X_n)}{p(X_1,\cdots,X_n)}\\
&= \frac{h(\theta)f(X_1,\theta)\cdots f(X_n, \theta)}{\int h(\theta) f(X_1,\theta)\cdots f(X_n, \theta) \textrm{d} \theta}
\end{split}
\end{equation}
由此，我们按照贝叶斯法得出了$\theta$的后验概率分布，可以看到这个后验分布其实是$\theta$在样本$X_1,\cdots,X_n$的条件概率分布，它综合了先验分布$h(\theta)$和样本带来的信息$p(X_1,\cdots,X_n)$。
\subsection{点估计的优良性准则}
\subsubsection{估计量的无偏性}
下面给出一个*无偏估计量*的定义：
\begin{Definition}
设总体的分布有未知参数$\theta_1,\cdots,\theta_k$，$X_1,\cdots,X_n$是从总体抽出的样本，要估计的分布形式为$g(\theta_1,\cdots,\theta_k)$，$g$的形式是已知的。若$\hat{g}(X_1,\cdots,X_n)$是一个估计量。则对\uwave{任何}可能的$\theta_1,\cdots,\theta_k$，都有：
\begin{equation}
\label{unbias_definition}
E_{\theta_1,\cdots,\theta_k}[\hat{g}(X_1,\cdots,X_n)] = g(\theta_1,\cdots,\theta_k)
\end{equation}
则把满足\eqref{unbias_definition}的$\hat{g}$称为$g(\theta_1,\cdots,\theta_k)$的一个无偏估计量。
\end{Definition}

无偏估计量是一种优良的性质，其原因有二：

1. 说明估计量没有系统性误差，虽然$\hat{g}$的值可能是在真实$g$上下波动的，但在概率上平均，这种上下的波动并没有偏向上或者下，也就是说在偏差能相互抵消，说明估计量本身没有系统性的误差。下面以一个例子来说明

```{r unbias_example,eval=T, echo=F, warning=F, error=F, fig.height=3, fig.width=4}
n <- 1000
mu1 <- 0
mu2 <- 1
unbias <- rnorm(n, mu1)
bias <- rnorm(n, mu2)
ob <- c(1:n)
fig_unbias <- ggplot(data.frame(ob, unbias)) + geom_point(aes(ob, unbias), alpha=1/3) + geom_hline(yintercept = mu1, col="green", size=1.1) + ylab(TeX("$\\hat{g}_1$")) + xlab("observations")
fig_bias <- ggplot(data.frame(ob, bias)) + geom_point(aes(ob, bias),alpha=1/3) + geom_hline(yintercept = mu1, col="green", size=1.1) + ylab(TeX("$\\hat{g}_2$")) + xlab("observations") + geom_hline(yintercept = 1, col="red", size=1.1)

grid.newpage()
pushViewport(viewport(layout=grid.layout(1, 2)))
print(fig_unbias, vp=vplayout(1, 1))
print(fig_bias, vp=vplayout(1, 2))
```

可以看到，在上图的两个估计量$\hat{g_1}$和$\hat{g_2}$的1000次对正态总体$N(0, 1)$的均值估计中，$\hat{g_1}$的估计值虽然分布在0的上下两端，但可以看到分布基本呈一个对称态势，位于0上侧的和下侧的差不多，且越靠近0的点，它的数量就越多，这说明$\hat{g_1}$对总体的估计从概率上来说不总是偏高也不总是偏低，相反，估计量大于0的可能和小于0的可能是一样的，其期望值就是真实的0。

而$\hat{g_2}$就不同它的估计值大多数都大于0， 显然是偏高的，并且在1附近聚集的很明显，这就表明用这个估计量来估计总体$N(0, 1)$的均值，有很多时候会得到一个偏高的结果，这样它的期望就不会等于0，而是大于0，所以它不是一个无偏的估计量

2. 结合大数定理，若把$\hat{g}(X_1, \cdots,X_n)$在很多样本上使用，例如在样本$(X_1^{(1)},\cdots,X_n^{(1)})$上的估计值记为$\hat{g}(X_1^{(1)},\cdots,X_n^{(1)})$，就这样在N个样本使用，得到这样一组估计值$\hat{g}(X_1^{(1)},\cdots,X_n^{(1)}),\cdots,\hat{g}(X_1^{(N)},\cdots,X_n^{(N)})$，则根据大数定理，当$N \rightarrow \infty$时有：
\begin{displaymath}
\frac{\sum_{i=1}^N \hat{g}(X_1^{(i)},\cdots,X_n^{(i)})}{N} \rightarrow g(\theta_1, \cdots,\theta_k)
\end{displaymath}

即表明估计量的均值会依概率收敛到真实值$g(\theta_1,\cdots,\theta_k)$。而对无偏估计量而言，这个收敛到真实值的概率会是100%。反之，如果一个估计量不是无偏的，无论使用多少次，其均值也会和真实值保持一定距离，说明这个估计量存在一定的系统偏差。下面以一个例子来说明。

前面提到，样本的二阶中心矩$m_2 = \sum_{i=1}^n(X_i-\bar{X})/n$不是对总体分布方差$\sigma^2$无偏估计量，而样本的方差$S^2 = \sum_{i=1}^n(X_i-\bar{X})/(n-1)$则是对总体分布方差$\sigma^2$的无偏估计量。下面通过一个实验来表明$S^2$和$m_2$在大数定理下的收敛情况。

```{r unbias_caculate2,eval=T, echo=F, warning=F, error=F, fig.height=3, fig.width=4}
set.seed(1234)
true_sd <- 2
simsteps <- 100000
sample_cap <- 10
sample_matrix <- matrix(rnorm(simsteps*sample_cap, sd=true_sd), ncol=sample_cap)
sample_sd <- apply(sample_matrix, 1, var)
sample_m2 <- sample_sd * (sample_cap-1) / sample_cap

meanstep <- seq(100, simsteps, 100)
sd_mean <- vector(mode="numeric", length=length(meanstep))
m2_mean <- sd_mean
for (i in c(1:length(meanstep))){
  index <- meanstep[i]
  sd_mean[i] <- mean(sample_sd[1:index])
  m2_mean[i] <- mean(sample_m2[1:index])
}
```

```{r unbias_fig2,eval=T, echo=F, warning=F, error=F, fig.height=1.5, fig.width=4}
fig_sd <- ggplot(data=data.frame(meanstep, sd_mean)) + geom_line(aes(meanstep, sd_mean)) + geom_hline(yintercept = true_sd^2, col="green") + xlab(TeX("$N$")) + ylab(TeX("$\\bar{S^2}$")) + scale_x_continuous(labels=c("0", "", "50000", "", "100000"))
fig_m2 <- ggplot(data=data.frame(meanstep, m2_mean)) + geom_line(aes(meanstep, m2_mean)) + geom_hline(yintercept = true_sd^2, col="green") + xlab(TeX("$N$")) + ylab(TeX("$\\bar{m_2^2}$")) + geom_hline(yintercept = true_sd^2 * (sample_cap-1) / sample_cap, col="red")+ scale_x_continuous(labels=c("0", "", "50000", "", "100000"))

grid.newpage()
pushViewport(viewport(layout = grid.layout(1, 2)))
print(fig_sd, vp=vplayout(1, 1))
print(fig_m2, vp=vplayout(1, 2))
```

对正态总体$N(0,4)$进行抽样，样本大小为`r sample_cap`，重复抽样次数1-100000次，每进行$N$次抽样，就计算出$S^2$和$m_2$在这$N$次抽样下的均值，最后就得到了抽样次数在1-100000下的两个统计量的均值。以均值和抽样次数作折线图，得到上图

可以发现，随着抽样实验的次数的增多，$\bar{S^2}$的值越来越接近真实的值$4$，而$\bar{m_2}$明显收敛到$3.6$，这说明如果用$m_2$估计总体方差$\sigma^2$只会偏小。

\subsubsection{最小方差无偏估计}
除了用无偏性来评价估计量外，也使用*最小方差*这一概念来评价一估计量，首先给出*均方误差*的定义：
\begin{Definition}
设$X_1,\cdots,X_n$是从某一参数$\theta$的总体中抽出的样本，若有估计量$\hat{\theta}=\hat{\theta}(X_1,\cdots,X_n)$，则这个估计量与真实值$\theta$的误差为$\hat{\theta}(X_1,\cdots,X_n)-\theta$，显然误差本身也是由样本决定的，是一个随机变量，对其平方并取均值，则有：
\begin{equation}
M_{\hat{\theta}}(\theta) = E_{\theta}[\hat{\theta}(X_1,\cdots,X_n)-\theta]^2
\end{equation}
上式子中的$M_{\hat{\theta}}(\theta)$即称为估计量$\hat{\theta}$的均方误差
\end{Definition}

显然，均方误差越小的估计量越优秀，根据方差的一个定理：

\begin{equation}
Var(X) = E(X - EX)^2 = E(X^2) - EX ^2
\end{equation}
可将$M_{\hat{\theta}}(\theta)$改写：
\begin{equation}
\begin{split}
M_{\hat{\theta}}(\theta) &= E_{\theta}(\hat{\theta} - \theta)^2\\
&= E(\hat{\theta}^2) - [E(\hat{\theta})]^2 + [E(\hat{\theta})]^2 + \theta^2 - 2E(\hat{\theta})\theta\\
&= Var_{\theta}(\hat{\theta}) + [E_{\theta}(\hat{\theta}) - \theta] ^2
\end{split}
\end{equation}
这就表明均方误差由$\textrm{Var}_{\theta}(\hat{\theta})$，即估计量$\hat{\theta}$自身方差，另外一部分$[E_{\theta}(\hat{\theta}) - \theta]^2$就是前面描述无偏性的系统误差。
显然，如果$\hat{\theta}$是一个无偏估计量，则描述系统误差的项为0，那么就有：
\begin{equation}
M_{\hat{\theta}}(\theta) = \textrm{Var}_{\theta}(\hat{\theta})
\end{equation}
这个时候均方误差就只是由估计量$\hat{\theta}$的方差来决定。那么，对无偏估计量来说，均方误差的比较久归结为估计量自身的方差的比较，方差较小者较优。这样就能给出*最小方差无偏估计*的定义：
\begin{Definition}
设有$\hat{\theta}$是分布$g(\theta)$的一个无偏估计，若对$g(\theta)$的任何一个无偏估计$\hat{\theta_1}$，$\hat{\theta}$都满足：
\begin{equation}
\textrm{Var}_{\theta}(\hat{\theta}) \leq \textrm{Var}_{\theta}(\hat{\theta_1})
\end{equation}
对$\theta$的任何取值都成立，就把$\hat{\theta}$称为$g(\theta)$的一个最小方差无偏估计（MVU估计）
\end{Definition}
如何寻找MVU估计不在笔记讨论范围，详情见陈希孺《概率论与数理统计》中关于克拉美-劳不等式的讨论。

\subsubsection{估计量的相合性与正态性}
前面讨论的估计量的无偏性是针对有限的样本来引申出的一种性质，例如前面的例子中用的样本数量都是10。设想当样本数量接近无穷时，估计量的两种新的性质就被引申出来，即估计量的*相合性*和*正态性*

1. 相合性

估计量的相合性是对大数定理的一种引申，大数定理是指对$X_1,X_2,\cdots, X_n,\cdots$独立同分布，都属于均值为$\theta$的总体，记$\bar{X_n} = \sum\limits_{i=1}^n X_i/n$，当$n \rightarrow \infty$时，对任一给定$\varepsilon > 0$都有：

\begin{displaymath}
\lim_{n \rightarrow \infty} P(|\bar{X_n} - \theta| \geq \varepsilon) =0
\end{displaymath}

这个定理表明，随着样本容量的增大，估计值$\bar{X_n}$与真实值$\theta$的差距达到某个$\varepsilon$的概率越来越小，以至于趋近于0。换句话说，样本足够大，均值的误差就可以任意小。

把这个定理引申，就得到相合性的定义：
\begin{Definition}
设总体分布参数为$\theta_1,\cdots,\theta_k$，$g(\theta_1,\cdots,\theta_k)$是$\theta_1,\cdots,\theta_k$d的一个函数，若$X_1,\cdots,X_n$是从该总体中抽出的样本，$T(X_1,\cdots, X_n)$是$g(\theta_1,\cdots, \theta_k)$的估计量，对任一$\varepsilon > 0$，有：
\begin{equation}
\lim_{n \rightarrow \infty}P_{\theta_1, \cdots, \theta_k}(|T(X_1,\cdots,X_n) - g(\theta_1,\cdots,\theta_k)| \geq \varepsilon) =0
\end{equation}
对一切$(\theta_1,\cdots,\theta_k)$成立，则称$T(X_1,\cdots, X_n$是$g(\theta_1, \cdots, \theta_k)$的一个相合估计
\end{Definition}
一个具有相合性的估计量表明，只有样本取的足够大，那么估计量与真实值的误差就会越来越小，这是一个很基本性质。下面以一个例子直观展现样本均值$\bar{X}$的相合性

```{r bigsample_cal,eval=T, echo=F, warning=F, error=F, fig.height=1.5, fig.width=4}
set.seed(1234)
step_mean <- seq(1, 1000)
mean_ex <- sapply(step_mean, function(x){mean(rnorm(x))})
fig_meanex <- ggplot(data.frame(step_mean, mean_ex)) + geom_line(aes(step_mean, mean_ex)) + geom_hline(yintercept = 0, col="green") + xlab(TeX("$n$")) + ylab(TeX("$\\bar{X_n}$"))
fig_meanex
```

上面的例子是抽自正态分布$N(0,1)$的样本在样本大小从1-1000的情况，可以看到随着样本容量的变大，样本均值$\bar{X_n}$越来越接近真实值0。这就是所谓的相合性

2. 渐进正态性

估计量$T$是样本的一个函数，自身往往也满足一种分布，但这种分布往往不容易直接求出，类似中心极限定理，当样本容量$n$很大时，可以证明，许多统计量都会趋近于正态分布，这就是所谓的*渐进正态性*。

\subsection{区间估计}
\subsubsection{基本概念}
点估计是使用一个数对总体的参数进行估计，区间估计则是通过样本得到一段区间$[\hat{\theta_1},\hat{\theta_2}]$，让未知参数$\theta$，有很大可能落入这个区间，与此同时也要让区间的长度$\theta_2 - \theta_1$越小越好。显然同时满足以上这两点是不可能的。区间长度长了，$\theta$落入区间的可能性高了，但估计的精度就下去了，反之短的区间虽然有高的精度，但$\theta$落入区间的概率也降低，估计的可靠性不足。由此我们给出一个权衡的定义。
\begin{Definition}
\label{conflevel_def}
给定一很小的数$\alpha > 0$，如果参数的落入估计区间的概率满足：
\begin{equation}
\label{conflevel}
P_{\theta}(\hat{\theta_1}(X_1,\cdots,X_n) \leq \theta \leq \hat{\theta_2}(X_1,\cdots,X_n)) = 1 - \alpha 
\end{equation}
则称区间估计$[\hat{\theta_1}, \hat{\theta_1}]$的置信系数为$1 - \alpha$
\end{Definition}
置信系数$1 - \alpha$量化了区间估计的可靠程度，指该区间有多大概率包含未知参数$\theta$。但在更一般的情况下，无法保证$P_{\theta}(\hat{\theta_1} \leq \theta \leq \hat{\theta_2})$对一切的$\theta$都满足式\eqref{conflevel}，但可保证这个概率不小于$1 - \alpha$，即有：
\begin{equation}
P_{\theta}(\hat{\theta_1}(X_1,\cdots,X_n) \leq \theta \leq \hat{\theta_2}(X_1,\cdots,X_n)) \leq 1 - \alpha 
\end{equation}
此时称$1 - \alpha$为区间$[\hat{\theta_1}, \hat{\theta_2}]$的*置信水平*，一般情况下$\alpha = 0.05$是使用的比较多的情况，也就是说，一般会计算置信水平为$0.95$的区间估计。
\subsubsection{枢轴变量法}
设$X_1,\cdots,X_n$是抽自正态总体$N(\mu, \sigma^2)$的样本，要通过这个样本来求$\mu$的区间估计，分两种情况：

*1. 总体方差$\sigma^2$已知*

显然，样本的均值$\bar{X}$是一个对总体均值的良好估计，接着构造一个新的随机变量，它显然满足：
\begin{equation}
\label{quzhou}
\frac{\sqrt{n}(\bar{X} - \mu)}{\sigma} \sim N(0, 1)
\end{equation}
为了求出$\mu$的区间估计，先引入*分位点*的概念。
\begin{Definition}
以$\Phi(x)$记$N(0,1)$的分布函数，对一个较小的$0 < \beta < 1$，若$u_\beta$满足方程：
\begin{equation}
\Phi(u_\beta) = 1-\beta
\end{equation}
则称$u_{\beta}$是分布$N(0, 1)$的上$\beta$分位点。
\end{Definition}

```{r uptail,eval=T, echo=F, warning=F, error=F, fig.height=1.5, fig.width=4}
x_norm <- seq(-3, 3, 0.1)
y_norm <- dnorm(x_norm)
u_0.95 <- qnorm(0.95)
u_0.05 <- qnorm(0.05)

fig_norm <- ggplot(data.frame(x_norm, y_norm)) + geom_line(aes(x_norm, y_norm),col="blue", size=1) + geom_vline(xintercept = u_0.95) + geom_vline(xintercept = u_0.05) + scale_x_continuous(labels = c("", TeX("$u_{0.05}$"),"", "0", "", TeX("$u_{0.95}$"), ""), breaks = c(-3, -2, -1, 0, 1, 2, 3)) + xlab(TeX("$x$")) + ylab(TeX("$\\Phi(x)$"))
fig_norm
```

可以看到，如果分布是关于原点对称的，显然有$u_{1-\alpha} =  -u_{\alpha}$。这是一个很有用的性质。

式\eqref{quzhou}中的随机变量$\sqrt{n}(\bar{X} - \mu)/\sigma$满足$N(0, 1)$，并且它的分布类型与$\mu$无关，因此我们可以通过这个变量来换算得到$\mu$的置信区间。根据定义\eqref{conflevel_def}，写出：
\begin{displaymath}
\begin{split}
P(-u_{\alpha/2} \leq \frac{\sqrt{n}(\bar{X}-\mu)}{\sigma} \leq u_{\alpha/2}) &= \Phi(u_{\alpha/2}) - \Phi(-u_{\alpha/2})\\
&= \Phi(u_{\alpha/2}) - (1 - \Phi(u_{\alpha/2}))\\
&= 1 - \alpha
\end{split}
\end{displaymath}

对上面的随机变量$\sqrt{n}(\bar{X}-\mu)/{\sigma}$及其两端$u_{\alpha/2}$稍作改写即可转换成关于$\mu$的置信区间的定义：
\begin{displaymath}
P(\bar{X} - \frac{\sigma u_{\alpha/2}}{\sqrt{n}}) \leq \mu \leq P(\bar{X} + \frac{\sigma u_{\alpha/2}}{\sqrt{n}}) = 1 - \alpha
\end{displaymath}

这样就得到了$\mu$的置信系数$1-\alpha$的置信区间为
\begin{equation}
\label{muconf}
[\hat{\theta_1}, \hat{\theta_2}] = [\bar{X} - \frac{\sigma u_{\alpha/2}}{\sqrt{n}},\bar{X} + \frac{\sigma u_{\alpha/2}}{\sqrt{n}}]
\end{equation}

由上面的例子可以看出随机变量$\sqrt{n}(\bar{X} - \mu)/\sigma$的作用，它起一个“中转枢纽”的作用，首先它包含未知参数$\mu$，但又有一个确切已知的分布形式，通过这个分布我们能写出符合定义\eqref{conflevel_def}的表达式，通过变形最终得到了$mu$的置信区间。

*2. 总体方差$\sigma^2$未知*

在总体方差未知时，枢轴变量中的$\sigma$就无法使用，我们使用样本标准差$S$来代替，这样数轴变量就变成了$\sqrt{n}(\bar{X} - \mu)/S$，经过一些证明可以知道它服从自由度为$n-1$的$t$分布，由于$t$分布同样和正态分布一样关于原点对称，因而可以继续沿用1.中的归纳方法，得出此情况下$\mu$的$1-\alpha$置信区间为：
\begin{equation}
\label{muconf_nsigma}
[\hat{\theta_1}, \hat{\theta_2}] = [\bar{X} - \frac{S t_{n-1}(\alpha/2)}{\sqrt{n}},\bar{X} + \frac{S t_{n-1}(\alpha/2)}{\sqrt{n}}]
\end{equation}

\section{假设检验}

\subsection{问题的提法和基本概念}

\subsubsection{问题的提法}

我们常常需要对取自总体的样本的一些参数，如均值，标准差进行判断（检验），设有如下问题：

$X_1, X_2, \cdots, X_n$是取自一正态分布总体$N(\theta, \sigma^2)$的样本，现在需根据这个样本来判断：总体的均值是否有$\theta \geq \theta_0$？

在这里$\theta$是一个未知的随机变量，命题$\theta \geq \theta_0$则称为一个“统计假设”，假设检验就是围绕这一事先预定好的假设来进行的。

由此引出问题的提法中的几个重要概念：

*1. 原假设和对立假设*

常把被判断或检验的假设称之为原假设，原假设记为$H_0$，上面的例子中原假设可以这样表述：

\begin{displaymath}
H_0: \{\theta | \theta \geq \theta_0\}
\end{displaymath}

另外一个与原假设对立的假设称为“对立假设”或“备择假设”，它与零假设互补组成整个假设空间，常把对立假设记为$H_1$，上面的例子的对立假设可以这样表述：

\begin{displaymath}
H_1: \{\theta | \theta < \theta_0\}
\end{displaymath}

原假设和对立假设成对出现，并且概率上是互斥事件，不会同时发生，假设检验的工作就是判断原假设和对立假设的可能性，并最后选择一个假设来接受。由此引出如下概念。

*2. 检验统计量、接受域、否定域、临界域和临界值*

根据上面的检验问题，提出的假设（简写）：

\begin{displaymath}
\begin{split}
&H_0: \theta \geq \theta_0 \\
&H_1: \theta < \theta_0
\end{split}
\end{displaymath}

由于在上面的假设中，$\theta$是一个未知的参数，我们需要一个统计量$Z$来估计$\theta$，然后看这个统计量$Z$是否满足上面的统计假设。直观上，样本的均值$\bar{X}$是一个不错的用来估计总体$\theta$的统计量，于是我们的统计假设问题就变成了这样一个检验流程：

\begin{displaymath}
\begin{split}
&\mbox{如果}\bar{X} \geq C,\mbox{接受}H_0\\
&\mbox{如果}\bar{X} < C,\mbox{拒绝}H_0
\end{split}
\end{displaymath}

很明显，如果我们计算得到的$\bar{X}$有$\bar{X} \geq C$，此时原假设$H_0$正确，我们接受它，相反，如果$\bar{X} < C$，此时原假设$H_0$错误，我们否定它并接受备择假设$H_1$。以上的这一套操作准则，就称之为对$\theta$的*检验*。这里用的$\bar{X}$就是一个*检验统计量*

正如$\bar{X} = \frac{\sum_{i=1} ^ {n} X_i}{n}$，检验统计量由样本$X_1, \cdots, X_n$决定，这样一来，原假设是否接受也由样本来决定。把使得原假设接受的那些样本组成的区域成为这个检验的*接受域*，而把使原假设被否定的那些样本组成的区域则成为检验的*否定域*。

这里把接受域和否定域记为：

\begin{displaymath}
\begin{split}
A &= \{(X_1, \cdots, X_n)| X_1 + \cdots + X_n \geq nC\}\\
R &= \{(X_1, \cdots, X_n) | X_1 + \cdots + X_n < nC\}
\end{split}
\end{displaymath}

可见，当这里的检验中$C$有重要作用，当$\bar{X}$的值越过这一个界限，结论就由接受变成否定，这个$C$称之为检验统计量$\bar{X}$的*临界值*。

### 功效函数

原假设的接受与否要看检验统计量是否越过了临界值，而检验统计量是由样本决定的，由于样本的随机性，检验统计量是一个随机变量。因此上面的例子中$\bar{X}$就是一个随机变量。因此，可以知道，原假设$H_0: \theta \geq \theta_0$被否定的概率为：

\begin{equation}
\beta_{\Phi}(\theta) = P_{\theta}(\bar{X} < C)\label{gongxiao}
\end{equation}

由于样本$X_1,\cdots, X_n$之间独立，且来自同一总体$N(\theta, \sigma^2)$，所以由正态分布的性质：若$X_1, \cdots, X_n$相互独立，且分别服从正态分布$N(\theta_1, \sigma_1^2), \cdots, N(\theta_n, \sigma_n^2)$，那么随机变量$X_1 + \cdots + X_n$的分布满足正态分布$N(\theta_1 + \cdots + \theta_n, \sigma_1^2 + \cdots + \sigma_n^2)$，可以对\eqref{gongxiao}进行化简。

\begin{equation}
\begin{split}
\beta_{\Phi}(\theta) &= P_{\theta}(\bar{X} < C)\\
&= P_{\theta}(X_1 + X_2 + \cdots + X_n < nC)\\
&= \Phi_{(n\theta, n\sigma^2)}(nC)\label{gongxiaohanshu}
\end{split}
\end{equation}

可以看到，式\eqref{gongxiaohanshu}就是参数为$N(n\theta, n\sigma^2)$的正态分布的累积分布函数，这个函数的取值完全和$\theta$与$\sigma$有关，这里假设$\sigma$是已知的，那么就只和$\theta$有关了。

式\eqref{gongxiaohanshu}就是一个检验$\Phi$的功效函数，由此给出功效函数的一般定义：

\begin{Definition}
\label{gongxiaodingyi}
设总体分布包含若干个未知参数$\theta_1,\cdots, \theta_k$，$H_0$是关于这些参数的一个原假设，设有了样本$X_1,\cdots,X_n$，而$\Phi$是针对这些样本而做的一个检验，则称检验$\Phi$的功效函数为:
\begin{equation}
\beta_{\Phi} = P_{\theta_1, \cdots, \theta_k}(\Phi: \mbox{拒绝}\quad H_0)\label{beta_phi}
\end{equation}
\end{Definition}

可见，式\eqref{beta_phi}是$\theta_0,\cdots,\theta_k$的函数。

功效函数$\beta_{\Phi}(\theta_1,\cdots,\theta_k)$计算的是在检验$\Phi$下，原假设$H_0$被否定的概率，另一种方式就是说假设$H_1$被接受的概率，我们希望功效函数最好能满足如下性质：

如果能有一组$\theta_0,\cdots,\theta_k$使得$H_0$成立，也就是说当$H_0$为真，$H_1$位假，我们不希望否定$H_0$,，那么我们希望$\beta_{\Phi}(\theta_1,\cdots,\theta_k)$能尽量小，这样就能尽量小的概率否定$H_0$。反过来，若有一组$\theta_0,\cdots,\theta_k$使得$H_1$成立，也就是说当$H_0$为假，$H_1$位真时，我们不希望接受$H_0$，那么我们希望，我们希望$\beta_{\Phi}(\theta_1,\cdots,\theta_k)$能尽量的大，这样就能以尽量大的概率否定$H_0$，或者说接受$H_1$。

值得注意，“功效”一词在$\theta_1, \cdots, \theta_k$使得对立假设成立时更为恰当，因当$\theta_1, \cdots, \theta_k$属于对立假设时，$\beta_{\Phi}(\theta_1,\cdots,\theta_k)$应当尽可能大。而当$\theta_1,\cdots,\theta_k$属于原假设时，$\beta_{\Phi}(\theta_1,\cdots,\theta_k)$取小值，这时叫做“功效”就不恰当。

\subsubsection{两类错误，检验的水平}

通过功效函数$\beta_{\Phi}$来对假设$H_0$进行检验时，由于检验的结果是一个概率事件，那么可能出现这两种情况：

1. $H_0$实际是正确的（$H_1$实际上错误），但我们却否定了$H_0$。
2. $H_0$实际是错误的（$H_1$实际上正确），但我们却接受了$H_1$。

由于检验结果是和总体的分布的参数$N(\theta,\sigma^2)$有关的，因此犯错误的类型也和这些参数有关，这一点在后面会详细解释。由此总结出第一类错误和第二类错误的数学形式：

\begin{equation}
\begin{split}
&\alpha_{1\Phi}(\theta_1,\cdots,\theta_k) =
\left\{
\begin{split}
&\beta_{\Phi}(\theta_1,\cdots,\theta_k),\quad (\theta_1,\cdots,\theta_k) \in H_0\\
&0,\quad (\theta_1,\cdots,\theta_k) \in H_1
\end{split}
\right.\\
&\alpha_{2\Phi}(\theta_1,\cdots,\theta_k) =
\left\{
\begin{split}
&0,\quad (\theta_1,\cdots,\theta_k) \in H_0\\
&1 - \beta_{\Phi}(\theta_1,\cdots,\theta_k),\quad (\theta_1,\cdots,\theta_k) \in H_1
\end{split}
\right.
\end{split}
\end{equation}

一般的，更希望一类错误的概率更小，这时会定出一个$\alpha$来限定一类错误小于它，再在此基础上尽量减少第二类错误。由此给出*检验水平*的定义

\begin{Definition}
\label{jianyanshuiping}
设$\Phi$是原假设$H_0$下的一个检验，$\beta_{\Phi}(\theta_1,\cdots,\theta_k)$为其功效函数，$\alpha$满足$\alpha \in [0, 1]$，如果：
\begin{equation}
\label{jysp-eq}
\beta_{\Phi}(\theta_1,\cdots,\theta_k) \leq \alpha \quad (\forall (\theta_1,\cdots,\theta_k) \in H_0)
\end{equation}
则称$\Phi$为$H_0$的水平为$\alpha$的检验。
\end{Definition}

可以看到水平$\alpha$实际上指是功效函数$\beta_{\Phi}(\theta_1,\cdots,\theta_k)$的值在区间$H_0$上永远小于我们给定的$\alpha$，如果这个条件不成立，那么$\alpha$就不是这个检验$\Phi$在$H_0$上的一个水平了。（功效函数的目的就是首先保证一类错误要小于一定水平$\alpha$）。

\subsubsection{一致最优检验}

检验的水平让检验函数在$(\theta_0,\cdots,\theta_k) \in H_0$时小于指定的$\alpha$，使得第一类错误发生的概率小于$\alpha$。同样的，当$(\theta_0,\cdots,\theta_k) \in H_1$时，我们可以给出一致最优检验的定义：

\begin{Definition}
\label{yizhizuiyou}
设$\Phi$为一个水平为$\alpha$的检验，若对其他任何一个水平为$\alpha$的检验$g$，必有
\begin{equation}
\beta_{\Phi}(\theta_1,\cdots,\theta_k) \geq \beta_{g}(\theta_0,\cdots,\theta_k) \quad (\forall (\theta_0,\cdots,\theta_k) \in H_1)
\end{equation}
\end{Definition}
可以看到，根据\eqref{yizhizuiyou}的定义，一致最优检验检验不仅仅在$H_0$上满足水平$\alpha$的检验，在$H_1$上，根据第二类错误概率定义$\alpha_{2 \Phi}(\theta_0,\cdots,\theta_k) = 1 - \beta_{\Phi}(\theta_0,\cdots,\theta_k)$，它发生二类错误的概率也是所有水平为$\alpha$的检验中最小的。

用图来描述，假设有三个功效函数$\Phi_1,\Phi_2,\Phi_3$，他们都是针对假设$H_0:\theta > \theta_0,H_1:\theta \leq \theta_0$的两个检验，如果$\Phi_1$是一致最优检验，那么它的功效函数必然满足如下样子：

```{r scatter,eval=T, echo=F, warning=F, error=F, fig.height=3, fig.width=4}
par(mar = c(2, 2, .1, .1), las = 1)
th0 <- 20
x <- c(-100:100)
y1 <- -pnorm(x, mean=th0, sd=20) + 0.5
y2 <- -pnorm(x, mean=th0, sd=30) + 0.5
y3 <- -pnorm(x, mean=th0, sd=40) + 0.5

data <- data.frame(x, y1, y2, y3)
fig <- ggplot(data) + geom_line(aes(x, y1), colour="red",size=1.5) + geom_line(aes(x, y2), colour="blue") + geom_line(aes(x, y3), colour="green") + geom_vline(xintercept = th0) +scale_x_continuous(breaks=c(-100, -50, 0, th0, 50, 100), labels = c("", TeX("$H_1:\\theta \\leq \\theta_0$"), "0", TeX("$\\theta_0$"), TeX("$H_0:\\theta > \\theta_0$"), "")) + scale_y_continuous(breaks=c(0.5, 0, -0.5), labels=c("", TeX("$\\alpha$"), "")) + xlab(TeX("$\\theta$")) + ylab(TeX("$\\beta_{\\Phi}(\\theta)$"))
fig
```

可以看到，三个功效函数在$\theta>\theta_0$处都满足水平为$\alpha$的检验，而在$\theta \leq \theta_0$处，标红色的功效函数明显取更大的$\beta_{\Phi}(\theta)$，也就是说它第二类错误的概率是三个功效函数中最小的，此时才能称之为*一致最优检验*

一直最优检验要求功效函数在$H_1$上处处出现第二类错误概率最小，这是个很严格的条件。实际上许多针对同一假设的检验之间无法找到一个一致最优检验，即它们之中的任何一个都不能达到“处处”最小的要求，反而是，在某一区间内，$\Phi_1$是最小的，而到了另外一个区间内$\Phi_2$是最小的。

\subsection{重要参数检验}

基于参数的点估计是一个直观的检验方法，因为要检验的参数是未知量，往往通过样本来得出一个参数的估计量用做检验，点估计是最常用的检验统计量。下面给出在已知总体分布情况下，对总体的未知参数设计检验的过程。

\subsubsection{正态总体均值的检验}

$X_1, X_2, \cdots, X_n$是取自一正态分布总体$N(\theta, \sigma^2)$的样本，针对总体的均值，先定一个$\theta_0$，有常见如下假设检验问题：

1. $H_0:\theta \geq \theta_0$，$H_1:\theta < \theta_0$；
2. $H_0:\theta \leq \theta_0$，$H_1:\theta > \theta_0$；
3. $H_0:\theta = \theta_0$，$H_1:\theta \neq \theta_0$；

问题1和问题2形式上接近，问题3则有略微的不同，先对问题1设计检验，这里有两种情况：总体方差已知和总体方差未知。

*1.方差$\sigma^2$已知*

如果问题如上文1中所示，我们用点估计$\bar{X}$来估计总体的方差，直观上，我们可认为$\bar{X}$越大的情况，越有可能满足$H_0$，至于$\bar{X}$要大到什么程度，这里用$C$来表示，这个$C$是在一开始不知道的量。由此提出如下一个检验方法
\begin{equation}
\Phi: \mbox{当}\bar{X} \geq C,\mbox{接受原假设},\mbox{当}\bar{X} < C\mbox{否定原假设}
\end{equation}

接着给出这个检验的功效函数，考虑功效函数的定义\eqref{gongxiaodingyi}，又因总体分布为$N(\theta, \sigma^2)$的分布，这时统计量$\frac{\sqrt{n}(\bar{X} - \theta)}{\sigma} \sim N(0, 1)$，于是功效函数可以化为：
\begin{equation}
\begin{split}
\beta_{\Phi}(\theta)&=P_{\theta}(\bar{X} < C)\\
&= P_{\theta}(\frac{\sqrt{n}(\bar{X} - \theta)}{\sigma} < \frac{\sqrt{n}(C-\theta)}{\sigma})\\
&= \Phi(\frac{\sqrt{n}(C-\theta)}{\sigma})
\end{split}
\end{equation}

若要让功效函数满足其检验水平为$\alpha$，即想让我们这个检验第一类错误概率为$\alpha$，根据其定义\eqref{jianyanshuiping}可知，这个函数$\Phi(\frac{\sqrt{n}(C-\theta)}{\sigma})$和$\theta$有关，当$\theta$增加，函数值变小，可见函数$\Phi$在$\theta \in H_0:\{\theta|\theta \geq \theta_0\}$上单调递减，由此给出下式：

\begin{equation}
\label{lingjiezhi}
\begin{split}
&\beta_{\Phi}(\theta)=\Phi(\frac{\sqrt{n}(C-\theta)}{\sigma}) \leq \alpha\quad \forall \theta \in H_0:\{\theta|\theta \geq \theta_0\}\\
&\mbox{相当于}\\
&\frac{\sqrt{n}(C-\theta)}{\sigma} \leq u_{1-\alpha}\\
&\mbox{让}\theta = \theta_0\\
&\frac{\sqrt{n}(C-\theta_0)}{\sigma} = u_{1-\alpha} = -u_{\alpha}\\
&\mbox{由此可得：}\\
&C = \theta_0 - \frac{\sigma u_{\alpha}}{\sqrt{n}}\\
\end{split}
\end{equation}

这样就得到了功效函数$\beta_{\Phi}(\theta)$在水平$\alpha$下检验统计量$\bar{X}$的临界值$C$，把$C$带回，得到检验$\Phi$的功效函数的完整形式。
\begin{equation}
\label{wanzhenggongxiao}
\beta_{\Phi}(\theta) = \Phi(\frac{\sqrt{n}(\theta_0-\theta)}{\sigma} - u_{\alpha})
\end{equation}

下面给出一个例子，并绘制出这个例子的功效函数。
```{r sample1, eval=T, echo=F, warning=F, message=F}
set.seed(1234)
th0 <- 10
th_real <- 11
th_set <- seq(0, 20, 0.1)
sd_real <- 1
n <- 10
alpha <- 0.05
u <- qnorm(alpha, lower.tail = F)
x_sample <- rnorm(n, mean=th_real, sd=sd_real)
z_stat <- sqrt(n)*(th0 - th_set) - u
c <- th0 - (sd_real*u/sqrt(n))
y <- pnorm(z_stat)
```
\begin{lemma}
已知样本`r x_sample`来自总体$N(\theta, 1)$，考虑假设检验问题$H_0:\theta \geq 10,H_1:\theta < 10$,在检验水平$\alpha=0.05$情况下的情况。
\end{lemma}
根据\eqref{lingjiezhi}，直接计算临界值$C$:
\begin{displaymath}
\begin{split}
C &= \theta_0 - \frac{\sigma u_{0.05}}{\sqrt{n}}\\
&= 10 - \frac{1 \times 1.644854}{\sqrt{10}} = `r c`
\end{split}
\end{displaymath}

显然，样本的均值$\bar{X}$为`r mean(x_sample)`，满足$\bar{X} \geq `r c`$，接受原假设$H_0: \theta \geq 10$，事实上，以上的数据取自正态分布$N(`r th_real`, `r sd_real`)$，检验的结果符合总体的分布情况。

下面考虑更一般的，绘制出该检验下的功效函数图像：

```{r lizitu, eval=T, echo=F, warning=F, message=F, error=F,fig.height=3, fig.width=5.5}
fig_lizitu <- ggplot(data = data.frame(th_set, y)) + geom_line(aes(th_set, y), colour="blue", size=0.8) + geom_hline(yintercept = 0.05) + geom_hline(yintercept = 0.9, colour="red") + geom_vline(xintercept = 9.1, colour="red")
fig_lizitu <- fig_lizitu + xlab(TeX("$\\theta$")) + ylab(TeX("$\\beta_{\\Phi}(\\theta)$"))
x_label_h1 <- TeX("$H_1:\\theta<10$")
x_label_h0 <- TeX("$H_0:\\theta \\geq 10$")
x_label_th0 <- TeX("$\\theta_0=10$")
x_label_th1 <- TeX("$\\theta_1$")
fig_lizitu <- fig_lizitu + scale_y_continuous(breaks = c(0.00, 0.05, 0.25, 0.5, 0.75, 0.90, 1.00), labels=c("0.00", TeX("$\\alpha$"), "0.25", "0.5", "0.75", TeX("$1 - \\beta$"), "1.00"))
fig_lizitu <- fig_lizitu + scale_x_continuous(labels = c("", x_label_h1, x_label_th1, x_label_th0, x_label_h0, ""), breaks=c(0.0, 5.0, 9.1, 10.0, 15.0, 20.0))
fig_lizitu
```

仔细观察功效功效函数和它的图像，可以得出如下结论

1. 当分布的真实值$\theta$越大时，越容易满足零假设$H_0:\theta\geq 10$，此时功效函数急剧下降，最后趋近于零，说明在越大的$\theta$下，零假设被拒绝的概率是十分小，犯第一类错误的概率十分小。相反的，越小的$\theta$，越容易远离零假设$H_0$，而满足备择假设$H_1:\theta < 10$，此时功效函数急剧上升，说明在越小的$\theta$下，零假设会以很大的概率被拒绝，犯第二类错误的概率也很小（$\alpha_{2\Phi}(\theta)=1-\beta_{\Phi}(\theta)$）。

2. 当分布的真实值$\theta < 10$，但$\theta$与10差距很小时，可以看到功效函数在10附近的变化很急剧，此时的$\beta_{\Phi}(\theta)$与我们预先设计的水平$\alpha=0.05$，接近，这是犯第二类错误的概率就比较大了$\alpha_{2\Phi}(\theta)\simeq 1-0.05=0.95$。可见若$\theta$很接近我们预先设定$\theta_0$，功效函数的性能就不佳了，此时虽然能保证第一类错误的概率是一个很低的值，但第二类错误的概率却大大上升了。

3. 如果提出要求，让犯第二类错误的概率小于指定的$\beta$，即让$H_1$正确，但被拒绝的概率要小于$\beta$。这等价于有:

\begin{equation}
\label{erleigongxiao}
1 - \beta_{\Phi}(\theta) < \beta \Leftrightarrow \beta_{\Phi}(\theta) \geq 1 - \beta \m (\forall \theta \in {\theta < \theta_0})
\end{equation}

我们一般希望$\beta$取一个很小的值，比如$0.1$，但实际上根据函数的性质，这一点在整个$H_1$上无法实现。当$\theta < \theta_0$但十分接近$\theta_0$时，$\beta_{\Phi}(\theta) \simeq \alpha$，显然由于$\alpha$和$\beta$都是一个很小的值，那么$\beta_{\Phi}(\theta) \simeq \alpha \geq 1-\beta$是无法实现的。但是如果稍微放宽一点限制，让$\theta$小于某一个$\theta_1 < \theta_0$，因为函数在$\theta_0$附近急剧变化，稍小一点的$\theta < \theta_1$能让$\beta_{\Phi}(\theta) \geq 1 - \beta$很容易满足。

$\beta_{\Phi}(\theta)$中另外两个重要参数是总体分布的$\theta$和$\sigma$，在固定$\theta$情况下，它们对检验的结果有重要影响，下面给出它们的函数图像

```{r eval=T, echo=F, warning=F, message=F, error=F,fig.height=2.5, fig.width=4}
#set.seed(1234)
th0 <- 10
th_real <- 8
sd_set <- seq(1, 10, 0.1)
n <- 10
alpha <- 0.05
u <- qnorm(alpha, lower.tail = F)
z_stat <- (sqrt(n)*(th0 - th_real)/sd_set) - u
y <- pnorm(z_stat)

fig_sd_zstat <- ggplot(data=data.frame(sd_set, z_stat)) + geom_line(aes(x=sd_set, z_stat), colour = "blue") + xlab(TeX("$\\sigma$")) + ylab(TeX("$Z$")) + scale_x_continuous(labels = c("1", "", "5.0", "", "10.0"))
fig_zstat_y <- ggplot(data=data.frame(z_stat, y)) + geom_line(aes(z_stat, y), colour="blue")+xlab(TeX("$Z$")) + ylab(TeX("$\\beta_{\\Phi}(\\theta)$"))
fig_sd_y <- ggplot(data=data.frame(sd_set, y)) + geom_line(aes(sd_set, y), colour="blue") + xlab(TeX("$\\sigma$")) + ylab(TeX("$\\beta_{\\Phi}(\\theta)$")) + scale_x_continuous(labels = c("1", "", "5.0", "", "10.0"))

grid.newpage()
pushViewport(viewport(layout=grid.layout(1, 3)))

print(fig_sd_zstat, vp=vplayout(1, 1))
print(fig_zstat_y, vp=vplayout(1, 2))
print(fig_sd_y, vp=vplayout(1, 3))
```

1. 由上面的三幅图像可知，对某个固定的$\theta < 10$，$\sigma$越大，$Z=\frac{\sqrt{n}(\theta_0-\theta)}{\sigma}-u_{\alpha}$越小，由第二图，分布函数是增函数，可知$\beta_{\Phi}(\theta)$越小。越有可能出现第二类错误。这说明误差越大，$\theta$与$\theta_0=10$越不容易分清，这样就越容易犯错。

```{r eval=T, echo=F, warning=F, message=F, error=F,fig.height=2.5, fig.width=4}
#set.seed(1234)
th0 <- 15.5
th_real <- 15
sd_real <- 1
n <- 10
alpha_set <- seq(0.01, 0.99, 0.01)
u_set <- qnorm(alpha_set, lower.tail = F)
z_stat <- (sqrt(n)*(th0 - th_real)/sd_real) - u_set
y <- pnorm(z_stat)

fig_alpha_zstat <- ggplot(data=data.frame(alpha_set, z_stat)) + geom_line(aes(x=alpha_set, z_stat), colour = "blue") + xlab(TeX("$\\alpha$")) + ylab(TeX("$Z$")) + scale_x_continuous(labels = c("0.0", "", "0.5", "", "1.0"))
fig_zstat_y <- ggplot(data=data.frame(z_stat, y)) + geom_line(aes(z_stat, y), colour="blue")+xlab(TeX("$Z$")) + ylab(TeX("$\\beta_{\\Phi}(\\theta)$"))
fig_alpha_y <- ggplot(data=data.frame(alpha_set, y)) + geom_line(aes(alpha_set, y), colour="blue") + xlab(TeX("$\\alpha$")) + ylab(TeX("$\\beta_{\\Phi}(\\theta)$"))+scale_x_continuous(labels = c("0.0", "", "0.5", "", "1.0"))

grid.newpage()
pushViewport(viewport(layout=grid.layout(1, 3)))

print(fig_alpha_zstat, vp=vplayout(1, 1))
print(fig_zstat_y, vp=vplayout(1, 2))
print(fig_alpha_y, vp=vplayout(1, 3))

```

2. 增大功效$\alpha$，$u_{\alpha}$就变小了，这样$\beta_{\Phi}(\theta)$就更大，所以这种情况下出现一类错误的可能性就变大了，反过来，二类错误的可能性就小了。可见$\alpha$的选择无法兼顾让两种都小，小的$\alpha$使得第一类错误很小，但与此同时出现第二类错误的可能就变大了。

问题2$H_0:\theta \leq \theta_0$，$H_1:\theta > \theta_0$的检验和1十分相似，这里不展开讨论，下面开始讨论问题3即$H_0:\theta = \theta_0,H_1:\theta \neq \theta_0$的检验。直观上看，仍然采用点估计$\bar{X}$来做检验统计量，我们可以认为当$\bar{X}$与$\theta_0$比较接近时，$H_0$成立，反之，距离较远则否定$H_1$，由此得出问题3的一个检验法：

\begin{equation}
\label{shuangcejianyan}
\Phi^{''}:\mbox{当}|\bar{X} - \theta_0| \leq C \mbox{时接受}H_0,\mbox{反之则否定}H_0.
\end{equation}

根据功效函数的定义\eqref{gongxiaodingyi}，直接写出检验$\Phi^{''}$的功效函数：

\begin{equation}
\label{shuangcegongxiao}
\begin{split}
\beta_{\Phi^{''}}(\theta) &= P_{\theta}(|\bar{X}-\theta| > C)\\
&= P_{\theta}(\{\frac{(\bar{X}-\theta)\sqrt{n}}{\sigma} > \frac{C\sqrt{n}}{\sigma}\} \cup \{\frac{(\bar{X}-\theta)\sqrt{n}}{\sigma} < -\frac{C\sqrt{n}}{\sigma}\})\\
&= 1 - \Phi(\frac{C\sqrt{n}}{\sigma}) + \Phi(-\frac{C\sqrt{n}}{\sigma})\\
&= 2 - 2\Phi(\frac{C\sqrt{n}}{\sigma})
\end{split}
\end{equation}

接着，根据检验水平的定义\eqref{jianyanshuiping}，写出在水平$\alpha$下的临界值$C$的推导过程：

\begin{displaymath}
\begin{split}
&\Phi(\frac{C\sqrt{n}}{\sigma}) = 2 - 2\Phi(\frac{C\sqrt{n}}{\sigma}) \leq \alpha \quad \forall \theta \in H_0:\{\theta|\theta = \theta_0\}\\
&\mbox{相当于}\\
&\Phi(\frac{C\sqrt{n}}{\sigma}) \geq 1 - \frac{\alpha}{2}\\
&\mbox{由与函数}\Phi \mbox{单调递增}可知\\
& \frac{C\sqrt{n}}{\sigma} = u_{\frac{\alpha}{2}}
\end{split}
\end{displaymath}

由此得出临界值$C$的值为：

\begin{equation}
C = \frac{u_{\frac{\alpha}{2}}\sigma}{\sqrt{n}}
\end{equation}

把$C$带回\eqref{shuangcejianyan}中，得到$\bar{X}$的接受域为
\begin{equation}
\theta_0 - \frac{\sigma u_{\frac{\alpha}{2}}}{\sqrt{n}} \leq \bar{X} \leq \theta_0 + \frac{\sigma u_{\frac{\alpha}{2}}}{\sqrt{n}}
\end{equation}

只要$\bar{X}$落入这个范围，我们就接受$H_0:\theta=\theta_0$，否则就拒绝$H_0$。

把$C$带入功效函数\eqref{shuangcegongxiao}中，可以得到$\Phi^{''}$的功效函数：
\begin{equation}
\begin{split}
\beta_{\Phi^{''}}(\theta) &= 2 - 2\Phi(u_{\frac{\alpha}{2}})\\
&= 2 - 2(1 - \frac{\alpha}{2})\\
&= \alpha
\end{split}
\end{equation}

很显然，功效函数不含未知的参数$\theta$，也与样本没有关系，只和我们预先指定的水平$\alpha$有关，这说明在这种检验下，*一致最优检验*是不存在的。

*2.方差$\sigma^2$未知*

首先我们看方差已知时考虑检验问题1的情形，由\eqref{lingjiezhi}，检验$\Phi$写成:

\begin{displaymath}
\Phi: \mbox{当}\bar{X} \geq \theta_0 - \frac{\sigma u_{\alpha}}{\sqrt{n}},\mbox{接受原假设},\mbox{当}\bar{X} < \theta_0 - \frac{\sigma u_{\alpha}}{\sqrt{n}}\mbox{否定原假设}
\end{displaymath}

变换一下形式得到：
\begin{displaymath}
\Phi: \mbox{当}\frac{\sqrt{n}(\bar{X}-\theta_0)}{\sigma} \geq -u_{\alpha},\mbox{接受原假设},\mbox{反之则否定原假设}
\end{displaymath}

可以看到随机变量$\frac{\sqrt{n}(\bar{X}-\theta_0)}{\sigma}$满足$N(0, 1)$的正态分布，但在总体方差未知情况，我们无法已知$\sigma$，如果我们用点估计$S^2 = \sum_{i=1}^n\frac{(X_i - \bar{X})^2}{n-1}$来估计$\sigma^2$，分布就发生了变化。我们这里先写明此时$\frac{\sqrt{n}(\bar{X}-\theta_0)}{\sigma}$满足自由度为$n-1$的$t$分布，下面对这一点给出证明。

若$X$与$Y$相互独立，且有$X \sim N(0, 1)$，$Y \sim \chi_n^2$，又设$\delta$为常数，则随机变量$(X+\delta)/\sqrt{\frac{1}{n}Y}$的分布称为自由度为n，非中心参数为$\delta$的非中心$t$分布。记为$Z \sim t_{n,\delta}$，分布函数记为$F_{n,\delta}(x)$，若有一组符合$t$分布的随机变量$Z_1, Z_2, Z_3$，它们的满足$\delta_1 < \delta_2 < \delta_3$，下面给出这组变量$t$分布的图像。

```{r tdis, eval=T, echo=F, warning=F, message=F, error=F,fig.height=2.5, fig.width=4}
df <- 10
ncp1 <- 0
ncp2 <- 2
ncp3 <- 4
x <- seq(-10, 15, 0.1)
dt1 <- dt(x, df=df, ncp=ncp1)
dt2 <- dt(x, df=df, ncp=ncp2)
dt3 <- dt(x, df=df, ncp=ncp3)
ft1 <- pt(x, df=df, ncp=ncp1)
ft2 <- pt(x, df=df, ncp=ncp2)
ft3 <- pt(x, df=df, ncp=ncp3)
da <- data.frame(x, dt1, dt2, dt3, ft1, ft2, ft3)

figdt <- ggplot(data=da) + geom_line(aes(x, dt1), colour="blue", size=0.5) + geom_line(aes(x, dt2), colour="red", size=0.8) + geom_line(aes(x, dt3), colour="green", size=1.2) + xlab(TeX("$\\Z$")) + ylab(TeX("$t_{n,\\delta}$"))
figft <- ggplot(data=da) + geom_line(aes(x, ft1), colour="blue", size=0.5) + geom_line(aes(x, ft2), colour="red", size=0.8) + geom_line(aes(x, ft3), colour="green", size=1.2) + xlab(TeX("$\\Z$")) + ylab(TeX("$F_{n, \\delta}(Z)$"))

grid.newpage()
pushViewport(viewport(layout=grid.layout(1, 2)))
print(figdt, vp=vplayout(1, 1))
print(figft, vp=vplayout(1, 2))
```

如上图蓝色，红色，绿色的曲线分别代表$Z_1, Z_2, Z_3$，可见若有$\delta_1 < \delta_2$，则对任一的$Z$，有$F_{n, \delta_1}(Z)\geq F_{n, \delta_2}(Z)$

有了如上的信息，可知：

\begin{displaymath}
\frac{\sqrt{n}(\bar{X}-\theta_0)}{S} = (\frac{\sqrt{n}(\bar{X}-\theta)}{\sigma} + \frac{\sqrt{n}(\theta - \theta_0)}{\sigma})/\sqrt{\frac{1}{\sigma^2}S^2}
\end{displaymath}

令$X = \frac{\sqrt{n}(\bar{X}-\theta)}{\sigma}$，$\delta = \frac{\sqrt{n}(\theta - \theta_0)}{\sigma}$，$Y = \frac{(n-1)S^2}{\sigma^2}$，容易知道$X$，$Y$相互独立且有$X \sim N(0, 1)$以及$Y \sim \chi_{n-1}^2$，根据$t$分布的定义，最后得到：

\begin{displaymath}
\frac{\sqrt{n}(\bar{X}-\theta_0)}{S} \sim t_{n-1, \sqrt{n}(\theta-\theta_0)/\sigma}
\end{displaymath}

所以，可以把上面的检验改写成：

\begin{equation}
\label{t_psi}
\Psi: \mbox{当}\frac{\sqrt{n}(\bar{X}-\theta_0)}{S} \geq -t_{n-1}(\alpha),\mbox{接受原假设},\mbox{反之则否定原假设}
\end{equation}

这里$t_{n-1}(\alpha)$与$u_{\alpha}$类似，是指自由度$n-1$，非中心参数为$\delta=0$（此时$\theta=\theta_0$）的$t$分布的上分位数为$\alpha$的点。

这样，可以直接给出检验$\Psi$的功效函数：

\begin{equation}
\begin{split}
\beta_{\psi}(\theta, \sigma) &= P_{\theta, \sigma}(\sqrt{n}(\bar{X}-\theta_0)/S <-t_{n-1}(\alpha) )\\
&=F_{n-1, \delta}(-t_{n-1}(\alpha))
\end{split}
\end{equation}

这里$\delta = \frac{\sqrt{n}(\theta-\theta_0)}{\sigma}$，$\beta_{\psi}(\theta, \sigma)$的取值与$\delta$有关，要想让功效函数有水平$\alpha$，就要求：

\begin{displaymath}
\beta_{\psi}(\theta, \sigma) = F_{n-1, \delta}(-t_{n-1}(\alpha)) \leq \alpha \quad \forall \theta \in H_0:\{\theta|\theta \geq \theta_0\}
\end{displaymath}

由上面对$t$分布的性质可知，$F$对于$\delta$是递减的，而$\delta=\frac{\sqrt{n}(\theta-\theta_0)}{\sigma}$对$\theta$是递增的。因此$\theta$变大时，$F$就会变小，所以当$\theta=\theta_0$时，对应了$F$的最大值。所以$\beta_{\psi}(\theta, \sigma)$在$H_0$上存在最大值，且当$\theta=\theta_0$时取最大。这样就证明了检验$\Psi$的功效函数存在水平$\alpha$。所以有：

\begin{displaymath}
\begin{split}
&\beta_{\Psi}(\theta, \sigma) =  P_{\theta, \sigma}(\sqrt{n}(\bar{X}-\theta_0)/S <-t_{n-1}(\alpha) ) \leq \beta_{\Psi}(\theta_0, \sigma)\\
&\mbox{等价于}\\
&F_{n-1, \delta}(-t_{n-1}(\alpha)) \leq F_{n-1, 0}(-t_{n-1}(\alpha))\\
&\mbox{分布}t_{n-1,0}\mbox{关于原点对称，所以有}-t_{n-1}(\alpha)=t_{n-1}(1-\alpha)\mbox{得出}\\
&F_{n-1, \delta}(-t_{n-1}(\alpha)) \leq F_{n-1, 0}(t_{n-1}(1-\alpha))=\alpha
\end{split}
\end{displaymath}

由此，$F$的水平为$\alpha$得证。同样的，我们给出一个例子讨论。

```{r eval=T, echo=F, error=F, warning=F, message=F}
set.seed(1234)
n <- 10
sigma <- 2
th0 <- 120
sample_t <- rnorm(n, 110, 2)
alpha <- 0.05
th_set <- seq(90, 150, 0.5)
delta <- sqrt(n)*(th_set - th0)/sigma

t_sample <- sqrt(n)*(mean(sample_t) - th0)/sd(sample_t)
t_alpha <- qt(alpha, df=n-1, ncp=0, lower.tail = F)
#print(sample)
#beta <- pt(-t_alpha, df=n-1, ncp=delta)
```
\begin{lemma}
\label{lizi_ttest}
已知样本`r sample_t`来自总体$N(\theta, \sigma)$，考虑假设检验问题$H_0:\theta \geq 120,H_1:\theta < 120$,在检验水平$\alpha=0.05$情况下的情况。
\end{lemma}
\begin{displaymath}
\begin{split}
&\mbox{直接计算检验统计量：}\\
&\frac{\sqrt{n}(\bar{X}-\theta_0)}{S} = \frac{\sqrt{`r n`}(`r mean(sample_t)`-`r th0`)}{`r sd(sample_t)`}=`r t_sample`\\
&\mbox{查表得到自由度为}`r n-1`，\mbox{上分位点为}`r alpha`\mbox{的}t_{n-1(\alpha)}\\
&t_{`r n-1`}(`r alpha`) = `r t_alpha`\\
&\mbox{显然有：}\\
&`r t_sample` < `r -t_alpha`\\
&\mbox{根据检验$\Psi$\eqref{t_psi}式，原假设被拒绝}
\end{split}
\end{displaymath}

\subsubsection{两个正态总体均值差的检验}

若样本$X_1,\cdots,X_n$是从总体$N(\theta_1, \sigma^2)$抽出，而样本$Y_1, \cdots, Y_n$是从正态总体$N(\theta_2, \sigma^2)$中抽出。两个样本的总体方差相同，但均值是不同的。考虑如下检验问题：

1. $H_0:\theta_1 - \theta_2 \geq \theta_0,H_1:\theta_1 - \theta_2 < \theta_0;$
2. $H_0:\theta_1 - \theta_2 \leq \theta_0,H_1:\theta_1 - \theta_2 > \theta_0;$
3. $H_0:\theta_1 - \theta_2 = \theta_0,H_1:\theta_1 - \theta_2 \neq \theta_0.$

这三个检验问题本质上与前面单一样本的均值检验没有什么不同，考虑检验统计量$\bar{X}-\bar{Y}$作为$\theta_1-\theta_2$的估计，同样可以定出一个$C$作为临界值，于是对于问题1来说，一个检验$\Phi$是，$\bar{X}-\bar{Y}>C$时接受原假设，反之则拒绝。考虑$\bar{X}-\bar{Y}$的方差为$S^2 = (1/n + 1/m)\sigma^2$，于是类似前面的$u$检验临界值定义\eqref{lingjiezhi}$C=\theta_0 - \sigma u_{\alpha}/\sqrt{n}$，这里同样可设计一个检验统计量：

\begin{equation}
\label{doubletu}
U = \sqrt{\frac{nm}{n+m}}\frac{(\bar{X}-\bar{Y}-\theta_0)}{\sigma}
\end{equation}

于是上述上述三个问题的检验为:

1. $g$: 当$U \geq -u_{alpha}$接受$H_0$，不然就否定$H_0$;
2. $g^{'}$: 当$U \leq u_{\alpha}$接受$H_0$，不然就否定$H_0$;
3. $g^{''}$: 当$|U| \leq u_{\alpha/2}$接受$H_0$，不然就否定$H_0$.

对于方差$\sigma$未知的情况，检验统计量中的$\sigma$以样本的标准差来代替。此时这个统计量服从$t$分布：

\begin{equation}
T = \sqrt{\frac{nm}{n+m}}\frac{(\bar{X}-\bar{Y}-\theta_0)}{S}
\end{equation}

当$\theta_1 - \theta_2=\theta_0$时，$T$服从自由度为$n+m - 2$的$t$分布，于是可针对上面的假设问题可提出如下检验法：

1. $h$: 当$T \geq -t_{n+m-2}(\alpha)$时接受$H_0$，不然就否定$H_0$;
2. $h^{'}$: 当$T \leq t_{n+m-2}(\alpha)$时接受$H_0$，不然就否定$H_0$;
3. $h^{''}$: 当$|T| \leq t_{n+m-2}(\alpha/2)$时接受$H_0$，不然就否定$H_0$;

上述三个检验合称*双样本$t$检验*，值得注意，这个检验要求两样本方差相同，应用上则要求两样本方差之比与1相差不大。


