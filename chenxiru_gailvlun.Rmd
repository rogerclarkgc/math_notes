---
title: "陈希孺概率论与数理统计读书笔记-数理统计部分"
author:
  - RogerClark
documentclass: ctexart
output:
  rticles::ctex:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 3
classoption: "hyperref,"
---
```{r eval=T, echo=F,warning=F,message=F}
library(tidyverse)
library(latex2exp)
library(grid)
vplayout <- function(x, y){
  viewport(layout.pos.row=x, layout.pos.col = y)
}

pdis <- function(n, nmean, nsd, tmean=0, each=10){
  rs <- rnorm(n*each, mean=nmean, sd=nsd)
  rs <- matrix(rs, ncol=each, byrow=T)
  p_res <- apply(rs, 1, function(x){t.test(x, mu=tmean)$p.value})
  return(p_res)
}

```
\newtheorem{Definition}{\hspace{2em}定义}
\newtheorem{theorem}{\hspace{2em}定理}
\newtheorem{lemma}{\hspace{2em}例子}
\newtheorem{Proof}{证明}




\section{参数估计}

\subsection{基本概念}

\subsubsection{总体、样本、统计量}

*总体*：所研究问题有关对象的全体构成的集合

\begin{lemma}
\begin{enumerate}
\item 研究试验田中水稻的根系长度，那么试验田中所有水稻根系长度构成问题总体，其中一株水稻的根系长度为其个体
\item 某厂生产的一批螺母的质量，那么这一批螺母的质量构成总体，每个螺母的质量则为个体
\end{enumerate}
\end{lemma}

总体可以理解为研究对象所有可能取值组成的一个空间，这个空间符合某种概率分布，那么总体就叫符合某种分布的总体。如前面
提到的水稻根系长度，一般符合正态分布，那么这个总体属于正态分布的总体

*样本*：按照一定规定从总体中抽出的一部分个体，这个规定指得是每个个体都有同等被抽出的机会。

*统计量*：完全由样本决定的量叫做统计量，统计量的只依赖样本，不能依赖其他任何未知的量。

\subsubsection{样本矩}

一类重要的统计量称为样本矩，它分为*样本原点矩*和*样本中心矩*，定义如下

\begin{Definition}
样本原点矩：设$X_1,\cdots,X_n$位样本，$k$为正整数，则
\begin{equation}
a_k = \frac{(X_1^k + \cdots + X_n^k)}{n}
\end{equation}
称为$k$阶原点矩.
\end{Definition}
一阶原点矩，也就是$a_1 = \bar{X}$就是常见的样本均值。

\begin{Definition}
样本中心矩:设$X_1,\cdots,X_n$位样本，$k$为正整数，则
\begin{equation}
m_k = \frac{\sum_{i=1}^n(X_i - \bar{X})}{n}
\end{equation}
称为$k$阶中心矩阵
\end{Definition}
值得注意的是样本二阶中心矩为$m2 = \sum_{i=1}^n(X_i - \bar{X})^2/n$，样本的方差为$S^2 = \sum_{i=1}^n(X_i - \bar{X})^2/(n-1)$，
它们之间相差一个常数$m_2 = \frac{n-1}{n}S^2$
\subsection{矩估计、极大似然估计和贝叶斯估计}

\subsubsection{参数点估计问题}
当已知总体是某种分布，但不知道其概率密度函数的参数时，这时需要根据从总体中抽出的独立随机样本来估计总体分布的某个或几个参数。即有概率
密度函数$g(\theta_1, \cdots, \theta_k)$，其中$\theta_1$是未知的，我们便构造一个统计量$\hat{\theta_1}=\hat{\theta_1}(X_1,\cdots,X_n)$，
当有样本$X_1, \cdots, X_n$就带入$\hat{\theta_1}(X_1, \cdots, X_n)$得到$theta_1$的估计值。这样用一个点$\hat{\theta_1}$来估计未知参数$\theta_1$
的行为称为点估计。

\subsubsection{矩估计法}
设总体分布为$f(x;\theta_1,\cdots,\theta_k)$，则它的矩（此处以原点矩为例）为：
\begin{displaymath}
a_m = \int_{-\infty}^{\infty}x^mf(x;\theta_1,\cdots,\theta_k)
\end{displaymath}
对离散型的分布则有：
\begin{displaymath}
a_m = \sum_i x_i^m f(x_i;\theta_1,\cdots,\theta_k)
\end{displaymath}
当在样本大小$n$较大时，它接近样本的原点矩，于是有
\begin{displaymath}
a_m = a_m(\theta_1,\cdots,\theta_k) \simeq a_m = \sum_{i=1}^{n}X_i^m/n
\end{displaymath}
这样，式子左边是包含$\theta_1,\cdots, \theta_k$的表达式，右边是一个实数的样本矩，于是让$m=1,\cdots,k$，得到一系列方程组：
\begin{equation}
a_m(\theta_1,\cdots, \theta_k) = a_m \quad (m=1, \cdots, k)
\end{equation}
解这个方程组，其根$\hat{\theta_i}=\hat{\theta_i}(X_1,\cdots,X_n)$就是$\theta_i$的估计。

\subsubsection{极大似然估计法}
设总体分布为$f(x;\theta_1,\cdots,\theta_k)$，$X_1,\cdots,X_k$为取自总体的样本（各个样本之间独立同分布），则样本的分布为：
\begin{displaymath}
f(x_1;\theta_1,\cdots,\theta_k)f(x_2;\theta_1,\cdots,\theta_k)\cdots f(x_n;\theta_1,\cdots,\theta_k)
\end{displaymath}
把这分布记为$L(x_1,\cdots,x_n;\theta_1,\cdots,\theta_k)$，极大似然法就是把$L(x_1,\cdots,x_n;\theta_1,\cdots,\theta_k)$中的样本
$x_1,\cdots,x_n$固定，把$L$看成$\theta_1,\cdots,\theta_k$的函数，这样只要找到一组$\theta_1^*,\cdots,\theta_k^*$使得$L$取最大，
这组值就可以作为$\theta_1,\cdots,\theta_k$的估计了。即满足如下：
\begin{equation}
L(X_1, \cdots,X_n;\theta_1^{*},\cdots,\theta_k^{*}) = \max \limits_{\theta_1,\cdots,\theta_k} L(X_1,\cdots,X_n;\theta_1,\cdots,\theta_k)
\end{equation}
的$(\theta_1^*,\cdots,\theta_k^*)$作为$(\theta_1,\cdots,\theta_k)$的估计值。
根据$L$的定义容易得到:
\begin{equation}
\begin{split}
\ln L &= \ln f(x_1;\theta_1,\cdots,\theta_k)f(x_2;\theta_1,\cdots,\theta_k)\cdots f(x_n;\theta_1,\cdots,\theta_k)\\
& = \sum_{i=1}^n \ln f(X_i;\theta_1,\cdots, \theta_k)
\end{split}
\end{equation}
要让$L$在$(\theta_1, \cdots,\theta_k)$上有最大，只需要让$L$在$\theta_k$上的一阶偏导为0，这样就有：
\begin{equation}
\frac{\partial \ln L}{\partial \theta_i} = 0 \quad (i = 1, \cdots, k)
\end{equation}
由此得到$k$个关于$\theta_k$的方程组，如果该方程组有唯一解且能证明这个解是极大值点，则可得出极大似然估计。

值得注意的是，极大似然估计法和矩估计法在一些特殊的情况下导出的对总体的估计量是等价的，例如，当总体是正态分布、指数分布时。当然更多的情况是两者并不相同。

\subsubsection{贝叶斯法}
贝叶斯法的核心思想是“先验分布”的设定和“后验分布”的计算，假定要估计的一个参数是$\theta$，在抽样或实验之前，我们可以根据以前的经验或仅仅是直观感受把这个参数的分布定为一个$h(\theta)$，这个分布就称为*先验概率密度分布*。

有了先验分布，我们来一步步推导$\theta$的*后验概率密度分布*，首先我们开始抽样，得到一系列样本$X_1,\cdots,X_n$，设总体的分布是$f(X,\theta)$，那么，这个样本的概率密度函数为：
\begin{equation}
\label{sample_condition}
f(X_1,\theta)\cdots f(X_n, \theta)
\end{equation}
这个概率密度相当于在给定了$\theta$下样本$(X_1,\cdots,X_n)$的条件概率密度。根据联合概率密度的公式：
\begin{equation}
f(X_1,X_2) = f_1(X_1)f(X_2|X_1)
\end{equation}
这样我们写出$(\theta, X_1,\cdots,X_n)$的联合概率密度，注意，这里应该把系列样本$(X_1,\cdots,X_n)$作为一个整体看，这样\eqref{sample_condition}就相当于是样本在分布参数为$\theta$时的条件概率密度了。这样就有:
\begin{equation}
f(\theta, X_1,\cdots,X_n) = h(\theta)f(X_1,\theta)\cdots f(X_n, \theta)
\end{equation}
有了联合密度函数$f(\theta,X_1,\cdots,X_n)$，对它在$\theta$方向积分，得到了样本$(X_1,\cdots,X_n)$的一个所谓“普遍性”的边缘密度分布，如下：
\begin{equation}
\label{sample_border}
p(X_1,\cdots,X_n) = \int h(\theta) f(X_1,\theta)\cdots f(X_n, \theta) \textrm{d} \theta
\end{equation}
由此，根据条件概率公式，很容易写出$\theta$在抽样后的后验概率密度函数$h(\theta|X_1,\cdots,X_n)$为：
\begin{equation}
\label{theta_post}
\begin{split}
h(\theta |X_1,\cdots,X_n) &= \frac{f(\theta, X_1,\cdots,X_n)}{p(X_1,\cdots,X_n)}\\
&= \frac{h(\theta)f(X_1,\theta)\cdots f(X_n, \theta)}{\int h(\theta) f(X_1,\theta)\cdots f(X_n, \theta) \textrm{d} \theta}
\end{split}
\end{equation}
由此，我们按照贝叶斯法得出了$\theta$的后验概率分布，可以看到这个后验分布其实是$\theta$在样本$X_1,\cdots,X_n$的条件概率分布，它综合了先验分布$h(\theta)$和样本带来的信息$p(X_1,\cdots,X_n)$。
\subsection{点估计的优良性准则}
\subsubsection{估计量的无偏性}
下面给出一个*无偏估计量*的定义：
\begin{Definition}
设总体的分布有未知参数$\theta_1,\cdots,\theta_k$，$X_1,\cdots,X_n$是从总体抽出的样本，要估计的分布形式为$g(\theta_1,\cdots,\theta_k)$，$g$的形式是已知的。若$\hat{g}(X_1,\cdots,X_n)$是一个估计量。则对\uwave{任何}可能的$\theta_1,\cdots,\theta_k$，都有：
\begin{equation}
\label{unbias_definition}
E_{\theta_1,\cdots,\theta_k}[\hat{g}(X_1,\cdots,X_n)] = g(\theta_1,\cdots,\theta_k)
\end{equation}
则把满足\eqref{unbias_definition}的$\hat{g}$称为$g(\theta_1,\cdots,\theta_k)$的一个无偏估计量。
\end{Definition}

无偏估计量是一种优良的性质，其原因有二：

1. 说明估计量没有系统性误差，虽然$\hat{g}$的值可能是在真实$g$上下波动的，但在概率上平均，这种上下的波动并没有偏向上或者下，也就是说在偏差能相互抵消，说明估计量本身没有系统性的误差。下面以一个例子来说明

```{r unbias_example,eval=T, echo=F, warning=F, error=F, fig.height=3, fig.width=4}
n <- 1000
mu1 <- 0
mu2 <- 1
unbias <- rnorm(n, mu1)
bias <- rnorm(n, mu2)
ob <- c(1:n)
fig_unbias <- ggplot(data.frame(ob, unbias)) + geom_point(aes(ob, unbias), alpha=1/3) + geom_hline(yintercept = mu1, col="green", size=1.1) + ylab(TeX("$\\hat{g}_1$")) + xlab("observations")
fig_bias <- ggplot(data.frame(ob, bias)) + geom_point(aes(ob, bias),alpha=1/3) + geom_hline(yintercept = mu1, col="green", size=1.1) + ylab(TeX("$\\hat{g}_2$")) + xlab("observations") + geom_hline(yintercept = 1, col="red", size=1.1)

grid.newpage()
pushViewport(viewport(layout=grid.layout(1, 2)))
print(fig_unbias, vp=vplayout(1, 1))
print(fig_bias, vp=vplayout(1, 2))
```

可以看到，在上图的两个估计量$\hat{g_1}$和$\hat{g_2}$的1000次对正态总体$N(0, 1)$的均值估计中，$\hat{g_1}$的估计值虽然分布在0的上下两端，但可以看到分布基本呈一个对称态势，位于0上侧的和下侧的差不多，且越靠近0的点，它的数量就越多，这说明$\hat{g_1}$对总体的估计从概率上来说不总是偏高也不总是偏低，相反，估计量大于0的可能和小于0的可能是一样的，其期望值就是真实的0。

而$\hat{g_2}$就不同它的估计值大多数都大于0， 显然是偏高的，并且在1附近聚集的很明显，这就表明用这个估计量来估计总体$N(0, 1)$的均值，有很多时候会得到一个偏高的结果，这样它的期望就不会等于0，而是大于0，所以它不是一个无偏的估计量

2. 结合大数定理，若把$\hat{g}(X_1, \cdots,X_n)$在很多样本上使用，例如在样本$(X_1^{(1)},\cdots,X_n^{(1)})$上的估计值记为$\hat{g}(X_1^{(1)},\cdots,X_n^{(1)})$，就这样在N个样本使用，得到这样一组估计值$\hat{g}(X_1^{(1)},\cdots,X_n^{(1)}),\cdots,\hat{g}(X_1^{(N)},\cdots,X_n^{(N)})$，则根据大数定理，当$N \rightarrow \infty$时有：
\begin{displaymath}
\frac{\sum_{i=1}^N \hat{g}(X_1^{(i)},\cdots,X_n^{(i)})}{N} \rightarrow g(\theta_1, \cdots,\theta_k)
\end{displaymath}

即表明估计量的均值会依概率收敛到真实值$g(\theta_1,\cdots,\theta_k)$。而对无偏估计量而言，这个收敛到真实值的概率会是100%。反之，如果一个估计量不是无偏的，无论使用多少次，其均值也会和真实值保持一定距离，说明这个估计量存在一定的系统偏差。下面以一个例子来说明。

前面提到，样本的二阶中心矩$m_2 = \sum_{i=1}^n(X_i-\bar{X})/n$不是对总体分布方差$\sigma^2$无偏估计量，而样本的方差$S^2 = \sum_{i=1}^n(X_i-\bar{X})/(n-1)$则是对总体分布方差$\sigma^2$的无偏估计量。下面通过一个实验来表明$S^2$和$m_2$在大数定理下的收敛情况。

```{r unbias_caculate2,eval=T, echo=F, warning=F, error=F, fig.height=3, fig.width=4}
set.seed(1234)
true_sd <- 2
simsteps <- 100000
sample_cap <- 10
sample_matrix <- matrix(rnorm(simsteps*sample_cap, sd=true_sd), ncol=sample_cap)
sample_sd <- apply(sample_matrix, 1, var)
sample_m2 <- sample_sd * (sample_cap-1) / sample_cap

meanstep <- seq(100, simsteps, 100)
sd_mean <- vector(mode="numeric", length=length(meanstep))
m2_mean <- sd_mean
for (i in c(1:length(meanstep))){
  index <- meanstep[i]
  sd_mean[i] <- mean(sample_sd[1:index])
  m2_mean[i] <- mean(sample_m2[1:index])
}
```

```{r unbias_fig2,eval=T, echo=F, warning=F, error=F, fig.height=1.5, fig.width=4}
fig_sd <- ggplot(data=data.frame(meanstep, sd_mean)) + geom_line(aes(meanstep, sd_mean)) + geom_hline(yintercept = true_sd^2, col="green") + xlab(TeX("$N$")) + ylab(TeX("$\\bar{S^2}$")) + scale_x_continuous(labels=c("0", "", "50000", "", "100000"))
fig_m2 <- ggplot(data=data.frame(meanstep, m2_mean)) + geom_line(aes(meanstep, m2_mean)) + geom_hline(yintercept = true_sd^2, col="green") + xlab(TeX("$N$")) + ylab(TeX("$\\bar{m_2^2}$")) + geom_hline(yintercept = true_sd^2 * (sample_cap-1) / sample_cap, col="red")+ scale_x_continuous(labels=c("0", "", "50000", "", "100000"))

grid.newpage()
pushViewport(viewport(layout = grid.layout(1, 2)))
print(fig_sd, vp=vplayout(1, 1))
print(fig_m2, vp=vplayout(1, 2))
```

对正态总体$N(0,4)$进行抽样，样本大小为`r sample_cap`，重复抽样次数1-100000次，每进行$N$次抽样，就计算出$S^2$和$m_2$在这$N$次抽样下的均值，最后就得到了抽样次数在1-100000下的两个统计量的均值。以均值和抽样次数作折线图，得到上图

可以发现，随着抽样实验的次数的增多，$\bar{S^2}$的值越来越接近真实的值$4$，而$\bar{m_2}$明显收敛到$3.6$，这说明如果用$m_2$估计总体方差$\sigma^2$只会偏小。

\subsubsection{最小方差无偏估计}
除了用无偏性来评价估计量外，也使用*最小方差*这一概念来评价一估计量，首先给出*均方误差*的定义：
\begin{Definition}
设$X_1,\cdots,X_n$是从某一参数$\theta$的总体中抽出的样本，若有估计量$\hat{\theta}=\hat{\theta}(X_1,\cdots,X_n)$，则这个估计量与真实值$\theta$的误差为$\hat{\theta}(X_1,\cdots,X_n)-\theta$，显然误差本身也是由样本决定的，是一个随机变量，对其平方并取均值，则有：
\begin{equation}
M_{\hat{\theta}}(\theta) = E_{\theta}[\hat{\theta}(X_1,\cdots,X_n)-\theta]^2
\end{equation}
上式子中的$M_{\hat{\theta}}(\theta)$即称为估计量$\hat{\theta}$的均方误差
\end{Definition}

显然，均方误差越小的估计量越优秀，根据方差的一个定理：

\begin{equation}
Var(X) = E(X - EX)^2 = E(X^2) - EX ^2
\end{equation}
可将$M_{\hat{\theta}}(\theta)$改写：
\begin{equation}
\begin{split}
M_{\hat{\theta}}(\theta) &= E_{\theta}(\hat{\theta} - \theta)^2\\
&= E(\hat{\theta}^2) - [E(\hat{\theta})]^2 + [E(\hat{\theta})]^2 + \theta^2 - 2E(\hat{\theta})\theta\\
&= Var_{\theta}(\hat{\theta}) + [E_{\theta}(\hat{\theta}) - \theta] ^2
\end{split}
\end{equation}
这就表明均方误差由$\textrm{Var}_{\theta}(\hat{\theta})$，即估计量$\hat{\theta}$自身方差，另外一部分$[E_{\theta}(\hat{\theta}) - \theta]^2$就是前面描述无偏性的系统误差。
显然，如果$\hat{\theta}$是一个无偏估计量，则描述系统误差的项为0，那么就有：
\begin{equation}
M_{\hat{\theta}}(\theta) = \textrm{Var}_{\theta}(\hat{\theta})
\end{equation}
这个时候均方误差就只是由估计量$\hat{\theta}$的方差来决定。那么，对无偏估计量来说，均方误差的比较久归结为估计量自身的方差的比较，方差较小者较优。这样就能给出*最小方差无偏估计*的定义：
\begin{Definition}
设有$\hat{\theta}$是分布$g(\theta)$的一个无偏估计，若对$g(\theta)$的任何一个无偏估计$\hat{\theta_1}$，$\hat{\theta}$都满足：
\begin{equation}
\textrm{Var}_{\theta}(\hat{\theta}) \leq \textrm{Var}_{\theta}(\hat{\theta_1})
\end{equation}
对$\theta$的任何取值都成立，就把$\hat{\theta}$称为$g(\theta)$的一个最小方差无偏估计（MVU估计）
\end{Definition}
如何寻找MVU估计不在笔记讨论范围，详情见陈希孺《概率论与数理统计》中关于克拉美-劳不等式的讨论。

\subsubsection{估计量的相合性与正态性}
前面讨论的估计量的无偏性是针对有限的样本来引申出的一种性质，例如前面的例子中用的样本数量都是10。设想当样本数量接近无穷时，估计量的两种新的性质就被引申出来，即估计量的*相合性*和*正态性*

1. 相合性

估计量的相合性是对大数定理的一种引申，大数定理是指对$X_1,X_2,\cdots, X_n,\cdots$独立同分布，都属于均值为$\theta$的总体，记$\bar{X_n} = \sum\limits_{i=1}^n X_i/n$，当$n \rightarrow \infty$时，对任一给定$\varepsilon > 0$都有：

\begin{displaymath}
\lim_{n \rightarrow \infty} P(|\bar{X_n} - \theta| \geq \varepsilon) =0
\end{displaymath}

这个定理表明，随着样本容量的增大，估计值$\bar{X_n}$与真实值$\theta$的差距达到某个$\varepsilon$的概率越来越小，以至于趋近于0。换句话说，样本足够大，均值的误差就可以任意小。

把这个定理引申，就得到相合性的定义：
\begin{Definition}
设总体分布参数为$\theta_1,\cdots,\theta_k$，$g(\theta_1,\cdots,\theta_k)$是$\theta_1,\cdots,\theta_k$d的一个函数，若$X_1,\cdots,X_n$是从该总体中抽出的样本，$T(X_1,\cdots, X_n)$是$g(\theta_1,\cdots, \theta_k)$的估计量，对任一$\varepsilon > 0$，有：
\begin{equation}
\lim_{n \rightarrow \infty}P_{\theta_1, \cdots, \theta_k}(|T(X_1,\cdots,X_n) - g(\theta_1,\cdots,\theta_k)| \geq \varepsilon) =0
\end{equation}
对一切$(\theta_1,\cdots,\theta_k)$成立，则称$T(X_1,\cdots, X_n$是$g(\theta_1, \cdots, \theta_k)$的一个相合估计
\end{Definition}
一个具有相合性的估计量表明，只有样本取的足够大，那么估计量与真实值的误差就会越来越小，这是一个很基本性质。下面以一个例子直观展现样本均值$\bar{X}$的相合性

```{r bigsample_cal,eval=T, echo=F, warning=F, error=F, fig.height=1.5, fig.width=4}
set.seed(1234)
step_mean <- seq(1, 1000)
mean_ex <- sapply(step_mean, function(x){mean(rnorm(x))})
fig_meanex <- ggplot(data.frame(step_mean, mean_ex)) + geom_line(aes(step_mean, mean_ex)) + geom_hline(yintercept = 0, col="green") + xlab(TeX("$n$")) + ylab(TeX("$\\bar{X_n}$"))
fig_meanex
```

上面的例子是抽自正态分布$N(0,1)$的样本在样本大小从1-1000的情况，可以看到随着样本容量的变大，样本均值$\bar{X_n}$越来越接近真实值0。这就是所谓的相合性

2. 渐进正态性

估计量$T$是样本的一个函数，自身往往也满足一种分布，但这种分布往往不容易直接求出，类似中心极限定理，当样本容量$n$很大时，可以证明，许多统计量都会趋近于正态分布，这就是所谓的*渐进正态性*。

\subsection{区间估计}
\subsubsection{基本概念}
点估计是使用一个数对总体的参数进行估计，区间估计则是通过样本得到一段区间$[\hat{\theta_1},\hat{\theta_2}]$，让未知参数$\theta$，有很大可能落入这个区间，与此同时也要让区间的长度$\theta_2 - \theta_1$越小越好。显然同时满足以上这两点是不可能的。区间长度长了，$\theta$落入区间的可能性高了，但估计的精度就下去了，反之短的区间虽然有高的精度，但$\theta$落入区间的概率也降低，估计的可靠性不足。由此我们给出一个权衡的定义。
\begin{Definition}
\label{conflevel_def}
给定一很小的数$\alpha > 0$，如果参数的落入估计区间的概率满足：
\begin{equation}
\label{conflevel}
P_{\theta}(\hat{\theta_1}(X_1,\cdots,X_n) \leq \theta \leq \hat{\theta_2}(X_1,\cdots,X_n)) = 1 - \alpha 
\end{equation}
则称区间估计$[\hat{\theta_1}, \hat{\theta_1}]$的置信系数为$1 - \alpha$
\end{Definition}
置信系数$1 - \alpha$量化了区间估计的可靠程度，指该区间有多大概率包含未知参数$\theta$。但在更一般的情况下，无法保证$P_{\theta}(\hat{\theta_1} \leq \theta \leq \hat{\theta_2})$对一切的$\theta$都满足式\eqref{conflevel}，但可保证这个概率不小于$1 - \alpha$，即有：
\begin{equation}
P_{\theta}(\hat{\theta_1}(X_1,\cdots,X_n) \leq \theta \leq \hat{\theta_2}(X_1,\cdots,X_n)) \leq 1 - \alpha 
\end{equation}
此时称$1 - \alpha$为区间$[\hat{\theta_1}, \hat{\theta_2}]$的*置信水平*，一般情况下$\alpha = 0.05$是使用的比较多的情况，也就是说，一般会计算置信水平为$0.95$的区间估计。
\subsubsection{枢轴变量法}
设$X_1,\cdots,X_n$是抽自正态总体$N(\mu, \sigma^2)$的样本，要通过这个样本来求$\mu$的区间估计，分两种情况：

*1. 总体方差$\sigma^2$已知*

显然，样本的均值$\bar{X}$是一个对总体均值的良好估计，接着构造一个新的随机变量，它显然满足：
\begin{equation}
\label{quzhou}
\frac{\sqrt{n}(\bar{X} - \mu)}{\sigma} \sim N(0, 1)
\end{equation}
为了求出$\mu$的区间估计，先引入*分位点*的概念。
\begin{Definition}
以$\Phi(x)$记$N(0,1)$的分布函数，对一个较小的$0 < \beta < 1$，若$u_\beta$满足方程：
\begin{equation}
\Phi(u_\beta) = 1-\beta
\end{equation}
则称$u_{\beta}$是分布$N(0, 1)$的上$\beta$分位点。
\end{Definition}

```{r uptail,eval=T, echo=F, warning=F, error=F, fig.height=1.5, fig.width=4}
x_norm <- seq(-3, 3, 0.1)
y_norm <- dnorm(x_norm)
u_0.95 <- qnorm(0.95)
u_0.05 <- qnorm(0.05)

fig_norm <- ggplot(data.frame(x_norm, y_norm)) + geom_line(aes(x_norm, y_norm),col="blue", size=1) + geom_vline(xintercept = u_0.95) + geom_vline(xintercept = u_0.05) + scale_x_continuous(labels = c("", TeX("$u_{0.05}$"),"", "0", "", TeX("$u_{0.95}$"), ""), breaks = c(-3, -2, -1, 0, 1, 2, 3)) + xlab(TeX("$x$")) + ylab(TeX("$\\Phi(x)$"))
fig_norm
```

可以看到，如果分布是关于原点对称的，显然有$u_{1-\alpha} =  -u_{\alpha}$。这是一个很有用的性质。

式\eqref{quzhou}中的随机变量$\sqrt{n}(\bar{X} - \mu)/\sigma$满足$N(0, 1)$，并且它的分布类型与$\mu$无关，因此我们可以通过这个变量来换算得到$\mu$的置信区间。根据定义\eqref{conflevel_def}，写出：
\begin{displaymath}
\begin{split}
P(-u_{\alpha/2} \leq \frac{\sqrt{n}(\bar{X}-\mu)}{\sigma} \leq u_{\alpha/2}) &= \Phi(u_{\alpha/2}) - \Phi(-u_{\alpha/2})\\
&= \Phi(u_{\alpha/2}) - (1 - \Phi(u_{\alpha/2}))\\
&= 1 - \alpha
\end{split}
\end{displaymath}

对上面的随机变量$\sqrt{n}(\bar{X}-\mu)/{\sigma}$及其两端$u_{\alpha/2}$稍作改写即可转换成关于$\mu$的置信区间的定义：
\begin{displaymath}
P(\bar{X} - \frac{\sigma u_{\alpha/2}}{\sqrt{n}}) \leq \mu \leq P(\bar{X} + \frac{\sigma u_{\alpha/2}}{\sqrt{n}}) = 1 - \alpha
\end{displaymath}

这样就得到了$\mu$的置信系数$1-\alpha$的置信区间为
\begin{equation}
\label{muconf}
[\hat{\theta_1}, \hat{\theta_2}] = [\bar{X} - \frac{\sigma u_{\alpha/2}}{\sqrt{n}},\bar{X} + \frac{\sigma u_{\alpha/2}}{\sqrt{n}}]
\end{equation}

由上面的例子可以看出随机变量$\sqrt{n}(\bar{X} - \mu)/\sigma$的作用，它起一个“中转枢纽”的作用，首先它包含未知参数$\mu$，但又有一个确切已知的分布形式，通过这个分布我们能写出符合定义\eqref{conflevel_def}的表达式，通过变形最终得到了$mu$的置信区间。

*2. 总体方差$\sigma^2$未知*

在总体方差未知时，枢轴变量中的$\sigma$就无法使用，我们使用样本标准差$S$来代替，这样数轴变量就变成了$\sqrt{n}(\bar{X} - \mu)/S$，经过一些证明可以知道它服从自由度为$n-1$的$t$分布，由于$t$分布同样和正态分布一样关于原点对称，因而可以继续沿用1.中的归纳方法，得出此情况下$\mu$的$1-\alpha$置信区间为：
\begin{equation}
\label{muconf_nsigma}
[\hat{\theta_1}, \hat{\theta_2}] = [\bar{X} - \frac{S t_{n-1}(\alpha/2)}{\sqrt{n}},\bar{X} + \frac{S t_{n-1}(\alpha/2)}{\sqrt{n}}]
\end{equation}

\section{假设检验}

\subsection{问题的提法和基本概念}

\subsubsection{问题的提法}

我们常常需要对取自总体的样本的一些参数，如均值，标准差进行判断（检验），设有如下问题：

$X_1, X_2, \cdots, X_n$是取自一正态分布总体$N(\theta, \sigma^2)$的样本，现在需根据这个样本来判断：总体的均值是否有$\theta \geq \theta_0$？

在这里$\theta$是一个未知的随机变量，命题$\theta \geq \theta_0$则称为一个“统计假设”，假设检验就是围绕这一事先预定好的假设来进行的。

由此引出问题的提法中的几个重要概念：

*1. 原假设和对立假设*

常把被判断或检验的假设称之为原假设，原假设记为$H_0$，上面的例子中原假设可以这样表述：

\begin{displaymath}
H_0: \{\theta | \theta \geq \theta_0\}
\end{displaymath}

另外一个与原假设对立的假设称为“对立假设”或“备择假设”，它与零假设互补组成整个假设空间，常把对立假设记为$H_1$，上面的例子的对立假设可以这样表述：

\begin{displaymath}
H_1: \{\theta | \theta < \theta_0\}
\end{displaymath}

原假设和对立假设成对出现，并且概率上是互斥事件，不会同时发生，假设检验的工作就是判断原假设和对立假设的可能性，并最后选择一个假设来接受。由此引出如下概念。

*2. 检验统计量、接受域、否定域、临界域和临界值*

根据上面的检验问题，提出的假设（简写）：

\begin{displaymath}
\begin{split}
&H_0: \theta \geq \theta_0 \\
&H_1: \theta < \theta_0
\end{split}
\end{displaymath}

由于在上面的假设中，$\theta$是一个未知的参数，我们需要一个统计量$Z$来估计$\theta$，然后看这个统计量$Z$是否满足上面的统计假设。直观上，样本的均值$\bar{X}$是一个不错的用来估计总体$\theta$的统计量，于是我们的统计假设问题就变成了这样一个检验流程：

\begin{displaymath}
\begin{split}
&\mbox{如果}\bar{X} \geq C,\mbox{接受}H_0\\
&\mbox{如果}\bar{X} < C,\mbox{拒绝}H_0
\end{split}
\end{displaymath}

很明显，如果我们计算得到的$\bar{X}$有$\bar{X} \geq C$，此时原假设$H_0$正确，我们接受它，相反，如果$\bar{X} < C$，此时原假设$H_0$错误，我们否定它并接受备择假设$H_1$。以上的这一套操作准则，就称之为对$\theta$的*检验*。这里用的$\bar{X}$就是一个*检验统计量*

正如$\bar{X} = \frac{\sum_{i=1} ^ {n} X_i}{n}$，检验统计量由样本$X_1, \cdots, X_n$决定，这样一来，原假设是否接受也由样本来决定。把使得原假设接受的那些样本组成的区域成为这个检验的*接受域*，而把使原假设被否定的那些样本组成的区域则成为检验的*否定域*。

这里把接受域和否定域记为：

\begin{displaymath}
\begin{split}
A &= \{(X_1, \cdots, X_n)| X_1 + \cdots + X_n \geq nC\}\\
R &= \{(X_1, \cdots, X_n) | X_1 + \cdots + X_n < nC\}
\end{split}
\end{displaymath}

可见，当这里的检验中$C$有重要作用，当$\bar{X}$的值越过这一个界限，结论就由接受变成否定，这个$C$称之为检验统计量$\bar{X}$的*临界值*。

### 功效函数

原假设的接受与否要看检验统计量是否越过了临界值，而检验统计量是由样本决定的，由于样本的随机性，检验统计量是一个随机变量。因此上面的例子中$\bar{X}$就是一个随机变量。因此，可以知道，原假设$H_0: \theta \geq \theta_0$被否定的概率为：

\begin{equation}
\beta_{\Phi}(\theta) = P_{\theta}(\bar{X} < C)\label{gongxiao}
\end{equation}

由于样本$X_1,\cdots, X_n$之间独立，且来自同一总体$N(\theta, \sigma^2)$，所以由正态分布的性质：若$X_1, \cdots, X_n$相互独立，且分别服从正态分布$N(\theta_1, \sigma_1^2), \cdots, N(\theta_n, \sigma_n^2)$，那么随机变量$X_1 + \cdots + X_n$的分布满足正态分布$N(\theta_1 + \cdots + \theta_n, \sigma_1^2 + \cdots + \sigma_n^2)$，可以对\eqref{gongxiao}进行化简。

\begin{equation}
\begin{split}
\beta_{\Phi}(\theta) &= P_{\theta}(\bar{X} < C)\\
&= P_{\theta}(X_1 + X_2 + \cdots + X_n < nC)\\
&= \Phi_{(n\theta, n\sigma^2)}(nC)\label{gongxiaohanshu}
\end{split}
\end{equation}

可以看到，式\eqref{gongxiaohanshu}就是参数为$N(n\theta, n\sigma^2)$的正态分布的累积分布函数，这个函数的取值完全和$\theta$与$\sigma$有关，这里假设$\sigma$是已知的，那么就只和$\theta$有关了。

式\eqref{gongxiaohanshu}就是一个检验$\Phi$的功效函数，由此给出功效函数的一般定义：

\begin{Definition}
\label{gongxiaodingyi}
设总体分布包含若干个未知参数$\theta_1,\cdots, \theta_k$，$H_0$是关于这些参数的一个原假设，设有了样本$X_1,\cdots,X_n$，而$\Phi$是针对这些样本而做的一个检验，则称检验$\Phi$的功效函数为:
\begin{equation}
\beta_{\Phi} = P_{\theta_1, \cdots, \theta_k}(\Phi: \mbox{拒绝}\quad H_0)\label{beta_phi}
\end{equation}
\end{Definition}

可见，式\eqref{beta_phi}是$\theta_0,\cdots,\theta_k$的函数。

功效函数$\beta_{\Phi}(\theta_1,\cdots,\theta_k)$计算的是在检验$\Phi$下，原假设$H_0$被否定的概率，另一种方式就是说假设$H_1$被接受的概率，我们希望功效函数最好能满足如下性质：

如果能有一组$\theta_0,\cdots,\theta_k$使得$H_0$成立，也就是说当$H_0$为真，$H_1$位假，我们不希望否定$H_0$,，那么我们希望$\beta_{\Phi}(\theta_1,\cdots,\theta_k)$能尽量小，这样就能尽量小的概率否定$H_0$。反过来，若有一组$\theta_0,\cdots,\theta_k$使得$H_1$成立，也就是说当$H_0$为假，$H_1$位真时，我们不希望接受$H_0$，那么我们希望，我们希望$\beta_{\Phi}(\theta_1,\cdots,\theta_k)$能尽量的大，这样就能以尽量大的概率否定$H_0$，或者说接受$H_1$。

值得注意，“功效”一词在$\theta_1, \cdots, \theta_k$使得对立假设成立时更为恰当，因当$\theta_1, \cdots, \theta_k$属于对立假设时，$\beta_{\Phi}(\theta_1,\cdots,\theta_k)$应当尽可能大。而当$\theta_1,\cdots,\theta_k$属于原假设时，$\beta_{\Phi}(\theta_1,\cdots,\theta_k)$取小值，这时叫做“功效”就不恰当。

\subsubsection{两类错误，检验的水平}

通过功效函数$\beta_{\Phi}$来对假设$H_0$进行检验时，由于检验的结果是一个概率事件，那么可能出现这两种情况：

1. $H_0$实际是正确的（$H_1$实际上错误），但我们却否定了$H_0$。
2. $H_0$实际是错误的（$H_1$实际上正确），但我们却接受了$H_0$。

由于检验结果是和总体的分布的参数$N(\theta,\sigma^2)$有关的，因此犯错误的类型也和这些参数有关，这一点在后面会详细解释。由此总结出第一类错误和第二类错误的数学形式：

\begin{equation}
\begin{split}
&\alpha_{1\Phi}(\theta_1,\cdots,\theta_k) =
\left\{
\begin{split}
&\beta_{\Phi}(\theta_1,\cdots,\theta_k),\quad (\theta_1,\cdots,\theta_k) \in H_0\\
&0,\quad (\theta_1,\cdots,\theta_k) \in H_1
\end{split}
\right.\\
&\alpha_{2\Phi}(\theta_1,\cdots,\theta_k) =
\left\{
\begin{split}
&0,\quad (\theta_1,\cdots,\theta_k) \in H_0\\
&1 - \beta_{\Phi}(\theta_1,\cdots,\theta_k),\quad (\theta_1,\cdots,\theta_k) \in H_1
\end{split}
\right.
\end{split}
\end{equation}

一般的，更希望一类错误的概率更小，这时会定出一个$\alpha$来限定一类错误小于它，再在此基础上尽量减少第二类错误。由此给出*检验水平*的定义

\begin{Definition}
\label{jianyanshuiping}
设$\Phi$是原假设$H_0$下的一个检验，$\beta_{\Phi}(\theta_1,\cdots,\theta_k)$为其功效函数，$\alpha$满足$\alpha \in [0, 1]$，如果：
\begin{equation}
\label{jysp-eq}
\beta_{\Phi}(\theta_1,\cdots,\theta_k) \leq \alpha \quad (\forall (\theta_1,\cdots,\theta_k) \in H_0)
\end{equation}
则称$\Phi$为$H_0$的水平为$\alpha$的检验。
\end{Definition}

可以看到水平$\alpha$实际上指是功效函数$\beta_{\Phi}(\theta_1,\cdots,\theta_k)$的值在区间$H_0$上永远小于我们给定的$\alpha$（说明当$\theta_1,\cdots, \theta_k$的真实值确实是$H_0$时，拒绝它的概率是小于$\alpha$的），如果这个条件不成立，那么$\alpha$就不是这个检验$\Phi$在$H_0$上的一个水平了。（功效函数的目的就是首先保证一类错误要小于一定水平$\alpha$）。

\subsubsection{一致最优检验}

检验的水平让检验函数在$(\theta_0,\cdots,\theta_k) \in H_0$时小于指定的$\alpha$，使得第一类错误发生的概率小于$\alpha$。同样的，当$(\theta_0,\cdots,\theta_k) \in H_1$时，我们可以给出一致最优检验的定义：

\begin{Definition}
\label{yizhizuiyou}
设$\Phi$为一个水平为$\alpha$的检验，若对其他任何一个水平为$\alpha$的检验$g$，必有
\begin{equation}
\beta_{\Phi}(\theta_1,\cdots,\theta_k) \geq \beta_{g}(\theta_0,\cdots,\theta_k) \quad (\forall (\theta_0,\cdots,\theta_k) \in H_1)
\end{equation}
\end{Definition}
可以看到，根据\eqref{yizhizuiyou}的定义，一致最优检验检验不仅仅在$H_0$上满足水平$\alpha$的检验，在$H_1$上，根据第二类错误概率定义$\alpha_{2 \Phi}(\theta_0,\cdots,\theta_k) = 1 - \beta_{\Phi}(\theta_0,\cdots,\theta_k)$，它发生二类错误的概率也是所有水平为$\alpha$的检验中最小的。

用图来描述，假设有三个功效函数$\Phi_1,\Phi_2,\Phi_3$，他们都是针对假设$H_0:\theta > \theta_0,H_1:\theta \leq \theta_0$的两个检验，如果$\Phi_1$是一致最优检验，那么它的功效函数必然满足如下样子：

```{r scatter,eval=T, echo=F, warning=F, error=F, fig.height=3, fig.width=4}
par(mar = c(2, 2, .1, .1), las = 1)
th0 <- 20
x <- c(-100:100)
y1 <- -pnorm(x, mean=th0, sd=20) + 0.5
y2 <- -pnorm(x, mean=th0, sd=30) + 0.5
y3 <- -pnorm(x, mean=th0, sd=40) + 0.5

data <- data.frame(x, y1, y2, y3)
fig <- ggplot(data) + geom_line(aes(x, y1), colour="red",size=1.5) + geom_line(aes(x, y2), colour="blue") + geom_line(aes(x, y3), colour="green") + geom_vline(xintercept = th0) +scale_x_continuous(breaks=c(-100, -50, 0, th0, 50, 100), labels = c("", TeX("$H_1:\\theta \\leq \\theta_0$"), "0", TeX("$\\theta_0$"), TeX("$H_0:\\theta > \\theta_0$"), "")) + scale_y_continuous(breaks=c(0.5, 0, -0.5), labels=c("", TeX("$\\alpha$"), "")) + xlab(TeX("$\\theta$")) + ylab(TeX("$\\beta_{\\Phi}(\\theta)$"))
fig
```

可以看到，三个功效函数在$\theta>\theta_0$处都满足水平为$\alpha$的检验，而在$\theta \leq \theta_0$处，标红色的功效函数明显取更大的$\beta_{\Phi}(\theta)$，也就是说它第二类错误的概率是三个功效函数中最小的，此时才能称之为*一致最优检验*

一直最优检验要求功效函数在$H_1$上处处出现第二类错误概率最小，这是个很严格的条件。实际上许多针对同一假设的检验之间无法找到一个一致最优检验，即它们之中的任何一个都不能达到“处处”最小的要求，反而是，在某一区间内，$\Phi_1$是最小的，而到了另外一个区间内$\Phi_2$是最小的。

\subsection{重要参数检验}

基于参数的点估计是一个直观的检验方法，因为要检验的参数是未知量，往往通过样本来得出一个参数的估计量用做检验，点估计是最常用的检验统计量。下面给出在已知总体分布情况下，对总体的未知参数设计检验的过程。

\subsubsection{正态总体均值的检验}

$X_1, X_2, \cdots, X_n$是取自一正态分布总体$N(\theta, \sigma^2)$的样本，针对总体的均值，先定一个$\theta_0$，有常见如下假设检验问题：

1. $H_0:\theta \geq \theta_0$，$H_1:\theta < \theta_0$；
2. $H_0:\theta \leq \theta_0$，$H_1:\theta > \theta_0$；
3. $H_0:\theta = \theta_0$，$H_1:\theta \neq \theta_0$；

问题1和问题2形式上接近，问题3则有略微的不同，先对问题1设计检验，这里有两种情况：总体方差已知和总体方差未知。

*1.方差$\sigma^2$已知*

如果问题如上文1中所示，我们用点估计$\bar{X}$来估计总体的方差，直观上，我们可认为$\bar{X}$越大的情况，越有可能满足$H_0$，至于$\bar{X}$要大到什么程度，这里用$C$来表示，这个$C$是在一开始不知道的量。由此提出如下一个检验方法
\begin{equation}
\Phi: \mbox{当}\bar{X} \geq C,\mbox{接受原假设},\mbox{当}\bar{X} < C\mbox{否定原假设}
\end{equation}

接着给出这个检验的功效函数，考虑功效函数的定义\eqref{gongxiaodingyi}，又因总体分布为$N(\theta, \sigma^2)$的分布，这时统计量$\frac{\sqrt{n}(\bar{X} - \theta)}{\sigma} \sim N(0, 1)$，于是功效函数可以化为：
\begin{equation}
\begin{split}
\beta_{\Phi}(\theta)&=P_{\theta}(\bar{X} < C)\\
&= P_{\theta}(\frac{\sqrt{n}(\bar{X} - \theta)}{\sigma} < \frac{\sqrt{n}(C-\theta)}{\sigma})\\
&= \Phi(\frac{\sqrt{n}(C-\theta)}{\sigma})
\end{split}
\end{equation}

若要让功效函数满足其检验水平为$\alpha$，即想让我们这个检验第一类错误概率为$\alpha$，根据其定义\eqref{jianyanshuiping}可知，这个函数$\Phi(\frac{\sqrt{n}(C-\theta)}{\sigma})$和$\theta$有关，当$\theta$增加，函数值变小，可见函数$\Phi$在$\theta \in H_0:\{\theta|\theta \geq \theta_0\}$上单调递减，由此给出下式：

\begin{equation}
\label{lingjiezhi}
\begin{split}
&\beta_{\Phi}(\theta)=\Phi(\frac{\sqrt{n}(C-\theta)}{\sigma}) \leq \alpha\quad \forall \theta \in H_0:\{\theta|\theta \geq \theta_0\}\\
&\mbox{相当于}\\
&\frac{\sqrt{n}(C-\theta)}{\sigma} \leq u_{1-\alpha}\\
&\mbox{让}\theta = \theta_0\\
&\frac{\sqrt{n}(C-\theta_0)}{\sigma} = u_{1-\alpha} = -u_{\alpha}\\
&\mbox{由此可得：}\\
&C = \theta_0 - \frac{\sigma u_{\alpha}}{\sqrt{n}}\\
\end{split}
\end{equation}

这样就得到了功效函数$\beta_{\Phi}(\theta)$在水平$\alpha$下检验统计量$\bar{X}$的临界值$C$，把$C$带回，得到检验$\Phi$的功效函数的完整形式。
\begin{equation}
\label{wanzhenggongxiao}
\beta_{\Phi}(\theta) = \Phi(\frac{\sqrt{n}(\theta_0-\theta)}{\sigma} - u_{\alpha})
\end{equation}

下面给出一个例子，并绘制出这个例子的功效函数。
```{r sample1, eval=T, echo=F, warning=F, message=F}
set.seed(1234)
th0 <- 10
th_real <- 11
th_set <- seq(0, 20, 0.1)
sd_real <- 1
n <- 10
alpha <- 0.05
u <- qnorm(alpha, lower.tail = F)
x_sample <- rnorm(n, mean=th_real, sd=sd_real)
z_stat <- sqrt(n)*(th0 - th_set) - u
c <- th0 - (sd_real*u/sqrt(n))
y <- pnorm(z_stat)
```
\begin{lemma}
已知样本`r x_sample`来自总体$N(\theta, 1)$，考虑假设检验问题$H_0:\theta \geq 10,H_1:\theta < 10$,在检验水平$\alpha=0.05$情况下的情况。
\end{lemma}
根据\eqref{lingjiezhi}，直接计算临界值$C$:
\begin{displaymath}
\begin{split}
C &= \theta_0 - \frac{\sigma u_{0.05}}{\sqrt{n}}\\
&= 10 - \frac{1 \times 1.644854}{\sqrt{10}} = `r c`
\end{split}
\end{displaymath}

显然，样本的均值$\bar{X}$为`r mean(x_sample)`，满足$\bar{X} \geq `r c`$，接受原假设$H_0: \theta \geq 10$，事实上，以上的数据取自正态分布$N(`r th_real`, `r sd_real`)$，检验的结果符合总体的分布情况。

下面考虑更一般的，绘制出该检验下的功效函数图像：

```{r lizitu, eval=T, echo=F, warning=F, message=F, error=F,fig.height=3, fig.width=5.5}
fig_lizitu <- ggplot(data = data.frame(th_set, y)) + geom_line(aes(th_set, y), colour="blue", size=0.8) + geom_hline(yintercept = 0.05) + geom_hline(yintercept = 0.9, colour="red") + geom_vline(xintercept = 9.1, colour="red")
fig_lizitu <- fig_lizitu + xlab(TeX("$\\theta$")) + ylab(TeX("$\\beta_{\\Phi}(\\theta)$"))
x_label_h1 <- TeX("$H_1:\\theta<10$")
x_label_h0 <- TeX("$H_0:\\theta \\geq 10$")
x_label_th0 <- TeX("$\\theta_0=10$")
x_label_th1 <- TeX("$\\theta_1$")
fig_lizitu <- fig_lizitu + scale_y_continuous(breaks = c(0.00, 0.05, 0.25, 0.5, 0.75, 0.90, 1.00), labels=c("0.00", TeX("$\\alpha$"), "0.25", "0.5", "0.75", TeX("$1 - \\beta$"), "1.00"))
fig_lizitu <- fig_lizitu + scale_x_continuous(labels = c("", x_label_h1, x_label_th1, x_label_th0, x_label_h0, ""), breaks=c(0.0, 5.0, 9.1, 10.0, 15.0, 20.0))
fig_lizitu
```

仔细观察功效功效函数和它的图像，可以得出如下结论

1. 当分布的真实值$\theta$越大时，越容易满足零假设$H_0:\theta\geq 10$，此时功效函数急剧下降，最后趋近于零，说明在越大的$\theta$下，零假设被拒绝的概率是十分小，犯第一类错误的概率十分小。相反的，越小的$\theta$，越容易远离零假设$H_0$，而满足备择假设$H_1:\theta < 10$，此时功效函数急剧上升，说明在越小的$\theta$下，零假设会以很大的概率被拒绝，犯第二类错误的概率也很小（$\alpha_{2\Phi}(\theta)=1-\beta_{\Phi}(\theta)$）。

2. 当分布的真实值$\theta < 10$，但$\theta$与10差距很小时，可以看到功效函数在10附近的变化很急剧，此时的$\beta_{\Phi}(\theta)$与我们预先设计的水平$\alpha=0.05$，接近，这是犯第二类错误的概率就比较大了$\alpha_{2\Phi}(\theta)\simeq 1-0.05=0.95$。可见若$\theta$很接近我们预先设定$\theta_0$，功效函数的性能就不佳了，此时虽然能保证第一类错误的概率是一个很低的值，但第二类错误的概率却大大上升了。

3. 如果提出要求，让犯第二类错误的概率小于指定的$\beta$，即让$H_1$正确，但被拒绝的概率要小于$\beta$。这等价于有:

\begin{equation}
\label{erleigongxiao}
1 - \beta_{\Phi}(\theta) < \beta \Leftrightarrow \beta_{\Phi}(\theta) \geq 1 - \beta \m (\forall \theta \in {\theta < \theta_0})
\end{equation}

我们一般希望$\beta$取一个很小的值，比如$0.1$，但实际上根据函数的性质，这一点在整个$H_1$上无法实现。当$\theta < \theta_0$但十分接近$\theta_0$时，$\beta_{\Phi}(\theta) \simeq \alpha$，显然由于$\alpha$和$\beta$都是一个很小的值，那么$\beta_{\Phi}(\theta) \simeq \alpha \geq 1-\beta$是无法实现的。但是如果稍微放宽一点限制，让$\theta$小于某一个$\theta_1 < \theta_0$，因为函数在$\theta_0$附近急剧变化，稍小一点的$\theta < \theta_1$能让$\beta_{\Phi}(\theta) \geq 1 - \beta$很容易满足。

$\beta_{\Phi}(\theta)$中另外两个重要参数是总体分布的$\theta$和$\sigma$，在固定$\theta$情况下，它们对检验的结果有重要影响，下面给出它们的函数图像

```{r eval=T, echo=F, warning=F, message=F, error=F,fig.height=2.5, fig.width=4}
#set.seed(1234)
th0 <- 10
th_real <- 8
sd_set <- seq(1, 10, 0.1)
n <- 10
alpha <- 0.05
u <- qnorm(alpha, lower.tail = F)
z_stat <- (sqrt(n)*(th0 - th_real)/sd_set) - u
y <- pnorm(z_stat)

fig_sd_zstat <- ggplot(data=data.frame(sd_set, z_stat)) + geom_line(aes(x=sd_set, z_stat), colour = "blue") + xlab(TeX("$\\sigma$")) + ylab(TeX("$Z$")) + scale_x_continuous(labels = c("1", "", "5.0", "", "10.0"))
fig_zstat_y <- ggplot(data=data.frame(z_stat, y)) + geom_line(aes(z_stat, y), colour="blue")+xlab(TeX("$Z$")) + ylab(TeX("$\\beta_{\\Phi}(\\theta)$"))
fig_sd_y <- ggplot(data=data.frame(sd_set, y)) + geom_line(aes(sd_set, y), colour="blue") + xlab(TeX("$\\sigma$")) + ylab(TeX("$\\beta_{\\Phi}(\\theta)$")) + scale_x_continuous(labels = c("1", "", "5.0", "", "10.0"))

grid.newpage()
pushViewport(viewport(layout=grid.layout(1, 3)))

print(fig_sd_zstat, vp=vplayout(1, 1))
print(fig_zstat_y, vp=vplayout(1, 2))
print(fig_sd_y, vp=vplayout(1, 3))
```

1. 由上面的三幅图像可知，对某个固定的$\theta < 10$，$\sigma$越大，$Z=\frac{\sqrt{n}(\theta_0-\theta)}{\sigma}-u_{\alpha}$越小，由第二图，分布函数是增函数，可知$\beta_{\Phi}(\theta)$越小。越有可能出现第二类错误。这说明误差越大，$\theta$与$\theta_0=10$越不容易分清，这样就越容易犯错。

```{r eval=T, echo=F, warning=F, message=F, error=F,fig.height=2.5, fig.width=4}
#set.seed(1234)
th0 <- 15.5
th_real <- 15
sd_real <- 1
n <- 10
alpha_set <- seq(0.01, 0.99, 0.01)
u_set <- qnorm(alpha_set, lower.tail = F)
z_stat <- (sqrt(n)*(th0 - th_real)/sd_real) - u_set
y <- pnorm(z_stat)

fig_alpha_zstat <- ggplot(data=data.frame(alpha_set, z_stat)) + geom_line(aes(x=alpha_set, z_stat), colour = "blue") + xlab(TeX("$\\alpha$")) + ylab(TeX("$Z$")) + scale_x_continuous(labels = c("0.0", "", "0.5", "", "1.0"))
fig_zstat_y <- ggplot(data=data.frame(z_stat, y)) + geom_line(aes(z_stat, y), colour="blue")+xlab(TeX("$Z$")) + ylab(TeX("$\\beta_{\\Phi}(\\theta)$"))
fig_alpha_y <- ggplot(data=data.frame(alpha_set, y)) + geom_line(aes(alpha_set, y), colour="blue") + xlab(TeX("$\\alpha$")) + ylab(TeX("$\\beta_{\\Phi}(\\theta)$"))+scale_x_continuous(labels = c("0.0", "", "0.5", "", "1.0"))

grid.newpage()
pushViewport(viewport(layout=grid.layout(1, 3)))

print(fig_alpha_zstat, vp=vplayout(1, 1))
print(fig_zstat_y, vp=vplayout(1, 2))
print(fig_alpha_y, vp=vplayout(1, 3))

```

2. 增大功效$\alpha$，$u_{\alpha}$就变小了，这样$\beta_{\Phi}(\theta)$就更大，所以这种情况下出现一类错误的可能性就变大了，反过来，二类错误的可能性就小了。可见$\alpha$的选择无法兼顾让两种都小，小的$\alpha$使得第一类错误很小，但与此同时出现第二类错误的可能就变大了。

问题2$H_0:\theta \leq \theta_0$，$H_1:\theta > \theta_0$的检验和1十分相似，这里不展开讨论，下面开始讨论问题3即$H_0:\theta = \theta_0,H_1:\theta \neq \theta_0$的检验。直观上看，仍然采用点估计$\bar{X}$来做检验统计量，我们可以认为当$\bar{X}$与$\theta_0$比较接近时，$H_0$成立，反之，距离较远则否定$H_1$，由此得出问题3的一个检验法：

\begin{equation}
\label{shuangcejianyan}
\Phi^{''}:\mbox{当}|\bar{X} - \theta_0| \leq C \mbox{时接受}H_0,\mbox{反之则否定}H_0.
\end{equation}

根据功效函数的定义\eqref{gongxiaodingyi}，直接写出检验$\Phi^{''}$的功效函数：

\begin{equation}
\label{shuangcegongxiao}
\begin{split}
\beta_{\Phi^{''}}(\theta) &= P_{\theta}(|\bar{X}-\theta| > C)\\
&= P_{\theta}(\{\frac{(\bar{X}-\theta)\sqrt{n}}{\sigma} > \frac{C\sqrt{n}}{\sigma}\} \cup \{\frac{(\bar{X}-\theta)\sqrt{n}}{\sigma} < -\frac{C\sqrt{n}}{\sigma}\})\\
&= 1 - \Phi(\frac{C\sqrt{n}}{\sigma}) + \Phi(-\frac{C\sqrt{n}}{\sigma})\\
&= 2 - 2\Phi(\frac{C\sqrt{n}}{\sigma})
\end{split}
\end{equation}

接着，根据检验水平的定义\eqref{jianyanshuiping}，写出在水平$\alpha$下的临界值$C$的推导过程：

\begin{displaymath}
\begin{split}
&\Phi(\frac{C\sqrt{n}}{\sigma}) = 2 - 2\Phi(\frac{C\sqrt{n}}{\sigma}) \leq \alpha \quad \forall \theta \in H_0:\{\theta|\theta = \theta_0\}\\
&\mbox{相当于}\\
&\Phi(\frac{C\sqrt{n}}{\sigma}) \geq 1 - \frac{\alpha}{2}\\
&\mbox{由与函数}\Phi \mbox{单调递增}可知\\
& \frac{C\sqrt{n}}{\sigma} = u_{\frac{\alpha}{2}}
\end{split}
\end{displaymath}

由此得出临界值$C$的值为：

\begin{equation}
C = \frac{u_{\frac{\alpha}{2}}\sigma}{\sqrt{n}}
\end{equation}

把$C$带回\eqref{shuangcejianyan}中，得到$\bar{X}$的接受域为
\begin{equation}
\theta_0 - \frac{\sigma u_{\frac{\alpha}{2}}}{\sqrt{n}} \leq \bar{X} \leq \theta_0 + \frac{\sigma u_{\frac{\alpha}{2}}}{\sqrt{n}}
\end{equation}

只要$\bar{X}$落入这个范围，我们就接受$H_0:\theta=\theta_0$，否则就拒绝$H_0$。

把$C$带入功效函数\eqref{shuangcegongxiao}中，可以得到$\Phi^{''}$的功效函数：
\begin{equation}
\begin{split}
\beta_{\Phi^{''}}(\theta) &= 2 - 2\Phi(u_{\frac{\alpha}{2}})\\
&= 2 - 2(1 - \frac{\alpha}{2})\\
&= \alpha
\end{split}
\end{equation}

很显然，功效函数不含未知的参数$\theta$，也与样本没有关系，只和我们预先指定的水平$\alpha$有关，这说明在这种检验下，*一致最优检验*是不存在的。

*2.方差$\sigma^2$未知*

首先我们看方差已知时考虑检验问题1的情形，由\eqref{lingjiezhi}，检验$\Phi$写成:

\begin{displaymath}
\Phi: \mbox{当}\bar{X} \geq \theta_0 - \frac{\sigma u_{\alpha}}{\sqrt{n}},\mbox{接受原假设},\mbox{当}\bar{X} < \theta_0 - \frac{\sigma u_{\alpha}}{\sqrt{n}}\mbox{否定原假设}
\end{displaymath}

变换一下形式得到：
\begin{displaymath}
\Phi: \mbox{当}\frac{\sqrt{n}(\bar{X}-\theta_0)}{\sigma} \geq -u_{\alpha},\mbox{接受原假设},\mbox{反之则否定原假设}
\end{displaymath}

可以看到随机变量$\frac{\sqrt{n}(\bar{X}-\theta_0)}{\sigma}$满足$N(0, 1)$的正态分布，但在总体方差未知情况，我们无法已知$\sigma$，如果我们用点估计$S^2 = \sum_{i=1}^n\frac{(X_i - \bar{X})^2}{n-1}$来估计$\sigma^2$，分布就发生了变化。我们这里先写明此时$\frac{\sqrt{n}(\bar{X}-\theta_0)}{\sigma}$满足自由度为$n-1$的$t$分布，下面对这一点给出证明。

若$X$与$Y$相互独立，且有$X \sim N(0, 1)$，$Y \sim \chi_n^2$，又设$\delta$为常数，则随机变量$(X+\delta)/\sqrt{\frac{1}{n}Y}$的分布称为自由度为n，非中心参数为$\delta$的非中心$t$分布。记为$Z \sim t_{n,\delta}$，分布函数记为$F_{n,\delta}(x)$，若有一组符合$t$分布的随机变量$Z_1, Z_2, Z_3$，它们的满足$\delta_1 < \delta_2 < \delta_3$，下面给出这组变量$t$分布的图像。

```{r tdis, eval=T, echo=F, warning=F, message=F, error=F,fig.height=2.5, fig.width=4}
df <- 10
ncp1 <- 0
ncp2 <- 2
ncp3 <- 4
x <- seq(-10, 15, 0.1)
dt1 <- dt(x, df=df, ncp=ncp1)
dt2 <- dt(x, df=df, ncp=ncp2)
dt3 <- dt(x, df=df, ncp=ncp3)
ft1 <- pt(x, df=df, ncp=ncp1)
ft2 <- pt(x, df=df, ncp=ncp2)
ft3 <- pt(x, df=df, ncp=ncp3)
da <- data.frame(x, dt1, dt2, dt3, ft1, ft2, ft3)

figdt <- ggplot(data=da) + geom_line(aes(x, dt1), colour="blue", size=0.5) + geom_line(aes(x, dt2), colour="red", size=0.8) + geom_line(aes(x, dt3), colour="green", size=1.2) + xlab(TeX("$\\Z$")) + ylab(TeX("$t_{n,\\delta}$"))
figft <- ggplot(data=da) + geom_line(aes(x, ft1), colour="blue", size=0.5) + geom_line(aes(x, ft2), colour="red", size=0.8) + geom_line(aes(x, ft3), colour="green", size=1.2) + xlab(TeX("$\\Z$")) + ylab(TeX("$F_{n, \\delta}(Z)$"))

grid.newpage()
pushViewport(viewport(layout=grid.layout(1, 2)))
print(figdt, vp=vplayout(1, 1))
print(figft, vp=vplayout(1, 2))
```

如上图蓝色，红色，绿色的曲线分别代表$Z_1, Z_2, Z_3$，可见若有$\delta_1 < \delta_2$，则对任一的$Z$，有$F_{n, \delta_1}(Z)\geq F_{n, \delta_2}(Z)$

有了如上的信息，可知：

\begin{displaymath}
\frac{\sqrt{n}(\bar{X}-\theta_0)}{S} = (\frac{\sqrt{n}(\bar{X}-\theta)}{\sigma} + \frac{\sqrt{n}(\theta - \theta_0)}{\sigma})/\sqrt{\frac{1}{\sigma^2}S^2}
\end{displaymath}

令$X = \frac{\sqrt{n}(\bar{X}-\theta)}{\sigma}$，$\delta = \frac{\sqrt{n}(\theta - \theta_0)}{\sigma}$，$Y = \frac{(n-1)S^2}{\sigma^2}$，容易知道$X$，$Y$相互独立且有$X \sim N(0, 1)$以及$Y \sim \chi_{n-1}^2$，根据$t$分布的定义，最后得到：

\begin{displaymath}
\frac{\sqrt{n}(\bar{X}-\theta_0)}{S} \sim t_{n-1, \sqrt{n}(\theta-\theta_0)/\sigma}
\end{displaymath}

所以，可以把上面的检验改写成：

\begin{equation}
\label{t_psi}
\Psi: \mbox{当}\frac{\sqrt{n}(\bar{X}-\theta_0)}{S} \geq -t_{n-1}(\alpha),\mbox{接受原假设},\mbox{反之则否定原假设}
\end{equation}

这里$t_{n-1}(\alpha)$与$u_{\alpha}$类似，是指自由度$n-1$，非中心参数为$\delta=0$（此时$\theta=\theta_0$）的$t$分布的上分位数为$\alpha$的点。

这样，可以直接给出检验$\Psi$的功效函数：

\begin{equation}
\begin{split}
\beta_{\psi}(\theta, \sigma) &= P_{\theta, \sigma}(\sqrt{n}(\bar{X}-\theta_0)/S <-t_{n-1}(\alpha) )\\
&=F_{n-1, \delta}(-t_{n-1}(\alpha))
\end{split}
\end{equation}

这里$\delta = \frac{\sqrt{n}(\theta-\theta_0)}{\sigma}$，$\beta_{\psi}(\theta, \sigma)$的取值与$\delta$有关，要想让功效函数有水平$\alpha$，就要求：

\begin{displaymath}
\beta_{\psi}(\theta, \sigma) = F_{n-1, \delta}(-t_{n-1}(\alpha)) \leq \alpha \quad \forall \theta \in H_0:\{\theta|\theta \geq \theta_0\}
\end{displaymath}

由上面对$t$分布的性质可知，$F$对于$\delta$是递减的，而$\delta=\frac{\sqrt{n}(\theta-\theta_0)}{\sigma}$对$\theta$是递增的。因此$\theta$变大时，$F$就会变小，所以当$\theta=\theta_0$时，对应了$F$的最大值。所以$\beta_{\psi}(\theta, \sigma)$在$H_0$上存在最大值，且当$\theta=\theta_0$时取最大。这样就证明了检验$\Psi$的功效函数存在水平$\alpha$。所以有：

\begin{displaymath}
\begin{split}
&\beta_{\Psi}(\theta, \sigma) =  P_{\theta, \sigma}(\sqrt{n}(\bar{X}-\theta_0)/S <-t_{n-1}(\alpha) ) \leq \beta_{\Psi}(\theta_0, \sigma)\\
&\mbox{等价于}\\
&F_{n-1, \delta}(-t_{n-1}(\alpha)) \leq F_{n-1, 0}(-t_{n-1}(\alpha))\\
&\mbox{分布}t_{n-1,0}\mbox{关于原点对称，所以有}-t_{n-1}(\alpha)=t_{n-1}(1-\alpha)\mbox{得出}\\
&F_{n-1, \delta}(-t_{n-1}(\alpha)) \leq F_{n-1, 0}(t_{n-1}(1-\alpha))=\alpha
\end{split}
\end{displaymath}

由此，$F$的水平为$\alpha$得证。同样的，我们给出一个例子讨论。

```{r eval=T, echo=F, error=F, warning=F, message=F}
set.seed(1234)
n <- 10
sigma <- 2
th0 <- 120
sample_t <- rnorm(n, 110, 2)
alpha <- 0.05
th_set <- seq(90, 150, 0.5)
delta <- sqrt(n)*(th_set - th0)/sigma

t_sample <- sqrt(n)*(mean(sample_t) - th0)/sd(sample_t)
t_alpha <- qt(alpha, df=n-1, ncp=0, lower.tail = F)
#print(sample)
#beta <- pt(-t_alpha, df=n-1, ncp=delta)
```
\begin{lemma}
\label{lizi_ttest}
已知样本`r sample_t`来自总体$N(\theta, \sigma)$，考虑假设检验问题$H_0:\theta \geq 120,H_1:\theta < 120$,在检验水平$\alpha=0.05$情况下的情况。
\end{lemma}
\begin{displaymath}
\begin{split}
&\mbox{直接计算检验统计量：}\\
&\frac{\sqrt{n}(\bar{X}-\theta_0)}{S} = \frac{\sqrt{`r n`}(`r mean(sample_t)`-`r th0`)}{`r sd(sample_t)`}=`r t_sample`\\
&\mbox{查表得到自由度为}`r n-1`，\mbox{上分位点为}`r alpha`\mbox{的}t_{n-1(\alpha)}\\
&t_{`r n-1`}(`r alpha`) = `r t_alpha`\\
&\mbox{显然有：}\\
&`r t_sample` < `r -t_alpha`\\
&\mbox{根据检验$\Psi$\eqref{t_psi}式，原假设被拒绝}
\end{split}
\end{displaymath}

\subsubsection{两个正态总体均值差的检验}

若样本$X_1,\cdots,X_n$是从总体$N(\theta_1, \sigma^2)$抽出，而样本$Y_1, \cdots, Y_n$是从正态总体$N(\theta_2, \sigma^2)$中抽出。两个样本的总体方差相同，但均值是不同的。考虑如下检验问题：

1. $H_0:\theta_1 - \theta_2 \geq \theta_0,H_1:\theta_1 - \theta_2 < \theta_0;$
2. $H_0:\theta_1 - \theta_2 \leq \theta_0,H_1:\theta_1 - \theta_2 > \theta_0;$
3. $H_0:\theta_1 - \theta_2 = \theta_0,H_1:\theta_1 - \theta_2 \neq \theta_0.$

这三个检验问题本质上与前面单一样本的均值检验没有什么不同，考虑检验统计量$\bar{X}-\bar{Y}$作为$\theta_1-\theta_2$的估计，同样可以定出一个$C$作为临界值，于是对于问题1来说，一个检验$\Phi$是，$\bar{X}-\bar{Y}>C$时接受原假设，反之则拒绝。考虑$\bar{X}-\bar{Y}$的方差为$S^2 = (1/n + 1/m)\sigma^2$，于是类似前面的$u$检验临界值定义\eqref{lingjiezhi}$C=\theta_0 - \sigma u_{\alpha}/\sqrt{n}$，这里同样可设计一个检验统计量：

\begin{equation}
\label{doubletu}
U = \sqrt{\frac{nm}{n+m}}\frac{(\bar{X}-\bar{Y}-\theta_0)}{\sigma}
\end{equation}

于是上述上述三个问题的检验为:

1. $g$: 当$U \geq -u_{\alpha}$接受$H_0$，不然就否定$H_0$;
2. $g^{'}$: 当$U \leq u_{\alpha}$接受$H_0$，不然就否定$H_0$;
3. $g^{''}$: 当$|U| \leq u_{\alpha/2}$接受$H_0$，不然就否定$H_0$.

对于方差$\sigma$未知的情况，检验统计量中的$\sigma$以样本的标准差来代替。此时这个统计量服从$t$分布：

\begin{equation}
T = \sqrt{\frac{nm}{n+m}}\frac{(\bar{X}-\bar{Y}-\theta_0)}{S}
\end{equation}

当$\theta_1 - \theta_2=\theta_0$时，$T$服从自由度为$n+m - 2$的$t$分布，于是可针对上面的假设问题可提出如下检验法：

1. $h$: 当$T \geq -t_{n+m-2}(\alpha)$时接受$H_0$，不然就否定$H_0$;
2. $h^{'}$: 当$T \leq t_{n+m-2}(\alpha)$时接受$H_0$，不然就否定$H_0$;
3. $h^{''}$: 当$|T| \leq t_{n+m-2}(\alpha/2)$时接受$H_0$，不然就否定$H_0$;

上述三个检验合称*双样本$t$检验*，值得注意，这个检验要求两样本方差相同，应用上则要求两样本方差之比与1相差不大。

\section{回归、相关与方差分析}
\subsection{回归分析基本概念}
\subsubsection{回归模型的一般形式}
若有一组$X_1,\cdots, X_p$和$Y$，现设想$Y$的取值由两部分决定，其中一部分由$X_1,\cdots, X_p$影响导致，这一部分可以表示成$f(X_1,\cdots, X_p)$即$X_1,\cdots, X_p$的一个函数，另外一部分是未考虑的因素，可以视为一个随机误差，记为$e$，于是$Y$就由下式决定：
\begin{equation}
Y = f(X_1,\cdots, X_p) + e
\end{equation}

$e$作为随机误差，我们要求$E(e) = 0$，于是，此时的$f(X_1,\cdots, X_p)$相当于在给定$X_1,\cdots, X_p$情况下，$Y$的条件期望:
\begin{displaymath}
f(X_1,\cdots, X_p) = E(Y|X_1,\cdots, X_p)
\end{displaymath}
把函数$f(X_1,\cdots, X_p)$称为$Y$对$X_1,\cdots, X_p$的回归函数，而方程
\begin{displaymath}
y = f(x_1,\cdots, x_p)
\end{displaymath}
称为$Y$对$X_1,\cdots, X_p$的*回归方程*。一般我们还把随机变量$Y$称为*因变量*，把随机变量$X_1,\cdots, X_p$称为自变量。值得注意的是$X_1,\cdots, X_p$虽然是一组随机变量，但在实际应用中要视情况而定，比如想建立一个用乔木的株高和基径来预测树龄的模型，事先获得了1000棵乔木的株高、基径、年龄数据，这时作为自变量的株高和基径就是随机变量，因为在测量时是随机抽样的，测量者无法决定被测量树木的株高和基径的分布。而非随机的情况则往往出现在一些预先设计的实验中，如要研究一个基因片段在小鼠表达量与小鼠年龄的关系，预先选出一组具有年龄梯度，其他条件接近的小鼠来做实验。这样作为自变量的小鼠年龄是受人为的控制，这就不是一个随机变量了。

正如如上的例子，实际问题中，回归模型往往未知，回归分析的任务就是根据$X_1,\cdots, X_p$和$Y$的观测值来估计$f(X_1,\cdots, X_n)$并围绕它来解决相关的统计推断问题。在回归以及统计推断中所用的方法很大程度取决于模型中的假定，具体的就是对回归函数$f$以及对随机误差$e$的假定。如果对$f$的形式没有假定，那么这就称为*非参数回归*。如果对$f$的情况有假定，只是其中几个参数未知，那么则称其为*参数回归*。在运用上最完善的是假定$f$是线性函数的情形，也就是所谓的*线性回归*：
\begin{displaymath}
f(x_1, \cdots, x_p) = b_0 + b1x_1 + \cdots + b_px_p
\end{displaymath}

对于随机误差，一个重要假定是误差的均值$E(e)=0$，另外一个要考虑的是误差的方差$\sigma^2$，因为有：
\begin{displaymath}
E[Y - f(X_1, \cdots, X_n)]^2 = E(e^2) = Var(e) = \sigma^2
\end{displaymath}
越小的$\sigma^2$表明$f(X_1, \cdots, X_n)$逼近$Y$的均方误差就越小，说明回归方程越有用。

另外一个对误差$e$的重要假定是要求$e$服从$N(0, \sigma^2)$的正态分布，只有这样才能在后续的统计推断中有相应的小样本方法。如果发现误差不符合上面的假定，可以考虑是否是$f$是否选的得当或者实验是否考虑足够全面的因素。

\subsubsection{回归模型的应用}

回归模型在不同的应用场景有不同的意义，对其结果的解释是不同的。

*1. 用于描述*

在累积了一批数据$(X_1,Y_1),\cdots,(X_n, Y_n)$的情况下，想看看$Y$和$X$的趋势变化，这是最好的方式就是在散点图上叠加一个$Y$和$X$之间的回归线，这样能更好的观察它们的关系。在这种情况下的应用仅仅是描述这批数据本身，不涉及到对这批数据总体的讨论，因此对模型的准确性要求不会很高。

*2. 用于估计*

如果建立了自变量身高$X$对因变量体重$Y$的函数$f(x)=b_0 + b_1x$，假定$(X, Y)$服从二维的正态分布。从前面的论述可知，回归函数$f(x)$相当于条件期望$E(Y|X=x)=b_0 + b_1x$，通过一组观测我们估计得到了$f(x) = \hat{b}_0 + \hat{b}_1x$，这可以相当于我们估计了身高为$x$的人群中平均体重为$\hat{b}_0 + \hat{b}_1x$。

*3. 用于预测*

在累积了一批田间施肥量和粮食产量的数据$(X_1, Y_1), \cdots, (X_n, Y_n)$的数据后，用施肥量$X$做自变量，粮食产量$Y$做因变量，得到了一个回归模型，这时需要对一批新的试验田的情况进行预测，这批试验田刚刚施肥，有一组施肥量数据$X^"_1, \cdots, X^"_n$，要用这批数据来预测未来粮食产量，可以知道产量为$y_n = f(x_n) + e_n$，这和上面的估计不同，这里要得到产量，首先要估计$f(x_n)$，然后再对随机误差$e_n$进行估计，而随机误差往往是无法考虑到的因素，所以无法得到其良好估计。可见用于预测的情形往往精度要比用于估计的情形差。

*4. 用于控制*

在药代动力学中，常常研究给药量随体内浓度变化的关系，如果根据以往实验建立了某种药物的服用量与一段时间后体内药物浓度的回归函数，这就可以根据这个函数给出服药建议，口服多少的药物才能让药物的浓度在体内代谢一段时间后达到某个仍能产生药效的阈值，进而给出一个具体的服药周期，如一天该几次服药，一次服药多少。

\subsection{一元线性回归}
一元线性回归中，自变量只有一个$X$，回归模型的形式为：
\begin{equation}
\label{linearmodel}
Y = b_0 + b_1 X + e
\end{equation}
其中$b_0$和$b_1$是未知参数，随机误差满足$E(e)=0$以及$0<\textrm{Var}(e) = \sigma^2 < \infty$。现在对\eqref{linearmodel}中的变量$X$和$Y$进行$n$次独立的观察，得到一组样本$(X_1, Y_1),\cdots,(X_n, Y_)$，根据线性模型的定义，这组样本满足：
\begin{equation}
\label{real_linearmodel}
Y_i = b_0 + b_1 X_i + e_i \quad (i = 1, \cdots, n)
\end{equation}
显然，$e_i$的值是每次观察的随机误差，这个值是无法获得的。又由于每次观察独立，所以$e_1, \cdots, e_n$满足：
\begin{equation}
\label{error_term}
\begin{split}
&e_1, \cdots, e_n \mbox{独立同分布}\\
&E(e_i) = 0, \quad \textrm{Var}(e_i) = \sigma^2 \quad (i = 1, \cdots, n)
\end{split}
\end{equation}
为了简化模型，还会加上$e_i$满足正态分布的假定。上面式\eqref{real_linearmodel}和式\eqref{error_term}就是对一元线性回归模型的描述。为了方便后续的讨论，对式\eqref{real_linearmodel}进行一些变形:
\begin{equation}
\label{central_model}
\begin{split}
&Y_i = \beta_0 + \beta_1(X_i - \bar{X}) + e_i \quad (i = 1, \cdots, n)\\
&\beta_1 = b_1, \beta_0 = b_0 + b_1 \bar{X}
\end{split}
\end{equation}
这种形式的一元线性模型称为*中心化的一元线性模型*，用这种形式的模型估计$\beta_1$和$\beta_0$运算相对简单，因此把这个模型作为后续讨论使用的模型。

\subsubsection{$\beta_0$和$\beta_1$的最小二乘估计}

对一组观测$(X_1, Y_1),\cdots, (X_n, Y_n)$来建立线性模型，设想已经得到了式\eqref{central_model}中$\beta_0$和$\beta_1$的估计量$\alpha_0$和$\alpha_1$，那么回归函数的估计为$\alpha_0 + \alpha_1(x  - \bar{X})$，则当$x = X_i$时有：
\begin{equation}
\hat{Y}_i = \alpha_0 + \alpha_1(X_i - \bar{X})
\end{equation}
这就是用$\alpha_0 + \alpha_1(X_i  - \bar{X})$对$Y_i$的一个估计$\hat{Y}_i$，显然这个估计与真实值$Y_i$有差距$Y_i - \hat{Y}_i$。如果能让这种差距尽量的小，我们得到的估计函数$\alpha_0 + \alpha_1(x - \bar{X})$就越有效。我们用这些误差的平方和来描述这些差距，由此有：
\begin{equation}
\label{error_squaresum}
Q(\alpha_0, \alpha_1) = \sum\limits_{i=1}^n (Y_i - \hat{Y}_i)^2 = \sum\limits_{i=1}^n [Y_i - \alpha_0 - \alpha_1 (X_i - \bar{X})]^2
\end{equation}
如果找到一组$\alpha_0$和$\alpha_1$能让式\eqref{error_squaresum}取最小，就能以其作为$\beta_0$，$\beta_1$的估计。容易得到$Q(\alpha_0, \alpha_1)$是二元的函数，分别另$Q$在$\alpha_0$和$\alpha_1$上偏导数为0，解方程组：
\begin{equation}
\label{solve_square}
\begin{split}
\frac{\partial Q}{\partial \alpha_0} &= 0\\
\frac{\partial Q}{\partial \alpha_1} &= 0
\end{split}
\end{equation}
就可得$\alpha_0$和$\alpha_1$的两个解$\hat{\beta}_0$和$\hat{\beta}_1$。显然，首先用$\partial Q/\partial \alpha_0 = 0$来解出$\hat{\beta}_0$：
\begin{displaymath}
\begin{split}
\frac{\partial Q}{\partial \alpha_0} &= -2 \sum\limits_{i=1}^n[Y_i - \alpha_0 - \alpha_1 (X_i - \bar{X}] = 0\\
\frac{\partial Q}{\partial \alpha_0} &= -2 [\sum\limits_{i=1}^n Y_i - \sum\limits_{i=1}^n \alpha_0 - \alpha_1(\sum\limits_{i=1}^n X_i - \sum\limits_{i=1}^n \bar{X})]=0\\
\frac{\partial Q}{\partial \alpha_0} &= -2 [\sum\limits_{i=1}^n Y_i - n\alpha_0] = 0\\
\mbox{易得：}\hat{\beta}_0 &= \sum\limits_{i=1}^n Y_i = \bar{Y}
\end{split}
\end{displaymath}
然后再把$\hat{\beta}_0$带入\eqref{solve_square}，解出$\hat{\beta}_1$：
\begin{displaymath}
\label{solve_square2}
\begin{split}
\frac{\partial Q}{\partial \alpha_1} &= -2 \sum\limits_{i = 1}^n (X_i - \bar{X})[Y_i - \alpha_0 - \alpha_1(X_i - \bar{X})] = 0\\
\hat{\beta}_1 &= \frac{\sum\limits_{i=1}^n(X_i - \bar{X})Y_i}{\sum\limits_{i=1}^n(X_i - \bar{X})^2}
\end{split}
\end{displaymath}

由此得到$\alpha_0$和$\alpha_1$的*最小二乘估计*：
\begin{equation}
\label{ols_estimator}
\begin{split}
\hat{\beta}_0 &= \bar{Y}\\
\hat{\beta}_1 &= \frac{\sum\limits_{i=1}^n(X_i - \bar{X})Y_i}{\sum\limits_{i=1}^n(X_i - \bar{X})^2}
\end{split}
\end{equation}

仔细观察上面的两个估计量，可以发现它们都是$Y_1, \cdots, Y_n$的线性函数，这是一个很好的性质。更进一步的，由式\eqref{ols_estimator}可以得到这两个估计量的一些性质，它们的均值、方差和协方差分别满足：
\begin{equation}
\label{ols_property}
\begin{split}
E(\hat{\beta}_0) &= \beta_0 \quad E(\hat{\beta}_1) = \beta_1\\
\textrm{Var}(\hat{\beta}_0) &= \frac{\sigma^2}{n}\\
\textrm{Var}(\hat{\beta}_1) &= \frac{\sigma^2}{\sum\limits_{i=1}^n(X_i - \bar{X})^2}\\
\textrm{Cov}(\hat{\beta}_0, \hat{\beta}_1) &= 0
\end{split}
\end{equation}
可见，由估计量的均值和真实值相等可知，最小二乘估计得出的$\hat{\beta}_0$和$\hat{\beta}_1$都是无偏估计量，而$\hat{\beta}_1$的方差显示与$X_1, \cdots, X_n$成反比即$\textrm{Var}(\hat{\beta}_1) = \sigma^2/\sum\limits_{i=1}^n(X_i-\bar{X}) = \sigma^2 / S_{x}^2$。这说明$X_1, \cdots, X_n$最好要散开一些，这样得到的$\hat{\beta}_1$方差较小，而在无偏估计量中，方差较小者较优。协方差为零表明两个估计量之间不相关，这是中心化带来的结果。（这里的$\sigma^2$是误差$e_i$的方差）

由前面式\eqref{central_model}，可以轻易的得出$b_0$和$b_1$的最小二乘估计：
\begin{equation}
\hat{b}_0 = \hat{\beta}_0 - \hat{b}_1 \bar{X},\quad \hat{b}_1 = \hat{\beta}_1
\end{equation}
它们同样也是$b_0$，$b_1$的无偏估计。

\subsubsection{误差的方差$\sigma^2$的估计}

有了估计量$\hat{\beta}_0$和$\hat{\beta}_1$，可以用它得到当$X=X_i$时，$Y$的预测值为$\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 (X_i - \bar{X})$，显然真实值$Y_i$和预测值$\hat{Y}_i$存在差距，两者的差:
\begin{equation}
\label{residual_def}
\delta_i = Y_i - \hat{Y}_i \quad (i = 1, \cdots, n)
\end{equation}
这个差距被称为*残差*。注意，残差和前面的误差$e$不等价，因为一般情况下，线性模型并不是一个完全能够描述事物本身的模型，更多的是一种近似处理，有时为了方便和简单，因变量$X$取的也不多，因此此时的线性模型中的$e$误差项可能还包含了一些可以观察到，但并未计入因变量的因素。而能确认模型正确，因变量$X$取的也恰当，此时$\delta$可以认为是$e$的一个估计，它能体现误差必须“杂乱无章”的性质。这一点在后面可以体现。

那么，当模型正确，即线性模型\eqref{central_model}以及关于误差的假定\eqref{error_term}能满足时，误差的方差的一个估计量可以由残差来描述：
\begin{equation}
\label{unbias_sigma}
\hat{\sigma}^2 = \frac{1}{n-2} \sum\limits_{i=1}^{n}\delta^2_i
\end{equation}
可以证明，这个估计量是$\sigma^2$的一个无偏估计量。首先对$Y_i - \hat{Y}_i$进行化简,容易写出：
\begin{displaymath}
\begin{split}
Y_i - \hat{Y}_i &= \beta_0 + \beta_1(X_i - \bar{X}) + e_i - \hat{\beta}_0 - \hat{\beta}_1(X_i - \bar{X})\\
&= \beta_0 - \hat{\beta}_0 + (\beta_1 - \hat{\beta}_1)(X_i - \bar{X}) + e_i
\end{split}
\end{displaymath}
然后分别对$\beta_0 - \hat{\beta}_0$和$\beta_1 - \hat{\beta}_1$进行化简，
\begin{displaymath}
\begin{split}
\beta_0 - \hat{\beta}_0 &= \beta_0 - \bar{Y} = \beta_0 - \frac{1}{n} \sum\limits_{i=1}^n Y_i\\
&= \beta_0 - \frac{1}{n} \sum\limits_{i=1}^n [\beta_0 + \beta_1 (X_i - \bar{X}) + e_i]\\
&= \beta_0 - \beta_0 - 0 - \bar{e}\\
&=-\bar{e}
\end{split}
\end{displaymath}

对$\beta_1 - \hat{\beta}_1$有：
\begin{displaymath}
\begin{split}
\beta_1 - \hat{\beta}_1 &= \beta_1 - \frac{\sum\limits_{j=1}^n (X_j - \bar{X}) Y_j}{\sum\limits_{j=1}^n (X_j - \bar{X})^2}\\
&= \beta_1 - \frac{\sum\limits_{j=1}^n (X_j - \bar{X}) [\beta_0 + \beta_1(X_j - \bar{X}) + e_j]}{\sum\limits_{j=1}^n (X_j - \bar{X})^2}\\
&= \beta_1 - \frac{\beta_0 \sum\limits_{j=1}^n (X_j - \bar{X}) + \beta_1 \sum\limits_{j=1}^n (X_j -\bar{X})^2 + \sum\limits_{j=1}^n (X_j -\bar{X})e_j}{\sum\limits_{j=1}^n (X_j - \bar{X})^2}\\
&= \beta_1 - 0 - \beta_1 - \frac{\sum\limits_{j=1}^n(X_j - \bar{X})e_j}{\sum\limits_{j=1}^n(X_j - \bar{X})^2}\\
& = - \frac{\sum\limits_{j=1}^n(X_j - \bar{X})e_j}{\sum\limits_{j=1}^n(X_j - \bar{X})^2}
\end{split}
\end{displaymath}
把上面的结果带回，得到：
\begin{displaymath}
\delta_i = e_i - \bar{e} - (X_i - \bar{X}) \frac{\sum\limits_{j=1}^n(X_j - \bar{X})e_j}{\sum\limits_{j=1}^n (X_j - \bar{X})^2}
\end{displaymath}

对$\delta_i$作平方和，有：
\begin{displaymath}
\begin{split}
\sum\limits_{i=1}^n \delta_i^2 = &\sum\limits_{i=1}^n (e_i - \bar{e})^2 + \sum\limits_{i=1}^n [(X_i - \bar{X}) \frac{\sum\limits_{j=1}^n (X_j - \bar{X})e_j}{\sum\limits_{j=1}^n (X_j - \bar{X})^2}]^2\\ 
&- \sum\limits_{i=1}^n 2 (e_i - \bar{e}) (X_i - \bar{X})\frac{\sum\limits_{j=1}^n (X_j - \bar{X})e_j}{\sum\limits_{j=1}^n (X_j - \bar{X})^2} 
\end{split}
\end{displaymath}

分别考虑上式中的后面两项：
\begin{displaymath}
\begin{split}
&\sum\limits_{i=1}^n [(X_i - \bar{X}) \frac{\sum\limits_{j=1}^n (X_j - \bar{X})e_j}{\sum\limits_{j=1}^n (X_j - \bar{X})^2}]^2\\
&= \frac{[\sum\limits_{j=1}^n (X_j - \bar{X})e_j]^2}{[\sum\limits_{j=1}^n (X_j - \bar{X})^2]^2}\sum\limits_{j=1}^n (X_i - \bar{X})^2\\
&= \frac{[\sum\limits_{j=1}^n (X_j - \bar{X}) e_j]^2}{\sum\limits_{j=1}^n (X_j - \bar{X})^2}
\end{split}
\end{displaymath}
\begin{displaymath}
\begin{split}
&\sum\limits_{j=1}^n (e_i - \bar{e}) (X_i - \bar{X}) = \sum\limits_{j=1}^n (X_i - \bar{X}) e_i\\
& \sum\limits_{i=1}^n [2 (e_i - \bar{e}) (X_i - \bar{X}) \frac{\sum\limits_{j=1} (X_j - \bar{X}) e_j}{\sum\limits_{j=1}^n (X_j - \bar{X})^2}] \\
&= 2 \frac{\sum\limits_{j=1} (X_j - \bar{X}) e_j}{\sum\limits_{j=1}^n (X_j - \bar{X})^2} \sum\limits_{i=1}^n (X_i - \bar{X})e_i \\
&= 2 \frac{[\sum\limits_{j=1}^n(X_j - \bar{X})e_j]^2}{\sum\limits_{j=1}^n (X_j - \bar{X})^2}
\end{split}
\end{displaymath}

把化简后的两项带入$\sum\limits_{i=1}^n \delta_i^2$中，统一记号为$i$最后得到：
\begin{equation}
\label{delta_final}
\sum\limits_{i=1}^n \delta_i^2 = \sum\limits_{i=1}^n (e_i - \bar{e})^2 -\frac{[\sum\limits_{i=1}^n (X_i - \bar{X}) e_i]^2}{\sum\limits_{i=1}^n (X_i - \bar{X})^2}
\end{equation}

由线性模型误差项的假定\eqref{error_term}可知，$e_1, \cdots, e_n$独立同分布且均值为0， 方差为$\sigma^2$。由此可得
\begin{equation}
E[\sum_{i=1}^n (e_i - \bar{e}) ^2 ] = (n-1)\sigma^2
\end{equation}
\begin{equation}
\begin{split}
E[\sum\limits_{i=1}^n (X_i - \bar{X}) e_i] ^2 &= \textrm{Var}[\sum\limits_{i=1}^n (X_i - \bar{X}) e_i]\\
&= \sum\limits_{i=1}^n (X_i - \bar{X}) ^2 \textrm{Var}(e_i)\\
&= \sigma^2 \sum\limits_{i=1}^n (X_i - \bar{X}) ^2
\end{split}
\end{equation}
把以上两个式带入式\eqref{delta_final}中得到：
\begin{equation}
E(\sum\limits_{i=1}^n \delta_i ^2) = (n-2)\sigma^2
\end{equation}

由此得证式\eqref{unbias_sigma}是一个$\sigma^2$的无偏估计量。一般把$\sum\limits_{i=1}^n \delta_i ^2$称为*残差平方和*，当误差$e_i \sim N(0, \sigma^2)$时，残差平方和满足：
\begin{equation}
\label{dis_residual}
\sum\limits_{i=1}^n \frac{\delta_i ^2}{\sigma^2} \sim \chi_{n-2}^2
\end{equation}
用式\eqref{delta_final}计算残差平方和太繁琐，一般采用下面的式子计算：
\begin{equation}
\label{residual_quik}
\sum\limits_{i=1}^n \delta_i ^2 = \sum\limits_{i=1}^n Y_i^2 - n\bar{Y}^2 - \hat{\beta}_1 \sum\limits_{i=1}^n (X_i - \bar{X}) Y_i
\end{equation}

在得到回归模型后，考察残差的一个重要目的就是考察模型的假定\eqref{central_model}和\eqref{error_term}是否正确，如果模型确实符合实际数据，因残差是误差的一个估计量，而$e_1, \cdots, e_n$是独立同分布的，所以残差$\delta_i, \cdots, \delta_i$也应该独立同分布，具有和$X_1, \cdots, X_n$不相关的“杂乱无章”的分布。相反，如果模型不正确，可能会发现残差随$X_i$变化，存在一定的相关性。下面以一个例来说明。

```{r eval=T, echo=F, warning=F, message=F, error=F}
set.seed(1234)
x_l <- c(1:100)
e_l <- rnorm(100, mean=0, sd=3)
y_l <- 2* x_l + 3 + e_l
y_nl <- 2*x_l + 0.1*x_l^2 + 3 + e_l

lm_l <- lm(y_l ~ x_l, data=data.frame(x_l, y_l))
lm_nl <- lm(y_nl ~ x_l, data=data.frame(x_l, y_nl))
```

\begin{lemma}
现对两批数据建立了两个线性模型，它们分别是：
\begin{displaymath}
\begin{split}
y_1 &= `r lm_l$coefficients[2]` x_1 + `r lm_l$coefficients[1]`\\
y_2 &= `r lm_nl$coefficients[2]` x_2 +`r lm_nl$coefficients[2]`
\end{split}
\end{displaymath}
其中第一个模型的$Y_1, \cdots, Y_n$是由$X_1, \cdots, X_n = 1, 2, \cdots, 100$通过线性关系$y = 2x + 3 + e$生成的，而第二个模型则加入了一个二次项即$y = 2x +0.1x^2 + 3 + e$。现在分别作出它们的回归线图和残差相对$X_1, \cdots, X_n$的关系图
\end{lemma}
首先画出模型$y_1 = `r lm_l$coefficients[2]` x_1 + `r lm_l$coefficients[1]`$的回归线图和残差图

```{r residual_graph1, eval=T, echo=F, warning=F, message=F, error=F,fig.height=1.5, fig.width=4}
fig_l <- ggplot(data=data.frame(x_l, y_l, lm_l$fitted.values)) + geom_point(aes(x_l, y_l)) + geom_line(aes(x_l, lm_l$fitted.values), colour="green", size = 1) + xlab(TeX("$x_1$")) + ylab(TeX("$y_1$"))
fig_resl <- ggplot(data=data.frame(x_l, lm_l$residuals)) + geom_point(aes(x_l, lm_l$residuals)) + geom_hline(yintercept = 0, col="red", size=1)+ xlab(TeX("$x_1$")) + ylab(TeX("$\\delta_i$"))

grid.newpage()
pushViewport(viewport(layout=grid.layout(1, 2)))
print(fig_l, vp=vplayout(1, 1))
print(fig_resl, vp=vplayout(1, 2))
```

很明显，模型$y_1 = `r lm_l$coefficients[2]` x_1 + `r lm_l$coefficients[1]`$虽然与我们设计数据时的模型有差别，但从回归线对散点图的拟合情况看，所有的观测点都均匀分布在回归线的两侧。残差图也显示出残差散乱的分布在0的两端，但有越靠近0越集中的趋势，说明此时的残差项与$X_i, \cdots, X_n$并无明显关联。这两点都表明了我们采用线性模型来拟合这批数据是基本正确的。

再来画出模型$y_1 = `r lm_nl$coefficients[2]` x_1 + `r lm_nl$coefficients[1]`$的回归线图和残差图

```{r residual_graph2, eval=T, echo=F, warning=F, message=F, error=F,fig.height=1.5, fig.width=4}
fig_nl <- ggplot(data=data.frame(x_l, y_nl, lm_nl$fitted.values)) + geom_point(aes(x_l, y_nl)) + geom_line(aes(x_l, lm_nl$fitted.values), colour="green", size = 1) + xlab(TeX("$x_2$")) + ylab(TeX("$y_2$"))
fig_resnl <- ggplot(data=data.frame(x_l, lm_nl$residuals)) + geom_point(aes(x_l, lm_nl$residuals)) + geom_hline(yintercept = 0, col="red", size=1)+ xlab(TeX("$x_2$")) + ylab(TeX("$\\delta_i$"))

grid.newpage()
pushViewport(viewport(layout=grid.layout(1, 2)))
print(fig_nl, vp=vplayout(1, 1))
print(fig_resnl, vp=vplayout(1, 2))
```

可以很明显的看到，回归线并没有较好的拟合所有观测点，观测点似乎存在一种二次关系，而残差图也表明了这一点，残差明显和$X_i$存在一种二次关系，这就表明了拟合的线性模型中存在未被$X_i$解释的二次项，而这个二次项被归入到了残差中，这是不符合线性模型的假定的，说明我们应该重新考虑模型的数学形式。

\subsubsection{区间估计和预测}

为了对线性模型的参数$\beta_1,\beta_0$作区间估计，需要对误差$e_1, \cdots, e_n$加入正态分布的假定。即对$e_1, \cdots, e_n$要求它们独立同分布，且有$e_i \sim N(0, \sigma^2)$。

先考虑$\beta_1$的区间估计，由前面的\eqref{ols_property}可知，$\hat{\beta}_1$均值$\beta_1$，方差为$\sigma^2S_x^{-2}$，因此与$\beta_1$有关的一个随机变量满足：
\begin{displaymath}
\frac{\hat{\beta}_1 - \beta_1}{\sigma S_x^{-1}} \sim N(0, 1)
\end{displaymath}
由于误差的方差$\sigma^2$未知，用式\eqref{unbias_sigma}中的估计量$\hat{\sigma^2}$来代替，因此正态分布$N(0, 1)$变成一个自由度$n-2$的中心$t$分布：
\begin{equation}
\frac{\hat{\beta}_1 - \beta_1}{\hat{\sigma} S_x^{-1}} \sim t_{n-2}
\end{equation}
由前面关于区间估计的计算方法，容易得到$\beta_1$在置信系数$1-\alpha$的置信区间：
\begin{equation}
[\hat{\beta}_1 - \hat{\sigma} S_x^{-1} t_{n-2}(\alpha/2),\hat{\beta}_1 + \hat{\sigma} S_x^{-1} t_{n-2}(\alpha/2)]
\end{equation}

其置信系数为$1-\alpha$上界与下界分别为：
\begin{equation}
\hat{\beta}_1 + \hat{\sigma} S_x^{-1} t_{n-2}(\alpha) \quad \hat{\beta}_1 - \hat{\sigma} S_x^{-1} t_{n-2}(\alpha) 
\end{equation}

对$\beta_0$的区间估计，类似上面的方法，已知$E(\hat{\beta}_0)=\beta_0,\textrm{Var}(\hat{\beta}_0) = \sigma^2 / n$，则枢轴变量满足；
\begin{equation}
\frac{\hat{\beta}_0 - \beta_0}{\hat{\sigma}/\sqrt{n} } \sim t_{n-2}
\end{equation}
由此可得，其置信系数的为$1-\alpha$的置信区间和置信上界、下界为：
\begin{equation}
\begin{split}
&[\hat{\beta}_0 - \frac{\hat{\sigma}}{\sqrt{n}} t_{n-2}(\alpha/2),\hat{\beta}_0 + \frac{\hat{\sigma}}{\sqrt{n}} t_{n-2}(\alpha/2)]\\
&\hat{\beta}_0 + \frac{\hat{\sigma}}{\sqrt{n}} t_{n-2}(\alpha)\\
&\hat{\beta}_0 - \frac{\hat{\sigma}}{\sqrt{n}} t_{n-2}(\alpha)
\end{split}
\end{equation}

下面对回归函数$m(x)=\beta_0 + \beta_1 (x - \bar{X})$做区间估计，容易知道，$m(x)$的估计量是$\hat{m}(x)=\hat{\beta}_0 + \hat{\beta}_1 (x - \bar{X})$，由此可以可以算出$\hat{m}(x)$的均值和方差：
\begin{displaymath}
\begin{split}
E(\hat{m}(x)) &= E(\hat{\beta}_0) + E(\hat{\beta}_1) (x - \bar{X})\\
& = \beta_0 + \beta_1 (x - \bar{X}) = m(x)\\
\textrm{Var}(\hat{m}(x)) &= \textrm{Var}(\hat{\beta}_0) + (x - \bar{X})^2 \textrm{Var}(\hat{\beta}_1)\\
&= \frac{\sigma^2}{n} + (x - \bar{X})^2 \frac{\sigma^2}{S_x^2}\\
&=\sigma^2[\frac{1}{n} + \frac{(x - \bar{X})^2}{S_x^2}]
\end{split}
\end{displaymath}
令$\lambda(x)=\textrm{Var}(\hat{m}(x))$，容易写出$m(x)$的置信系数为$1-\alpha$的置信区间为：
\begin{equation}
[\hat{m}(x) - \sqrt{\lambda(x)} t_{n-2}(\alpha/2),\hat{m}(x) + \sqrt{\lambda(x)} t_{n-2}(\alpha/2)]
\end{equation}
置信系数为$1-\alpha$的置信上界和置信下界为：
\begin{equation}
\hat{m}(x) +\sqrt{\lambda(x)} t_{n-2}(\alpha) \quad \hat{m}(x) -\sqrt{\lambda(x)} t_{n-2}(\alpha) 
\end{equation}
容易得到，这个$1-\alpha$的置信区间长度为$2\sqrt{\lambda(x)} t_{n-2}(\alpha/2)$与$x$有关，而$\lambda(x) = \sigma^2[1/n + (x - \bar{X})^2/S_x^2]$，可见$x$与$\bar{X}$越接近，$m(x)$的区间长度就越短。这说明在$1-\alpha$的置信系数下，$x$越靠近样本中心点$\bar{X}$,回归方程$m(x)$的精度就越高。

接下来对因变量$Y$作区间预测，值得注意这里的$Y$和上面的$m(x)$不是同样的，$m(x)$是假定的线性函数$\beta_0 + \beta_1 (x-\bar{X})$，没有包含误差项$e$，而$Y$是实际的线性模型，其中包含了误差项，也就是说$Y = m(x) + e$是我们才是我们作区间预测的部分。假定$Y_0$为$x = x_0$时的真实观测值。我们用$\hat{m}(x_0)$来估计$Y_0$，显然由于$m(x)$不包含误差项，所有$Y_0$与$\hat{m}(x_0)$之间存在差距$\eta = Y_0 -\hat{m}(x_0)$，考虑$\eta$的均值和方差，注意对$e$的独立和正态假定，对均值有
\begin{displaymath}
\begin{split}
E(\eta) &= E(Y_0) - E(\hat{m}(x_0))\\ 
&= E(\hat{m}(x_0)) + E(e_0) - E(\hat{m}(x_0))\\
&= m(x_0) + 0 - m(x_0)=0
\end{split}
\end{displaymath}
对方差有，注意$Y_1, \cdots, Y_n$之间的独立性，所以$\hat{m}(x_0)$与$Y_0$独立
\begin{displaymath}
\begin{split}
\textrm{Var}(\eta) &= \textrm{Var}(Y_0) + \textrm{Var}(\hat{m}(x_0))\\
&= \textrm{Var}(m(x_0)) + \textrm{Var}(e_0)+\textrm{Var}(\hat{m}(x_0))\\
&= 0 + \sigma^2 + \sigma^2[\frac{1}{n} + \frac{(x - \bar{X})^2}{S_x^2}]\\
&= \sigma^2[1 + \frac{1}{n} + \frac{(x - \bar{X})^2}{S_x^2}]
\end{split}
\end{displaymath}
上面的演算中，$m(x_0)$是在真实的$\beta_0$和$\beta_1$下计算得到的，此时$\beta_0$和$\beta_1$不像它们的估计量$\hat{\beta}_0$和$\hat{\beta}_1$，是一个固定的数值而非随机变量，因此$m(x_0)$是一个数值，其方差为0。

用$\eta$做枢轴变量，容易得到:
\begin{displaymath}
\frac{\eta}{\sigma^2[1 + \frac{1}{n} + \frac{(x - \bar{X})^2}{S_x^2}]} \sim t_{n-2}
\end{displaymath}
由此可以写出$Y_0$在置信系数$1-\alpha$下的置信区间，为简便，记录方差为$\zeta(x) = \sigma^2[1 + 1/n + (x - \bar{X})^2/S_x^2]$。得到：
\begin{equation}
[\hat{m}(x_0) - \sqrt{\zeta(x_0)} t_{n-2}(\alpha/2), \hat{m}(x_0) + \sqrt{\zeta(x_0)} t_{n-2}(\alpha/2)]
\end{equation}

下面来讨论前面提到的用于描述数据的回归模型和用于预测的预测模型的精度区别的问题。当用线性模型来描述已知的数据集时相当于对线性模型$Y = m(x) + e_0$，我们要得到一条直线方程$\hat{Y} = \hat{m}(x)$来尽可能好的描述这批数据的趋势。对一个数据集中的$x_0$，带入$\hat{m}(x)$中，得到一个$Y_0$的估计量$\hat{m}(x_0)$，因为$\hat{m}(x_0)$是一个随机变量，我们当然希望这个随机变量的置信区间越短越好，前面提到，此时的区间长度为$2\hat{\sigma}[1/n + (x_0-\bar{X})^2/S_x^2]^{1/2} t_{n-2}(\alpha/2)$,可以证明当样本容量趋于$\infty$时：
\begin{displaymath}
\begin{split}
\lim\limits_{n \rightarrow \infty}(\frac{1}{n}) &= 0\\
\lim\limits_{n \rightarrow \infty}(\frac{(x_0 - \bar{X})}{S_x^2}) &= 0
\end{split}
\end{displaymath}
另外，此时$\lim\limits_{n \rightarrow \infty}\hat{\sigma} = 0$，$t_{n-2}(\alpha/2)$趋于$u(\alpha/2)$，因此，此时有：
\begin{displaymath}
\lim\limits_{n \rightarrow \infty}2\hat{\sigma}[1/n + (x_0-\bar{X})^2/S_x^2]^{1/2} = 0
\end{displaymath}
可见，如果样本容量趋于无穷，回归函数$m(x)$的区间估计就精度越高，区间长度直到趋于0。而对用于预测的情况，我们得到了$Y_0$的区间长度为$2\hat{\sigma}[1 +1/n + (x_0-\bar{X})^2/S_x^2]^{1/2} t_{n-2}(\alpha/2)$。其中有一个常数项1，可见无论n怎么大，总有当$n\rightarrow \infty$，区间长度大于1的情况。故无论样本容量如何，区间预测的精度总是有一个界限，不像用于描述的情况，区间精度可以越来越高。

下面用图像来展示预测和描述两种情况下的区间估计精度差异。
```{r precision_interval_ex, eval=T, echo=F, warning=F, message=F}
set.seed(1234)
x_set <- c(1:100)
rd <- rnorm(100, mean=0, sd=16)
y_ob <- 4*x_set + 10 + rd
lm_ep <- lm(y_ob ~ x_set, data=data.frame(x_set, y_ob))
x_brod <- c(-20:120)

beta0_est <- lm_ep$coefficients[1] + (lm_ep$coefficients[2] * mean(x_set))
beta1_est <- lm_ep$coefficients[2]
sigmavar_est <- sum(lm_ep$residuals^2) / (length(x_set) - 2)
y_est <- lm_ep$fitted.values
y_brod <- predict.lm(lm_ep, newdata = data.frame(x_set = x_brod))
var_x_set <- var(x_set)
t_025 <- qt(0.025, df=length(x_set)-2, ncp=0, lower.tail=F)

mx_var <- sigmavar_est * ((1/length(x_set)) + ((x_brod - mean(x_set))^2 / var_x_set))
y_var <- sigmavar_est * (1 + (1/length(x_set)) + ((x_brod - mean(x_set))^2 / var_x_set))
mx_est_lwr <- y_brod - (sqrt(mx_var) * t_025)
mx_est_upr <- y_brod + (sqrt(mx_var) * t_025)
y_est_lwr <- y_brod - (sqrt(y_var) * t_025)
y_est_upr <- y_brod + (sqrt(y_var) * t_025)
```

```{r precision_graph, eval=T, echo=F, warning=F, message=F, error=F,fig.height=3, fig.width=4}
fig_precision <- ggplot(data=data.frame(x_brod, y_brod, mx_est_lwr, mx_est_upr, y_est_lwr, y_est_upr)) + geom_point(data = data.frame(x_set, y_ob), aes(x_set, y_ob), size=0.5) + geom_line(aes(x_brod, mx_est_lwr), colour="darkgreen") + geom_line(aes(x_brod, mx_est_upr), colour = "darkgreen") + geom_line(aes(x_brod, y_est_lwr), colour="red") + geom_line(aes(x_brod, y_est_upr), colour="red") + geom_line(aes(x_brod, y_brod), colour="blue")
fig_precision <- fig_precision + xlab("x") + ylab("y")
fig_precision
```

上图中，蓝色直线是回归方程是对$m(x)$的点估计，绿色曲线是对回归方程$m(x)$的区间估计，红色曲线是对$Y$的区间估计，可以看到，$m(x)$的区间估计在$x = \bar{X}$的附近基本贴合在回归直线上，这说明在这个范围内的估计$\hat{m}(x)$很接近真实的$m(x)$，具有很高的精度。而在距离$\bar{X}$较远的两端，蓝色直线与绿色曲线的距离越来越大，表明点估计$\hat{m}(x)$与真实的$m(x)$距离越来越远，精度越来越差。

下面来看红色曲线，可以发现红色曲线同样也有与蓝色曲线类似的趋势，在$x = \bar{X}$的附近区间较短，精度较高，在远离$\bar{X}$的区域精度越来越差，但红色曲线所夹区间长度总比绿色曲线长，表明预测的$\hat{Y}$的精度总要比用于描述的$\hat{m}(x)$要差一些。

\subsubsection{假设检验}

在得到线性函数$\hat{m}(x)$后，最有用的是通过假设检验来验证$\hat{m}(x)$中参数$\hat{\beta}_1$和$\hat{\beta}_2$是否接近一个数，最常用的是考虑它们是否与0接近，因为一旦被证明与0接近，尤其是$\hat{\beta}_1=0$被接受，说明线性模型的斜率项为0，那么这说明因变量和自变量之间的相关性很弱。

*1. 对$\hat{\beta}_1$的假设检验*

首先确定假设检验的问题：
\begin{displaymath}
\begin{split}
H_0: \beta_1 &= c;\\
H_1: \beta_1 &\neq c;
\end{split}
\end{displaymath}
根据这个假设检验问题，已经得到了$\beta_1$的估计量为$\hat{\beta}_1$，很容易写出一个用来检验上述假设问题的检验$\Phi$:
\begin{equation}
\begin{split}
\Phi: &|\hat{\beta}_1 - c| \leq \varepsilon,\mbox{接受}H_0;\\
      &|\hat{\beta}_1 - c| > \varepsilon,\mbox{拒绝}H_0
\end{split}
\end{equation}

要让检验$\Phi$有检验水平为$\alpha$，根据检验水平的定义\eqref{jianyanshuiping}：
\begin{displaymath}
\beta_{\Phi} = P(|\hat{\beta}_1 - c| > \varepsilon) \leq \alpha; \quad \forall \beta_1 \in H_0
\end{displaymath}

我们知道$\hat{\beta}_1$有$E(\hat{\beta_1}) = \beta_1$，$\textrm{Var}(\hat{\beta}_1) = \hat{\sigma}^2/S_x^2$，由于$\sigma$未知，所以用$\hat{\sigma}$代替，这样随机变量：
\begin{displaymath}
\frac{\hat{\beta}_1 - \beta_1}{\hat{\sigma}^2 / S_x^2} \sim t_{n-2}
\end{displaymath}

由上面结论，由此推出检验临界值$\varepsilon$：
\begin{displaymath}
\begin{split}
P(\frac{|\hat{\beta}_1 - c|}{\hat{\sigma} S_x^{-1}} &\geq \frac{\varepsilon}{\hat{\sigma} S_x^{-1}}) = \alpha\\
\Phi(-\frac{\varepsilon}{\hat{\sigma}S_x^{-1}}) &+ 1 - \Phi(\frac{\varepsilon}{\hat{\sigma}S_x^{-1}}) = \alpha\\
\Phi(\frac{\varepsilon}{\hat{\sigma}S_x^{-1}}) &= 1 - \frac{\alpha}{2}\\
\frac{\varepsilon}{\hat{\sigma}S_x^{-1}} &= t_{n-2}(\alpha/2)\\
\varepsilon &= \hat{\sigma} S_x^{-1} t_{n-2}(\alpha/2)
\end{split}
\end{displaymath}

由上面的结果，将检验$\Phi$改写为：
\begin{equation}
\begin{split}
\Phi: &|\hat{\beta}_1 - c| \leq \hat{\sigma} S_x^{-1} t_{n-2}(\alpha/2),\mbox{接受}H_0;\\
      &|\hat{\beta}_1 - c| > \hat{\sigma} S_x^{-1} t_{n-2}(\alpha/2),\mbox{拒绝}H_0
\end{split}
\end{equation}

*2. 对$\hat{\beta}_0$的假设检验*

仿照1中的论证过程，直接写出检验$\Psi$为：
\begin{equation}
\begin{split}
\Psi: &|\hat{\beta}_0 - c| \leq \frac{\hat{\sigma}}{\sqrt{n}} t_{n-2}(\alpha/2),\mbox{接受}H_0;\\
      &|\hat{\beta}_0 - c| > \frac{\hat{\sigma}}{\sqrt{n}} t_{n-2}(\alpha/2),\mbox{拒绝}H_0
\end{split}
\end{equation}

\subsection{多元线性回归}
当自变量为多个时，如$X_1, \cdots, X_p$线性模型变成如下形式：
\begin{equation}
\label{mult_linearmodel}
\begin{split}
&Y = b_0 + b_1 X_1 + \cdots + b_p X_p + e\\
&e_1, \cdots, e_n \mbox{同分布};E(e_i)=0;\textrm{Var}(e_i) \in (0, \infty)
\end{split}
\end{equation}
这里假设对这批自变量$X_1, \cdots, X_p$和$Y$进行$n$次观测，得到一组数据，记录为如下：
\begin{displaymath}
\begin{pmatrix}
X_{11} & X_{21} & \cdots & X_{p1} & Y_1 \\
X_{12} & X_{22} & \cdots & X_{p2} & Y_2 \\
\vdots & \vdots & \ddots & \vdots & \vdots \\
X_{1n} & X_{1n} & \cdots & X_{pn} & Y_n
\end{pmatrix}
\end{displaymath}

上面这个矩阵中一行代表了一次观测，同样的类似一元线性模型，也可以写出多元下的中心化模型，只需要算出每个$X_k$在$n$次观测下的平均值，则得到：
\begin{displaymath}
\begin{split}
\bar{X}_k &= \frac{X_{k1} + \cdots + X_{kn}}{n}\\
X_{ki}^* &= X_{ki} - \bar{X}_k \quad (i = 1, \cdots, n;k = 1, \cdots, p)
\end{split}
\end{displaymath}

这样，中心化的多元线性模型表示为：
\begin{equation}
\begin{split}
&Y_i = \beta_0 + \beta_1 X_{1i}^* + \cdots + \beta_p X_{pi}^* + e_i \quad (i = 1, \cdots, n)\\
&\beta_0 = b_0 + b_1 \bar{X}_1 + \cdots + b_p\bar{X}_p\\
&\beta_k = b_k \quad (k = 1, \cdots, p) 
\end{split}
\end{equation}

\subsubsection{最小二乘估计}
为简便，直接写出$\beta_1, \cdots, \beta_n$和$\beta_0$的估计表达式子，详细证明见陈希孺《概率论与数理统计》的6.3节。
首先定义几个矩阵和向量，方便表述。对经过中心化的观测$X_{ki}$，定义一个$p\times n$的矩阵：
\begin{displaymath}
X = \begin{pmatrix}
X_{11} & X_{12} & \cdots & X_{1n}\\
X_{21} & X_{22} & \cdots & X_{2n}\\
\vdots & \vdots & \ddots & \vdots\\
X_{p1} & X_{p2} & \cdots & X_{pn}
\end{pmatrix}
\end{displaymath}
这个矩阵一般称为*设计矩阵*，不过这里的$X_{ki}$是经过中心化的，与原始的矩阵不同。接下来定义一个方阵$L_{p\times p}$，其中元素满足$l_{uv} = \sum\limits_{i=1}^n X_{ui}X_{vi}$。
\begin{displaymath}
L = \begin{pmatrix}
l_{11} & l_{12} & \cdots & l_{1p}\\
l_{21} & l_{22} & \cdots & l_{2p}\\
\vdots & \vdots & \ddots & \vdots\\
l_{p1} & l_{p2} & \cdots & l_{pp}
\end{pmatrix}
\end{displaymath}

把因变量$Y_i$，未知的$\beta$，估计量$\hat{\beta}$以及方程组中的要求解的量$\alpha$写成列向量：
\begin{displaymath}
Y_{(n)} = \begin{pmatrix}
Y_1\\ \vdots \\Y_n
\end{pmatrix},
\beta = \begin{pmatrix}
\beta_1\\ \vdots \\ \beta_n
\end{pmatrix},
\hat{\beta} = \begin{pmatrix}
\hat{\beta}_1\\ \vdots \\ \hat{\beta}_n
\end{pmatrix}
\alpha = \begin{pmatrix}
\alpha_1 \\ \vdots \\ \alpha_n
\end{pmatrix}
\end{displaymath}

由此，写出一个用来解$\alpha$的方程：
\begin{equation}
L \alpha = X Y_{(n)}
\end{equation}
这个方程组称为正则方程，由于$L$是方阵，当且仅当$\textrm{rank}(L) = p$即$L$是一个满秩方阵时，正则方程有唯一解，此时的解就是我们对$\beta$的估计量$\hat{\beta}$：
\begin{equation}
\label{multi_olsestimator}
\begin{split}
\hat{\beta}_0 &= \bar{Y}\\
\hat{\beta} &= L^{-1} X Y_{(n)}
\end{split}
\end{equation}

能够证明\eqref{multi_olsestimator}中的两个估计量满足如下性质
\begin{enumerate}
\item $\hat{\beta}_0$和$\hat{\beta}$都是$\beta_0$和$\beta$的无偏估计
\item $\textrm{Cov}(\hat{\beta}_0, \hat{\beta}_j) = 0(j=1, \cdots, p)$，即$\beta_0$和$\beta_j$之间不相关
\item $\textrm{Var}(\hat{\beta}_0) = \sigma^2/n$，若：
\begin{displaymath}
C = (c_{ij}) = L^{-1}
\end{displaymath}
则有：$\textrm{Var}(\hat{\beta}_j) = c_{jj}\sigma^2$，$\textrm{Cov}(\hat{\beta}_j, \hat{\beta}_k) = c_{jk}\sigma^2$
\end{enumerate}

同样的，也能根据一元线性模型中关于误差方差的估计，来得到多元下的情况，可以证明：
\begin{equation}
\hat{\sigma}^2 = \frac{\sum\limits_{i=1}^n \delta_i^2}{n-p-1}
\end{equation}
同样的当误差服从正态分布$e_i \sim N(0, \sigma^2)$时：
\begin{equation}
\sum\limits_{i=1}^n \delta_i^{2}/ \sigma^2 = \chi_{n-p-1}^2
\end{equation}
这里的自由度为$n - p - 1$，显然一共有$\beta_0$，$\beta_1,\cdots, \beta_p$即$p+1$个参数要估计，故用去$p+1$个自由度。

\subsubsection{假设检验问题}
类似于一元线性模型对误差的要求，这里的多元模型下的假设检验同样在随机误差满足正态分布的前提下进行。由于回归系数$\beta_j$有多个，所以检验问题的提法也有下面这几种。

*1. 单个回归系数的检验*

单个回归系数的检验与一元形式无太大不同，这里检验问题为$H_0:\beta_j=c$，则得到一个$t$检验：
\begin{displaymath}
\Phi: \mbox{当}|\hat{\beta}_j - c| \leq \hat{\sigma}\sqrt{c_{ij}}t_{n-p-1}(\alpha/2)\mbox{时接受$H_0$，不然否定$H_0$}
\end{displaymath}

*2. 全体回归系数皆为0的检验*

此时原假设为$H_0:\beta_1 = \beta_2 = \cdots = \beta_p=0$。当满足这个假设时，多元线性模型\eqref{mult_linearmodel}变成：
\begin{equation}
\label{false_mult}
Y_i = \beta_0 + e_i \quad (i=1, \cdots, n)
\end{equation}
此时说明模型的自变量$X_1, \cdots, X_p$对因变量$Y$关系很小或者无意义。在此模型下的残差平方和可以计算：
\begin{displaymath}
R_2 = \min\limits_{\beta_0}\sum\limits_{i=1}^n (Y_i - \beta_0)^2 = \sum\limits_{i=1}^n (Y_i - \bar{Y})^2
\end{displaymath}
与\eqref{mult_linearmodel}的模型相比，模型\eqref{false_mult}的残差平方和一定不会小于前者，如果前者的残差平方和记为$R_1$，那么$R_2 - R_1$可以作为$H_0$的一个衡量，差值越小，说明越与$H_0$接近。可以证明，当$H_0$成立时，$R_2 -R_1$的分布满足：
\begin{displaymath}
\frac{1}{\sigma^2}(R_2 - R_1) \sim \chi_p^2, \mbox{$R_2-R_1$与$\hat{\sigma}^2$独立}
\end{displaymath}
如果$\sigma^2$未知，用$\hat{\sigma}^2$来代替，则分布变成：
\begin{equation}
\frac{1}{p}(R_2 - R_1)/\hat{\sigma}^2 \sim F_{p, n-p-1}
\end{equation}
这样得到一个检验法，一般称之为$F$检验：
\begin{equation}
\frac{1}{p} \sum\limits_{j=1}^p \hat{\beta}_j \sum\limits_{i=1}^n X_{ji}Y_i / \hat{\sigma}^2 \leq F_{p, n-p-1}(\alpha) \mbox{时接受$H_0$，反之就拒绝$H_0$}
\end{equation}

\subsection{相关分析}
\subsubsection{相关系数的估计和检验}
对于符合联合分布的$N(a, b, \sigma^2_1, \sigma^2_2, \rho)$的两个随机变量$X_1, X_2$，可以用一下式子来衡量两者之间相关性：
\begin{equation}
\label{rho_true}
\rho = \frac{\textrm{Cov}(X_1, X_2)}{\sqrt{\textrm{Var}(X_1)\textrm{Var}(X_2}}
\end{equation}

通过以上式子可以引申出对$\rho$的一个估计方法，若有对上面$X_1,X_2$的一系列观测值$(X_{11}, X_{21})\cdots(X_{1n}, X_{2n})$，则可提出如下的估计量：
\begin{equation}
\label{rho_estimator}
r = \frac{\sum\limits_{i=1}^n(X_{1i} - \bar{X}_1)(X_{2i} - \bar{X}_2)}{\sqrt{\sum\limits_{i=1}^n (X_{1i} - \bar{X}_1)^2 \sum\limits_{i=1}^n (X_{2i} - \bar{X}_2)^2}}
\end{equation}
一般把这个估计量$r$称之为*样本相关系数*

由此提出的一个统计假设问题是关注$H_0:\rho = 0$，可以推出一个检验为：
\begin{displaymath}
\mbox{当}|r| \leq \frac{t_{n-2}(\alpha/2)}{\sqrt{n-2 + t_{n-2}^2(\alpha/2)}}\mbox{时接受$H_0$，不然拒绝$H_0$}
\end{displaymath}

\subsubsection{偏相关与复相关}

*1.偏相关的概念和计算*
现实情况下，两个随机变量之间的关系可能还受到一些其他变量的影响。如人的饮食消费和衣着消费显然是收到总收入的影响的，总收入高的人群有较大可能享受更高水平的饮食和衣着，因此相当于收入同时拉动了衣着和饮食消费。这样在不对收入的影响进行消除的情况下，衣着和饮食的消费呈现正相关关系。如果尝试对收入进行分组，并在某一个组内观测衣着和饮食的关系，可能发现它们是负相关，因为收入一定的情况下，任一消费的增多必然导致其他的消费的下降。

如果使用一种方法消去收入的影响，再来研究衣着和饮食的关系，这是的相关性就称之为衣着和饮食对收入的*偏相关*。
\begin{Definition}
一般地，对$p$个随机变量$X_1, \cdots, X_p$，把$X_3, X_4, \cdots, X_p$的影响从$X_1, X_2$中消去，得到新的部分记为$X^`_1,X_2^`$，那么把$X^`_1,X_2^`$的相关系数记为$X_1, X_2$对$(X_3, \cdots, X_p)$的偏相关系数，并记为$\rho_{12\cdot(34\cdots p)}$  
\end{Definition}

这里的“消去”指的是找到一个线性关系：
\begin{displaymath}
L_1(X_3, \cdots,X_p) = c_0 + c_3 X_3 + \cdots + c_p X_p
\end{displaymath}
使$E[X_1 - L_1(X_3, \cdots, X_p)]^2$最小，这样用$L_1$减去$X_1, X_2$就得到了消去的$X^`_1,X_2^`$
\begin{displaymath}
\begin{split}
X_1^` &= X_1 - L_1(X_3, \cdots, X_p)\\
X_2^` &= X_2 - L_2(X_3, \cdots, X_p)\\
\end{split}
\end{displaymath}
如何找到这样一个$L$这里不做表述，下面给出一个通用的方法来计算偏相关系数。对$p$个自变量$X_1,\cdots,X_p$，首先计算两两之间的相关系数$\rho_{ij}$，得到一个相关系数矩阵：
\begin{equation}
P = \begin{pmatrix}
\rho_{11} & \rho_{12} & \cdots & \rho_{1p}\\
\rho_{21} & \rho_{22} & \cdots & \rho_{2p}\\
\vdots & \vdots & \ddots & \vdots \\
\rho_{p1} & \rho_{p2} & \cdots & \rho_{pp}
\end{pmatrix}
\end{equation}
记$P_{uv}$为$P$的$(u,v)$元子式，即从$P$中划去第$u$行第$v$列的行列式，那么变量$X_1,X_2$的偏相关系数记为：
\begin{equation}
\label{partial_rho}
\rho_{12\cdot(34\cdots p)} = \frac{P_{12}}{\sqrt{P_{11}P_{22}}}
\end{equation}

对偏相关系数的估计可以用样本相关系数来做出，类似的，可以得到在样本$X_{1i}, \cdots, X_{pi} ,(i=1,\cdots, n)$下的样本相关系数矩阵：
\begin{equation}
R = \begin{pmatrix}
r_{11} & r_{12} & \cdots & r_{1p}\\
r_{21} & r_{22} & \cdots & r_{2p}\\
\vdots & \vdots & \ddots & \vdots \\
r_{p1} & r_{p2} & \cdots & r_{pp}
\end{pmatrix}
\end{equation}

则对偏相关系数$\rho_{12\cdot(34\cdots p)}$的估计为：
\begin{equation}
\label{partial_rho_estimator}
r_{12\cdot(34\cdots p)} = \frac{R_{12}}{\sqrt{R_{11}R_{22}}}
\end{equation}

*2.复相关的概念和计算*

也有可能有这样一种情况，一组变量$X_1, \cdots, X_p$中，$X_1$可能对单独的$X_j$关系不显著，但却对全体的$X_2,\cdots, X_p$中的某个线性组合有较强的相关性。例如某农作物的产量与它一年内各个月的降水量，可能与某个月的降水量关系不大，但它肯定与一年的降雨总量有较强的关系。这种情况下的相关性就称之为*复相关*

类似于前面的偏相关，这里是要找一个线性组合$L(X_2, \cdots, X_p)$，使得$E[X_1 - L(X_2, \cdots, X_p)]^2$最小，然后求出$X_1$与$L(X_2, \cdots, X_p)$的相关系数。这里的求法和偏相关也类似，都是先给出相关系数矩阵，然后根据式子：
\begin{equation}
\label{multiple_correlation}
\rho_{1(23\cdots p)} = \sqrt{1 - \frac{|P|}{P_{11}}}
\end{equation}

同样的，用样本相关系数来估计偏相关系数：
\begin{equation}
\label{multiple_correlation_estimator}
r_{1(23\cdots p)} = \sqrt{1 - \frac{|R|}{R_{11}}}
\end{equation}

\subsection{方差分析}
\subsubsection{单因素完全随机化试验的方差分析}

单因素的试验指得是只有一个因素影响试验结果的情况，如试验几种降压药对治疗高血压的作用，这里影响高血压疗效的只有“降压药”一个因素，而药品的种类就是因素的*水平*，一般为了消除随机误差，会把一个水平的药物重复试验几次，这就叫做某个水平的*重复度*。

```{r var_ex_table, eval=T, echo=F, warning=F, message=F, error=F}
tom <- rep(c("A", "B", "C", "D"), each=3)
patientid <- c(1:length(tom))
res_tom <- c(0.85, 0.87, 0.84, 0.40, 0.45, 0.44, 0.90, 0.93, 0.88, 0.76, 0.75, 0.77)
dat <- data.frame(patientid, tom, res_tom)
colnames(dat) <- c("PatientID", "Medicine", "Result")
dat
```

如上面的例子，这就是一个有四个水平，每个水平重复度为3的单因素试验结果。

考虑只有一个因素$A$，有$k$个水平，以$Y_{ij}$代表第$i$个水平的第$j$个观察值，那么单因素方差模型为：
\begin{equation}
\label{single_variancemodel}
Y_{ij} = a_i + e_{ij} \quad (j=1, \cdots, n_i;i=1, \cdots, k)
\end{equation}
这里$a_i$相当于在水平$i$下的理论平均值，称为水平$i$的*效应*，$e_{ij}$是对水平$i$第$j$次观测时的随机误差，对随机误差的要求有:$E(e_{ij}) = 0, \textrm{Var}(e_{ij}) \in (0, \infty)$且所有$e_{ij}$独立同分布。$n_i$是各个水平下的重复度，一般我们设计试验是让各个水平的重复度一样，但也有不一样的情况。

单因素试验一个重要目的就是研究各个水平的效应是否相同，从模型上来说就是各个$a_i$是否相同，如果能验证得到$a_i$是不同的，就能说明因素$A$的效应是显著的，反之$a_i$相同，说明因素$A$的效应不显著。实际上就是考虑假设问题：
\begin{equation}
\label{same_ahyper}
H_0 : a_1 = a_2 = \cdots = a_k
\end{equation}

如果因素$A$是显著的，那么各个水平之间的效应值应该不同，则各个水平之间观测值的差异应该比较大，为了衡量这种“全体差异”，可以考虑如下变量：
\begin{equation}
\label{error_sumsquare}
SS = \sum\limits_{i=1}^k \sum\limits_{j=1}^{n_i} (Y_ij - \bar{Y})^2 \quad (\bar{Y} = \sum\limits_{i=1}^k \sum\limits_{j=1}^{n_i} \frac{Y_{ij}}{n})
\end{equation}

内部的$\sum\limits_{j=1}^{n_i}$相当于对水平$i$下的$n_i$个重复观测与总体均值的差的平方进行加和，外部的$\sum\limits_{i=1}^{k}$相当于对$k$个水平进行的误差平方和进行加和。容易理解如果$SS$越大，表明$Y_{ij}$之间差异越大，而$Y_{ij}$的差异由模型\eqref{single_variancemodel}可知，是由两部分引起，一个是$a_i$即各个水平的效应，另一个就是每次观测的随机误差$e_{ij}$。如果能对$SS$进行分解，得到因素$A$各个水平造成的部分和随机误差造成的部分，就能比较这差异的影响哪部分更大。

首先看随机误差造成的影响，把这部分误差平方和记为$SS_e$，可以知道，在一个水平下的所有观测值如$Y_{1j}, \cdots, Y_{1n_i}$，它们之间产生差异的原因必然是随机误差导致，因为同一水平下效应值都是$a_1$，这与$a_i$本身的差异无关。所以仿照上面计算总体误差平方和的思路，把每个观测$Y_{1j}$与这个水平下的所有观测的均值$\bar{Y}_1$比较再做平方和：
\begin{equation}
\label{sse_cal}
SS_e = \sum\limits_{i=1}^{k} \sum\limits_{j=1}^{n_i} (Y_{ij} - \bar{Y}_i)^2 \quad (\bar{Y}_i = \frac{Y_{i1} + \cdots + Y_{in_{i}}}{n_i})
\end{equation}

这样就得到了误差所占的部分计算方法，接下来考虑因素$A$解释的部分，这里记为$SS_A$，容易知道$SS_A = SS - SS_e$，这样就能计算出$SS_A$的表达式：
\begin{displaymath}
SS - SS_e = \sum\limits_{i=1}^k \sum\limits_{j=1}^{n_i} (Y_ij - \bar{Y})^2 - \sum\limits_{i=1}^{k} \sum\limits_{j=1}^{n_i} (Y_{ij} - \bar{Y}_i)^2
\end{displaymath}
为方便，先对$SS$进行分解，有$Y_{ij} - \bar{Y} = (Y_{ij} - \bar{Y}_i) + (\bar{Y}_i - \bar{Y})$，先对以上的式子进行平方，再固定$i$的情况下对$j$进行加和：
\begin{displaymath}
\begin{split}
\sum\limits_{j=1}^{n_i} (Y_{ij} - \bar{Y})^2 & = \sum\limits_{j=1}^{n_i} (Y_{ij} - \bar{Y}_i)^2 + \sum\limits_{j=1}^{n_i} (Y_i - \bar{Y})^2 + \sum\limits_{j=1}^{n_i}2(Y_{ij} - \bar{Y}_i)(\bar{Y}_i - \bar{Y})\\
&= \sum\limits_{j=1}^{n_i} (Y_{ij} - \bar{Y}_i)^2 + n_i(Y_i - \bar{Y})^2 + 0
\end{split}
\end{displaymath}
以上式子对$i=1,\cdots, k$加和再与$SS_e$相减，很容易就得到$SS_A$:
\begin{displaymath}
\begin{split}
SS_A &= \sum\limits_{i=1}^{k} \sum\limits_{j=1}^{n_i} (Y_{ij} - \bar{Y}_i)^2 + \sum\limits_{i=1}^{k} n_i(Y_i - \bar{Y})^2 - SS_e\\
&= \sum\limits_{i=1}^{k} \sum\limits_{j=1}^{n_i} (Y_{ij} - \bar{Y}_i)^2 + \sum\limits_{i=1}^{k} n_i(Y_i - \bar{Y})^2 - \sum\limits_{i=1}^{k} \sum\limits_{j=1}^{n_i} (Y_{ij} - \bar{Y}_i)^2\\
&= \sum\limits_{i=1}^{k} n_i(Y_i - \bar{Y})^2
\end{split}
\end{displaymath}
这样就得到了$SS_A$的表达式：
\begin{equation}
\label{ssa_cal}
SS_A = \sum\limits_{i=1}^{k} n_i(Y_i - \bar{Y})^2
\end{equation}

有了总平方和，误差平方和以及因素$A$的平方和，就可以通过设计一个检验统计量来考虑假设问题\eqref{same_ahyper}的检验。首先还是假定随机误差$e_{ij} \sim (0, \sigma^2)$，若记：
\begin{displaymath}
MS_A = \frac{SS_A}{k - 1} \quad MS_e = \frac{SS_e}{n-k}
\end{displaymath}
则统计量$MS_A / MS_e$在误差正态假定下满足分布：
\begin{displaymath}
\frac{MS_A}{MS_e} \sim F_{k-1, n-k}
\end{displaymath}
这里$MS_A$和$MS_e$是因素$A$和随机误差的平均平方和，这个检验统计量的目的很明显，如果$MS_A$与$MS_e$之比小于一定的值，说明$MS_e$占据的部分更大，那么我们应该接受$H_0$，由此设计的检验为：
\begin{equation}
\mbox{当}MS_A / MS_e \leq F_{k-1, n-k}(\alpha)\mbox{时接受$H_0$,不然否定$H_0$}
\end{equation}

把总平方和分级成误差平方和$SS_e$和因素平方和$SS_A$的操作就称为*方差分析*，由此进行的检验可以帮助我们解决章节开始的例子中判断减压药这效果是否有区别的问题。

\begin{lemma}
为验证四种减压药A，B，C，D是否有疗效上的区别，设计了一个单因素重复度为3的实验，实验结果如下：
\begin{enumerate}
\item 药物A: `r res_tom[1:3]`
\item 药物B: `r res_tom[4:6]`
\item 药物C: `r res_tom[7:9]`
\item 药物D: `r res_tom[10:12]`
\end{enumerate}
\end{lemma}

```{r ex5, eval=T, echo=F, warning=F, message=F, error=F}
mean_restom <- mean(dat$Result)
ss_restom <- sum((dat$Result - mean_restom)^2)
sse_restom <- sum((res_tom[1:3] - mean(res_tom[1:3]))^2) + sum((res_tom[4:6] - mean(res_tom[4:6]))^2) + sum((res_tom[7:9] - mean(res_tom[7:9]))^2) + sum((res_tom[10:12] - mean(res_tom[10:12]))^2)
ssa_restom <- ((mean(res_tom[1:3]) - mean_restom)^2 +(mean(res_tom[4:6]) - mean_restom)^2 +(mean(res_tom[7:9]) - mean_restom)^2 + (mean(res_tom[10:12]) - mean_restom)^2) * 3

mse_restom <- sse_restom / (12 - 4)
msa_restom <- ssa_restom / (4 - 1)
ftest <- msa_restom/mse_restom
fval <- qf(0.05, 3, 8, 0, lower.tail = F)
```
首先进行方差分析
\begin{displaymath}
\begin{split}
SS &= (`r res_tom[1]` - `r mean_restom`)^2 + \cdots + (`r res_tom[12]` - `r mean_restom`)^2 = `r ss_restom`\\
SS_e &= (`r res_tom[1]` - `r mean(res_tom[1:3])`)^2 + \cdots +(`r res_tom[12]` - `r mean(res_tom[10:12])`)^2 = `r sse_restom`\\
SS_A &= 3 \times (`r mean(res_tom[1:3])` - `r mean_restom`)^2 + \cdots + 3 \times (`r mean(res_tom[10:12])` - `r mean_restom`)^2 = `r ssa_restom`
\end{split}
\end{displaymath}

检查$SS = SS_e + SS_A$无误后，开始进行假设检验，检验问题即四种药物的效果是否相等。
\begin{displaymath}
\begin{split}
MS_A &= \frac{`r ssa_restom`}{4-1} = `r msa_restom`\\
MS_e &= \frac{`r sse_restom`}{12-8} = `r mse_restom`\\
F &= \frac{MS_A}{MS_e} =  `r ftest`
\end{split}
\end{displaymath}
查表得$F_{3, 8}(0.05) = `r fval`$，显然有$`r ftest` > `r fval`$，所以$H_0$不成立，这说明方差方差分析表明四种药物的效果是不同的。在方差分析证明了因素$A$几个水平之间确实存在不同的效应值后，就可以进行一步分析了，如均值的多重比较。如果无法拒绝水平效应相等的零假设，则可能是这个因素确实对试验影响不大，又或者是试验的随机误差太大导致真实的差别被掩盖，这时就需要重新考虑试验设计了。

\subsubsection{两因素完全试验的方差分析}

如果要研究的对象被多个因素影响，设有因素$A, B, C, \cdots,$，每个因素都有水平$k_1, k_2, k_3,\cdots$，如果要完全研究这些因素，就要设计$k_1 \times k_2 \times k_3 \cdots$个试验。这种试验方法称为*完全试验*，如果只对一部分水平组合做实验，就称为*部分试验*。
为简单，现考虑两因素的完全试验，设有两个因素$A$和$B$，他们的水平分别为$k$个和$l$个，每个水平组合$(i, j)$下的实验结果记为$Y_{ij}$，那么统计模型表述为：
\begin{equation}
\label{bivariate_devmodel}
Y_{ij} = \mu + a_i + b_j + e_{ij} \quad (i=1, \cdots, k;j=1, \cdots, l)
\end{equation}
随机误差$e_{ij}$的同样满足$E(e_{ij}) = 0;\textrm{Var}(e_{ij}) \in (0, \infty)$，而另一部分$\mu + a_i + b_j$是在水平组合$(i, j)$下的平均效应。其中$\mu$是总体平均，是一切水平组合下的平均值，相当于一个基准。$a_i$是因素$A$在水平$i$下的平均效应，越大表明效应越明显，$b_j$的意义与$a_i$类似。我们还要让其满足如下条件：
\begin{equation}
\label{constrait_for_twolevels}
a_1 + \cdots + a_k = 0, \quad b_1 + \cdots + b_l = 0
\end{equation}
加上以上的约束条件后，可以很容易的给出一个$\mu$的估计量$Y_{..}$，首先把$Y_{ij}$对$i$和$j$相加：
\begin{equation}
\begin{split}
&\sum\limits_{i=1}^k \sum\limits_{j=1}^l Y_{ij} = kl\mu + \sum\limits_{i=1}^k \sum\limits_{j=1}^l e_{ij}\\
&\mbox{同时除以$kl$，注意$E(e_{ij})=0$}\\
&Y_{..} = \sum\limits_{i=1}^k \sum\limits_{j=1}^l \frac{Y_{ij}}{kl}
\end{split}
\end{equation}
用类似的方法，可以分别得到$a_i$和$b_j$的估计量，把$Y_{ij}$只对$i$或$j$相加：
\begin{equation}
\label{twolevels_estimator}
\begin{split}
\sum\limits_{j=1}^l Y_{ij} &= l \mu + la + \sum\limits_{j=1}^l e_{ij}\\
Y_{i.} &= \sum\limits_{j=1}^l \frac{Y_{ij}}{l}\\
\hat{a}_i &= Y_{i.} - Y_{..} \quad (i = 1, \cdots, k)\\
&\mbox{类似，对$b_j$有}\\
\hat{b}_j &= Y_{j.} - Y_{..} \quad (j = 1, \cdots, j)
\end{split}
\end{equation}

有了$\mu, a_i, b_j$的估计量，可以进一步给出此情况下的方差分析：
\begin{equation}
\label{twolevels_devariation}
\begin{split}
SS &= SS_A + SS_B + SS_e\\
&= l \sum\limits_{i=1}^k (Y_{i.} - Y_{..})^2 + k \sum\limits_{j=1}^l (Y_{.j} - Y_{..})^2 + \sum\limits_{i=1}^k \sum\limits_{j=1}^l (Y_{ij} - Y_{i.} - Y_{.j} + Y_{..})^2
\end{split}
\end{equation}

有了以上的方差分析，并注意$SS_A$和$SS_B$的自由度就是本身的水平数目减去1，$SS$自由度为水平组合数$kl$减去1，而误差$SS_e$的自由度为$(kl-1) - (k-1) - (l-1) = (k-1)(l-1)$，可以用单因素方差分析中的检验方法，即先计算$MS_A, MS_B, MS_e$，再把$MS_A/MS_e, MS_B/MS_e$与对应$F$值比较来验证假设问题:
\begin{displaymath}
\begin{split}
&H_{0A} : a_1 = \cdots = a_k = 0\\
&H_{0B} : b_1 = \cdots = b_l = 0
\end{split}
\end{displaymath}
是否成立。

\section{附录}
\subsection{一型、二型和三型方差分析}

在实际使用方差分析时，会发现许多软件都提供三种方差分析，分别称为“Type-i”，“Type-ii”和“Type-iii”。这里简称为“一型”、“二型”和“三型”方差分析。在R语言里，stat包里anova函数采用的是一型的方差分析，如果要使用后面两种，需要额外加载car包并使用里面的Anova函数。这三种方差分析，尤其是一型与其他两种之间，有较大的不同，下面就来介绍这三种方差分析的不同之处。

\subsubsection{一型方差分析}
一型方差分析是有顺序性的，如果对于一个两个因子$A$和$B$的方差模型：
\begin{displaymath}
y_{ij} = \mu + a_i + b_j + c_{ij} + e_{ij}
\end{displaymath}
用$SS_A,SS_B, SS_{AB},SS_{A,B}$分别代表因子$A$，因子$B$，交互效应和总体模型的方差，一型方差分析会按照如下顺序计算：
\begin{enumerate}
\item 首先计算$SS_A$
\item 去除因子$A$的效应来计算$SS_{B}$
\item 去除因子$A$和$B$的效应计算交互项$SS_{AB}$
\end{enumerate}
一型方差分析是根据模型里因子的顺序来计算方差的，对于非均衡的因子层次，这种计算方式会因为计算顺序不同而导致结果差异，进而产生功效不足的问题。（对均衡设计的实验，这三种方差分析结果是相同的）

以$mtcars$数据集为例，这个数据集有多个因子对因变量$mpg$产生影响，现只考虑变量$vs$和$gear$，这时数据的层次如下
```{r vartype_ex, echo=F, eval=T, message=F, warning=F, error=F}
library(car)
with(mtcars, table(vs, gear))
```

可以看到因子$gear$有三个层次，$vs$有两个层次，各个层次组合里观测数不一样，是一个非均衡实验设计。如果使用的一型方差分析。结果会随着因子顺序不同而变化：

```{r change_var1, echo=T, eval=T, message=F, warning=F, error=F}
mtc <- mtcars
mtc$vsf <- as.factor(mtcars$vs)
mtc$gearf <- as.factor(mtcars$gear)
#效应gear在前
anova(lm(mpg ~ gearf + vsf, data=mtc))
#效应vs在前
anova(lm(mpg ~ vsf + gearf, data=mtc))
```

可以看到，结果发生了变化，把$gear$当第一个变量作为主效应时，具有强的显著性，而当成第二个变量的显著性就要低的多。这种情况对一型检验的功效性产生了影响，如果检验统计量$F$恰好在设定的$u_{\alpha}$附近，调换位置就是否接受零假设造成很大影响。

\subsubsection{二型方差分析}
二型方差分析没有主效应和副效应的区分，对所有的效应都是在去除其他效应影响的前提下进行，也就是说
\begin{enumerate}
\item 去除因子$B$的效应来计算$SS_{A}$
\item 去除因子$A$的效应来计算$SS_{B}$
\item 去除因子$A$和$B$的效应计算交互项$SS_{AB}$
\end{enumerate}

这样来看，二型分析其实可以用多次一型分析来代替，首先进行先$A$后$B$的一型分析，然后进行先$B$后$A$的一型分析，这样作为副效应的两个因子的$F$值是一样的。

R语言下默认没有提供二型分析，需要加载car包使用Anova函数来进行
```{r change_var2, echo=T, eval=T, message=F, warning=F, error=F}
#效应gear在前
Anova(lm(mpg ~ gearf + vsf, data=mtc), type=2)
#效应vs在前
Anova(lm(mpg ~ vsf + gearf, data=mtc), type=2)
```

可以看出，对不均衡设计，使用二型方差分析，即使交换了因子顺序也不会产生结果上的变化。这样比一型方差分析功效更强。

\subsubsection{三型方差分析}

前面的两种方差分析都没有在方差分解时去除交互效应的影响，而三型方差分析假定交互效应是存在且显著的，并且总是会从方差分解中去除这一影响。
\begin{enumerate}
\item 去除因子$B$和交互项的效应来计算$SS_{A}$
\item 去除因子$A$和交互项的效应来计算$SS_{B}$
\item 去除因子$A$和$B$的效应计算交互项$SS_{AB}$
\end{enumerate}

三型方差分析同二型一样，不会受方差分解时因子的顺序而影响，但当交互效应不显著或者不存在时，三型方差分析会大大降低分析的功效，让一些本来显著的因子得出不甚显著的结果，还是用$mtcars$数据来做例子
```{r change_var3, echo=T, eval=T, message=F, warning=F, error=F}
#效应gear在前
Anova(lm(mpg ~ gearf * vsf, data=mtc), type=3)
#效应vs在前
Anova(lm(mpg ~ vsf * gearf, data=mtc), type=3)
```
可以看到原本显著的因子$vs$和$gear$现在显著性大大降低，这样的结果与前面的相差很大。

\subsubsection{三种方法的比较}

对因子层次均衡且因子直接相互正交的实验数据（即不存在交互效应），三种方法结果上没有区别，如果是不均衡的实验设计且确定因子之间存在交互作用，三型分析较之二型分析更有效，反之，交互效应不显著，二型分析更优。

可以按照如下的步骤来进行选择使用何种方差分析：
\begin{enumerate}
\item 先用一型方差分析来探索模型，确认交互项是否存在
\item 如果存在交互效应，使用三型方差分析更优，如果不存在，使用二型
\item 对均衡的因子设计，没有发现交互效应时，三种方法结果一样
\end{enumerate}

关于如何选择方法上，Øyvind Langsrud的论文“ANOVA for unbalanced data: Use Type II instead of Type III sums of squares”有详细论述。

\subsection{对p值的理解}

在计算许多统计模型的参数时，现今的统计软件常常计算模型中参数的P值来辅助我们进行统计推断，并且常常有一个广泛使用的P值分割点$p.value < 0.05$来作结果“是否显著”的准则，然而统计学家设计P值的初衷却不是如此，这一广泛使用的教条给科学推断造成了一些麻烦，本文的目的在于弄清P值是什么以及把它和另一个常用的统计推断体系-基于Neyman-Pearson假设检验理论的统计推断来做一个区分。

\subsubsection{实验的显著性-Fisher的思想实验}

Fisher在他的《The Design of Experiments》一书中用一个思想实验来提出实验的“显著性”这一观点，这个实验可以帮助我们理解P值的定义。

\begin{lemma}
一位女士声称她能够尝出一杯奶茶是先加入茶还是先加入奶制成的，为了验证她的这个能力，实验者拿来8杯茶，4杯是先加入茶制成，另外4杯则是先加入奶制成。在实验前，实验者告诉这位女士，她接下来要品尝8杯茶，这8杯茶里有4杯先加茶，4杯先加奶。她的任务则是区分出哪些奶茶是先加奶或先加茶。实验进行时，为了防止人为因素干扰，实验者按照随机的顺序递给女士奶茶，女士品尝完后把她的结果告诉实验员。
\end{lemma}

如果女士确实有这个能力，即准确无误的区分两种奶茶，那无论怎么调换给她奶茶的顺序，她总能得到正确的结果。如果这位女士实际上不能区分，那么她可能给出的实验结果有$\textrm{C}_8^4=70$种（这相当于从8杯茶里随机拿四杯，认为它是先加奶的，那么剩下的就是先加茶的）。显然这70种组合已经包含了所有可能性，包括那个正确的组合。

用随机变量$X$来表示这位女士在每种奶茶中答对的次数，如果这位女士真的得到了正确的结果，并且我们已经假定了这个女士没有区分奶茶的能力，那么相当于她在两种奶茶的测试中都答对了，这样就有$X=4$

\begin{displaymath}
\textrm{P}(X=4|\mbox{无能力区分}) = \frac{1}{70} \simeq 0.014
\end{displaymath}

如果这位女士没有区分奶茶的能力，她得到正确结果的可能性只有$1.4\%$，这是一个比较小的概率，在这个实验中，我们可以这么说：“*在假定女士没有分辨能力的前提下*，这位女士正确区分8杯茶的可能性是0.014，我们认为在这种前提下，产生这种结果的可能性是很低的”

如果两组奶茶中，这位女士各答对3次错误1次，那么此时的概率应该为：
\begin{displaymath}
\textrm{P}(X=3|\mbox{无能力区分}) = \frac{\textrm{C}_4^3 \times \textrm{C}_4^3}{70} = \frac{16}{70} \simeq 0.229
\end{displaymath}

这个概率比$5\%$大多了，那我们可以这么说这个实验：“*在假定女士没有分辨能力的前提下*，这位女士的结果是两种奶茶各答对3次弄错1次，在这种前提下，产生这种结果的可能性是0.229，这种可能性比较大，不应该被忽视”

Fisher认为除了计算这个“答对3次答对1次”的概率外，还要计算*更好情况下*的结果，即“全答对”的情况，那么这个概率就变成了$\textrm{P}(X\geq3|\mbox{无能力区分})=(16 +1)/70 \simeq 0.243$。那么这个实验的结果可以这样说：“*在假定女士没有分辨能力的前提下*，这位女士的结果是两种奶茶各答对3次弄错1次，在这种前提下，这位女士答对大于等于3次的可能性是0.243，这种可能性比较大，不应该被忽视”

Fisher把计算这个概率并给出相关推断的过程称为“Test of significance”，他把计算得到的概率称为"Significance"即实验的显著性。在上面的例子中，如果实验结果是这位女士全部答对，我们计算得到这位女士在无能力前提下的概率是$0.014$，是一个比较小的概率，我们称这一实验结果是“显著的”。

在前面的计算中，可以发现我们实际是计算的一个条件概率，这个条件是“这位女士没有分辨能力”。Fisher把这个条件称之为实验的*零假设*（null hypothesis）。零假设的特点是确切的并且是可验证的，正如这个实验的零假设一样。反过来，如果我们的零假设是“这个女士有分辨能力”，那么无论我们做多少次实验并得到女士答对的结果，都不能成为支持这个假设的证据，因为只要在某次实验中这个女士没有答对，假设就不成立。在这个前提下，通过有限次数的实验来验证这个零假设是不可能的。

\subsubsection{P值的基本定义}

P值基本可以认为是Fisher推广并发扬光大的一种统计推断方法，Fisher认为，要想检验一个样本是否符合某种统计上的特征，可以用以下下思路来进行。为了方便描述，首先假设通过一次抽样得到一组样本$x_1, x_2,\cdots, x_k$，需要看看这组样本是否是来自满足分布为$N(0, 1)$的总体。要想完整的解决这个问题，需要考虑如下三个方面：

\begin{enumerate}
\item 样本是否是来自均值为0的总体
\item 样本是否是来自方差为1的总体
\item 样本是否是来自正态分布的总体
\end{enumerate}

以上三个问题就引申出来三个针对样本的假设，即*零假设*，为了方便，我们只考虑第一个问题并确认样本的确来自方差为1的正态总体，这样的话，零假设可以这么提：样本是来自均值为0的总体。有了这样的假设，我们用Fisher的思想来看看怎么来用P值来作统计推断
\begin{enumerate}
\item 假定零假设是正确的，在这样的前提下，用样本均值$\bar{X}$来做检验统计量
\item 在零假设的前提下，这样问题就变成了产生比$\bar{X}$“更极端”的数据的概率是多少？
\item 显然这个问题可以用计算条件概率来解决，即变成计算$\textrm{Pr}(|\bar{X}|>0|\mu=0)$
\end{enumerate}

Fisher把他的这套检验称为“test of significance”，即“显著性检验”，把P值称为结果的“显著性”，并且认为出现小的P值往往意味着强的显著性。这样可以引申出一个针对P值的定义：

\begin{Definition}
P值：在一个特定的统计模型下，一个统计量出现与观测值相等或更极端的事件的概率
\end{Definition}

这里的“特定”的统计模型，一般指的是针对问题而提出的一个*明确*、*可验证*的零假设，而“更极端”的则有三种情况来计算P值：

\begin{enumerate}
\item $\textrm{P}(X|X \geq C|H_0)$的右尾事件
\item $\textrm{P}(X|X \leq C|H_0)$的左尾事件
\item $2\min\{\textrm{P}(X\leq C|H_0),\textrm{P}(X \geq C|H_0)\}$的双尾事件
\end{enumerate}

```{r extrem_events, eval=T, echo=F, warning=F, message=F, error=F, fig.height=3, fig.width=4}
x <- seq(-5, 5, 0.1)
y <- dnorm(x, mean=0, sd=1.5)
const <- qnorm(0.05, mean=0, sd=1.5)
rxseq <- seq(-const, 5, 0.5)
lxseq <- seq(-5, const, 0.5)
n <- length(rxseq)
ryseq <- dnorm(rxseq, mean=0, sd=1.5)
lyseq <- dnorm(lxseq, mean=0, sd=1.5)
rxseq <- rep(rxseq, each=2)
lxseq <- rep(lxseq, each=2)
ryseq <- rep(ryseq, each=2)
lyseq <- rep(lyseq, each=2)
s <- rep(c(T, F),n)
ryseq[s] <- 0
lyseq[s] <- 0
rig <- data.frame(x=rxseq, y=ryseq)
lef <- data.frame(x=lxseq, y=lyseq)
fig_extrem <- ggplot() + geom_line(data=data.frame(x, y), aes(x, y), colour="blue")
fig_extrem <- fig_extrem + geom_line(data=rig, aes(x, y)) + geom_line(data=lef, aes(x, y))
fig_extrem <- fig_extrem + xlab("set of possible observations") + ylab("Probability density")
fig_extrem
```

上面的图像就是一个双侧事件的例子，这个更极端的情况就是指当数据落在折线段的区域时的情形。

Fisher的这个思想有个很容易落入的陷阱，它并不能回答*零假设是否真实*的统计问题。P值是在*零假设为真*的前提下计算得到的，是一个条件概率，也就是说，如果我们得到一个很小的P值，那么有两种可能性。

\begin{enumerate}
\item “更极端”事件确实是一个不寻常发生的事件
\item 零假设$H_0$可能有问题
\end{enumerate}

一般来说，如果缺少进一步信息，我们便倾向认为，这个零假设不太可能是真实。但实际上，Fisher认为单独考虑P值是不恰当的，他本人拒绝把P值作为唯一的给出科学推断的准则，他认为：

  >... no scientific worker has a fixed level of significance at which from year to year, and in all circumstances, he rejects hypotheses; he rather gives his mind to each particular case in the light of his evidence and his ideas.--*Statistical Methods and Scientific Inference*
  
  >In order to assert that a natural phenomenon is experimentally demonstrable we need, not an isolated record, but a reliable method of procedure, In relation to the experimentally demonstrate when we know how to conduct an experiment that will rarely fail to give us a statistically significant result.--*The Design of Experiments*
  
更进一步的，Goodman对Fisher这一观点的评论，他写道：

  >Fisher suggested that it [the P value] be used as part of the fluid, non-quantifiable process of drawing conclusions from observations, a process that included combining the P value in some unspecified way with background infromation
  
显然，Fisher认为一次实验结果只是“许多实验结果”中的一种可能，它对应的P值也是也仅仅是许多可能P值中的一种而已。如果我们能确认，我们实施的实验总是能给出一个显著性的结果，结合P值，我们才有足够的理由来确实一个实验现象是否存在，换句话说，一次实验的显著性不能说明问题，多次实验都展现显著性，才能说明问题，即实验的可重复性是更为重要的。

用一个例子来说明Fisher对如何解释P值的看法
\begin{lemma}
一个实验者使用双尾$t$检验做基于Fisher的统计推断，在自由度为5时，双尾检测给出的$t=4.09$，此时P值为$P=0.0094$。实验者翻阅以前的文献和实验记录，没有发现与这次实验结果相矛盾的地方，实验的误差也不会导致实验结果倾向零假设
\end{lemma}

这个例子就满足了Fisher提到的“重复性”，以前的实验和文献都出现了类似结果，并且实验本身也是可靠的，说明出现显著性结果并不是孤立偶然事件，那么我们就有理由相信，这次实验是一个显著性的结果，是对反驳零假设的一个有力的证据。如果这个实验者得出了$P=0.048$的结果呢？这个值显然比$0.0094$大的多，实验者为了确认这一点，需要重新设计实验来进行相同的假设检验，或者结合其他的实验结果来解释这个P值


\subsubsection{Neyman-Pearson的假设检验体系}

本笔记中介绍的假设检验体系就是Neyman-Person提出的假设检验体系，Neyman-Person假设检验体系首先*明确要求研究者要提出一组对立的命题*，也就是提出零假设和*对立假设*，然后在此基础上引入了一类错误和二类错误的观念，在研究开始前要以控制一类错误，尽可能减少二类错误为目的来设计设计实验。一般在这个体系下，用一个很小的数值$\alpha$来代表推断时出现一类错误的可能，用$\beta$来表示出现二类错误的可能，用$1-\beta$来表示实验的*功效*。这样，通过限制$\alpha$，控制$\beta$来设计实验方案（$\alpha$）以及调查的样本数量（$\beta$）。

在此基础上，可以给出此次实验的功效函数，通过功效函数进一步给出一个阈值，这个阈值称之为*检验的临界值*，临界值把检验统计量划分成两个区域，*接受域*和*否定域*，如果检验统计量落入*接受域*，则零假设被接受，如果落入*否定域*，则否定零假设，此时把检验统计量落入否定域的情形称之为“显著的”实验结果。

Neyman-Pearson的假设检验体系更像一个全局的控制手段，他们的意思在于，只要以控制一类错误和二类错误的观念来设计实验，那么我们有把握保证，在长期的实验结果上，一类错误的概率能小于$\alpha$，功效能达到$1-\beta$（往往表示成，当效应确实不存在时，有$\alpha$的概率误报效应存在，当效应确实存在时，有$1-\beta$的概率能检测出，因为零假设往往是假设效应不发生或不明显的）。

Neyman-Pearson是这样解释他们的假设检验的意义的，他们这样写道：

  > We are inclined to think that as far as a particular hypothesis is concerned, **no test based upon the theory or probability can by itself provide any valuable evidence of the truth or falsehood of that hypothesis.**
  
  >But we look at the purpose of tests trom another view-point, Without hoping to know wheter each separate hypothesis is true or false, we may search for **rules to govern our behaviour with regard to them**, in following which we insure that, **in the long run of experience, we shall not be too often wrong.** 
  
可以看到，Neyman和Pearson在第一段话中针锋相对的批判了Fisher的基于P值，也就是他说的“基于概率”的检验方法，他们甚至认为这种基于孤立事件的检验是毫无意义的。在第二段中，他们则提出了自己的观点，他们认为，虽然不能回答某次实验中的假设是否正确，但只要这个实验是按照一定的规则设计的，那么，在对实验结果进行统计推断时，犯错误的概率在长期上来说大大减少。
  
同样用一个类似的例子来说明如何用Neyman-Pearson的体系来解释实验结果：
\begin{lemma}
一个实验者准备进行一次实验，他翻阅以前的实验数据和文献，认为把实验的$\alpha$设置成$0.05$，$\beta$设置成$0.1$较为合适，依给定的这两个值，他计算出了实验应该要检测的效应大小和样本容量。在实验完成后，他对实验数据进行双侧$t$检验，检验的自由度为5，依据实验前设定的$\alpha$，他计算出$t$值的临界值为$2.57$，而实验给出的$t=4.09$。而双侧事件的拒绝域是$|t| > t_{\alpha=0.05}$
\end{lemma}

依据Neyman和Pearson的观点，这个实验在全局控制一类错误的条件下进行，如果实验实施无误，一类错误的可能不会大于$0.05$，根据此规定计算出拒绝域和接受域，而实验结果恰好落在拒绝域上，所以我们有理由相信，在长期的实验结果上，零假设应该被拒绝。我们得到了显著的实验结果。如果我们此时计算得到$t=2.46$怎么办，似乎这个值已经很接近临界值了，同样的，依据$|t| > t_{\alpha=0.05}$的规则，我们还是拒绝零假设。还是应该声明实验结果是显著的。那么有没有一种说法，认为$t=4.09$的结果比$t=2.46$更加好，也就是“更加显著”呢？这种说法显然是错误的，如果一个实验确实按照Neyman-Pearson的准则来设计，实际上无论实验是否实施，实验结果组成的集合的性质已经被确定，单次实验的结果只是这个集合中的一个元素，对实验结果进行统计推断是基于这个集合性质而非结果本身。

\subsubsection{两种检验体系的区别}

通过前面的论述，我们可以发现，两种统计推断体系其实建立在完全不同的观点上，有些地方甚至是相互矛盾的，我们可以这么指出他们之间的不同：

\begin{enumerate}
\item Fisher的P值衡量的是针对\textbf{某次}实验结果在\textbf{零假设成立前提}下，对零假设反驳的证据。Neyman体系是在控制实验一类错误和二类错误前提下，给出一个针对零假设和对立假设的判断，这个判断是建立在长期的实验上的。概括的说，Fisher的统计推断时针对\textbf{“某次”}实验证据强度的判断，而Neyman的方法更像一套体系，在这套体系下，\textbf{“全局”}的推断错误的概率得到控制。
\item Fisher的检验方法应该是\textbf{灵活的}，单次实验的P值不能说明问题，在做推断时必须考虑实验本身的特点和其他的信息。Neyman-Pearson的检验体系是\textbf{严格的}，实验在设计时已经考虑减少一类错误，控制二类错误，实验结果组成的集合在实验前就已确定，假设的接受和拒绝遵循二分对立的原则，只要接受域和拒绝域被计算出来，就应有一个明确的统计推断
\end{enumerate}

可以看到，首先在假设问题的提法上，Fisher其实根本没有提出“对立假设”的概念，更没有提出“拒绝零假设”的说法，因为P值是在零假设成立下计算的，用P值去拒绝零假设无异与犯了因果倒置的谬误。而Neyman的体系虽然提到了拒绝零假设的说法，但他也没有说某次实验的结果就能证明零假设是错误的，相反他认为根本不可能通过一次实验来验证一个假设，只能说在控制统计推断错误的前提下，如果实验结果出现了显著性结果，那么可以说“拒绝零假设”这个推断有很大可能是正确的。

然而，我们在很多时候误解了上面的两种体系的区别，一方面在使用$P$值时，我们过于依赖他，武断的认为很小的$P$值等价与显著性而忽略了实验结果可重复性的重要性（所谓的“因为$P<0.05$，所以我们认为实验结果是显著的”），另一方面，在使用Neyman体系时，我们又忘记了接受域和拒绝域的严格性，往往用“A比B更显著”的说法来下结论。这相当于用Neyman-Pearson的严格的二分哲学来解释具有灵活性的$P$值，而在用Neyman的体系时，我们又突然变的灵活起来，试图比较两个都落入拒绝域的结果那个更加好。

实际上，虽然Fisher与Neyman的观点有些地方是冲突的，但却有着共同点，他们都不认为自己的体系能回答“哪个假设正确”的问题，Fisher说P值只是一个反驳零假设的证据，并且还要依据实验的可重复性来下结论，Neyman则坚持在控制统计推断错误的前提下才能有信心说针对某个问题的论断在长期实验下的有效性。两者虽然出发点不同，但都认为**重复性**是解决科学问题的首要关注点。

\subsubsection{了解P值的性质}

我们总是希望观测到一个效应明显的实验结果，$P<0.05$往往被认为是效应明显，即显著性的一个分割点，正是这种观念的广泛传播导致了研究者往往在P值上动一些手脚，最常见的方式就是舍弃一些数据、改变数据的分布、变换数据来人为的产生一些足够小的P值，这样他们就能用P值来声明他们发现了足够显著的效应。这种操纵P值以或得显著性结果的行为称为"P hacking"

由于P值是通过一次实验结果计算而出的，其本身带有随机性质，可以把它当做一个随机变量来看，因此下面通过模拟的方法来研究P值的几个性质，正式这几个性质导致了“P hacking”现象的广泛传播。


*1. 只要样本数量足够，总能得到显著的结果*


在实践中，我们往往过分关注效应的显著性，而对效应本身却缺少关注，有些时候，确实能关注到一些效应是显著的，但如果效应本身很微弱以至于远远小于总体的方差，这种效应完全有可能被个体之间的变异度所掩盖。但在实验样本越大的趋势下，P值展现一个缩小的趋势，只要样本容量达到一定程度，P值就可以达到$P < 0.05$分界点，从而给出一个所谓“显著性”的结果。

\begin{lemma}
样本抽自$N(0.1, 1)$的总体，零假设为样本来自$N(0,1)$的总体，在样本容量分别为10，100，500，1000的情形下做数值模拟实验：从总体中抽样，进行$t$检验，重复这个过程1000次，最后得出每种样本容量下P值分布的频数直方图。
\end{lemma}
```{r p_hack1, eval=T, echo=F, message=F, warning=F, error=F, fig.height=3, fig.width=4}
set.seed(1234)
samcap <- c(10, 100, 500, 1000)
rep <- 1000
tmean <- 0.1
tsd <- 1
nmean <- 0

samp_list <- sapply(samcap, function(x){matrix(rnorm(x*rep, mean=tmean, sd=tsd), 
                                               ncol=x, byrow=T)})
p_list <- lapply(samp_list, function(l){apply(l, 1, function(x){t.test(x, mu=nmean, var.equal = T)$p.value})})
dat <- data.frame(a=p_list[[1]], b=p_list[[2]], c=p_list[[3]], d=p_list[[4]])
figa <- ggplot(data=dat) + geom_histogram(aes(a), bins=50, colour="blue") + geom_vline(xintercept = 0.05, colour="red")+xlab("Sample capacity=10") + ylab("") 
figb <- ggplot(data=dat) + geom_histogram(aes(b), bins=50, colour="blue") + xlab("Sample capacity=100") + ylab("") + geom_vline(xintercept = 0.05, colour="red")
figc <- ggplot(data=dat) + geom_histogram(aes(c), bins=50, colour="blue") + xlab("Sample capacity=500") + ylab("") + geom_vline(xintercept = 0.05, colour="red") 
figd <- ggplot(data=dat) + geom_histogram(aes(d), bins=50, colour="blue") + geom_vline(xintercept = 0.05, colour="red") + xlab("Sample capacity=1000") + ylab("")
grid.newpage()
pushViewport(viewport(layout=grid.layout(2, 2)))
print(figa, vp=vplayout(1, 1))
print(figb, vp=vplayout(1, 2))
print(figc, vp=vplayout(2, 1))
print(figd, vp=vplayout(2, 2))

sigpercent <- apply(dat, 2, function(x){sum(x < 0.05)/length(x)})
```

通过直方图来看，在样本容量为10时，P值的分布比较均匀，而当样本容量提高到100时，明显可以看到1000次实验中P值较小的结果明显升高，而当样本容量在1000时，P值分布明显左偏，1000次实验中，较小的P值占据了极大部分。

如果用我们常用的$P < 0.05$的显著性法则来统计这1000次实验哪些显著，哪些不显著，可以得到如下的结果：

|Sample capacity|10|100|500|1000|
|:------|:------|:------|:------|:------|
|Rate of significance|`r sigpercent[1]`|`r sigpercent[2]`|`r sigpercent[3]`|`r sigpercent[4]`|

```{r ratecap,  eval=T, echo=F, message=F, warning=F, error=F, fig.height=2, fig.width=4}
dat <- data.frame(x=samcap, y=sigpercent)
fig_ratecap <- ggplot(data=dat) + geom_line(aes(samcap, sigpercent), colour="blue") + xlab("Sample capacity") + ylab("Rate of significance")
fig_ratecap
```

通过上面的表格和折线图可以看到，随着样本容量的增大，得到显著性结果的可能性是明显增大的，哪怕要论证总体的效应值只有0.1，仅仅只是标准差的十分之1这么微小，只要保证能检测足够多的样本，一样有很大的把握给出显著性结果。

实际实验中，总体的效应值和方差往往是未知的，而我们往往倾向做足够大样本的实验。如果过分关注P值，这时确实很有可能得到一个显著性结果，但实际上，一个十分微小的显著性结果并不能说明科学问题，例如关注两种药物A和B的疗效，在做了大样本实验后，发现A和B之间疗效确有显著性不同，但差距十分微弱，那么这时做出谁哪种药物更优的论断是没有什么意义的。

*2. 某次小样本实验中，P值表现往往不稳定*

有些时候，对于一个巨大且个体差异较大的总体，单独的一次抽样实验出现显著性结果并不能给出总体也呈现显著性的推论。正如抽奖一样，有时确实会踩中一部分能得到显著性样本，但如果重复实验次数变多，会发现大多数时候结果是并不显著的，这和Fisher在对P值的悲观性看法是一致的，即P值从来不能回答零假设是否正确的问题。

有个实际已经发生的例子能够表明P值的这种陷阱。Matt Motyl是来自弗吉尼亚大学的一名心理学博士生，他发现了一个有趣的现象：那些有极端政治倾向（极左倾主义或极右倾主义）的人群相较于中立政治倾向的人群更不容易分辨颜色中的灰色。来自2000个人的样本数据支持这个观点，其P值仅仅只有0.01，甚至比0.05还要小。这种结果让他十分开心，因为这么强烈的证据很容易在一些具有高影响力的杂志上发表，但为了稳妥起见，他的导师建议他再重复一次实验，在重做实验后，他却发现，这次样本的P值大到了0.59，这么大的P值甚至连0.05的显著性都没有，这打破了他把发现发表到高影响力杂志的美梦，真是个悲剧的转折。

是Matt Motyl错了么，也不能这么说，他确实发现了这种现象，但为什么却又在重复实验后得到了令人沮丧的结果？可以说，是对P值的错误的理解造成了这种戏剧性的转折。2000个人的样本相对与人群总体不过是一个极小的样本，用一次实验的结果来对总体的特征来下结论，本来就是一个不严谨的科学推论。

同样的，我们用一个数值模拟实验来重现Matt遇到的情况，来看看在一个本身个体间差异巨大的总体下，单次小样本实验的P值会呈现一个怎样的特征。

\begin{lemma}
从总体分布为$N(1, 3)$的总体抽10个样本，零假设为：总体均值为0，重复进行这个实验，记下每次$t$检验时的P值
\end{lemma}

```{r p_var, eval=T, echo=F, message=F, warning=F, error=F, fig.height=3, fig.width=4}
set.seed(123)
n <- 1000
cap <- 10

p_varli <- sapply(rep(cap, n), function(x){
  t.test(rnorm(x, mean=1, sd=3), mu=0)$p.value
})

dat <- data.frame(x=c(1:n), p = p_varli)
fig_pline <- ggplot(data=dat[1:50, ]) + geom_line(aes(x=x, y=p), colour="blue") + geom_hline(yintercept = 0.05, colour="red") + xlab("Experiments") + ylab("P.value")
fig_phist <- ggplot(data=dat) + geom_histogram(aes(p), colour="blue") + geom_vline(xintercept = 0.05, colour="red") + xlab("P.value") + ylab("Counts")

grid.newpage()
pushViewport(viewport(layout=grid.layout(1, 2)))
print(fig_pline, vp=vplayout(1, 1))
print(fig_phist, vp=vplayout(1, 2))
```

|Inference|Significance|Non-significance|
|:------|:------|:------|
|Counts|`r sum(p_varli < 0.05)`|`r sum(p_varli >= 0.05)`|
|Frequency|`r sum(p_varli < 0.05)/length(p_varli)`|`r sum(p_varli >= 0.05)/length(p_varli)`|

在零假设为假的情况下，进行了1000次模拟实验，通过上面的图像可以看出，P值其实在$[0, 1]$范围内波动很大，而右图的频数直方图显示虽然P值的分布是左偏的，但得到显著性结果：即总体均值不是0的频数很低。下面的表格进一步表明，在1000次实验中，只有157次实验得到了显著性结果，其他843次结果均不显著

所以，单次实验并不能说明问题，尤其在研究的总体方差比均值大的情况下，由于均值的差异常常被方差掩盖，得到非显著结果的可能是大大增大的，通过多次的重复可以更好的下结论。

*3. P-curve: P值的分布曲线*

P-curve，是近年来提出的一种用于检查文献是否经过P-hacking的方法，它是一条描述P值分布的曲线，在零假设为真时，P-curve应当显示为一条水平直线，即为均匀分布，当零假设为假时，P-curve出现明显的左偏，即从P值从0到1的方向看，概率密度呈现从高到低的态势。

通过meta分析收集大量P值，做出P-curve，然后与理论上的P-curve进行比较能够看出总体上是否出现了P-hacking，本文不表述如何做这个检测，只是用数值模拟来展现两种类型的P-curv的形状。

\begin{lemma}
从分布为$N(1, 1)$的总体中抽出一份容量为10的样本，对下面两个零假设进行$t$检验：
\begin{enumerate}
\item 零假设为真：样本来自分布为$N(1, 1)$的总体
\item 零假设为假：样本来自分布为$N(0, 1)$的总体
\end{enumerate}
重复做这个实验1000次，然后做出两种情况下的P值分布的概率密度曲线
\end{lemma}

```{r p_curve, eval=T, echo=F, message=F, warning=F, error=F, fig.height=3, fig.width=4}
cap <- 10
n <- 10000

p_curvent <- sapply(rep(cap, n), function(x){t.test(rnorm(x, mean=1, sd=1), mu = 1, var.equal = T)$p.value})
p_curvenf <- sapply(rep(cap, n), function(x){t.test(rnorm(x, mean=1, sd=1), mu = 0, var.equal = T)$p.value})
dat <- data.frame(nt = p_curvent, nf = p_curvenf)

fig_nt <- ggplot(dat) + geom_density(aes(nt), bw=0.03, colour="blue", size=1) + xlab("True NULL:P-value") + geom_vline(xintercept = 0.05, colour="red")
fig_nf <- ggplot(dat) + geom_density(aes(nf), bw=0.05, colour="blue", size=1) + ylab("") + xlab("False NULL:P-value") + geom_vline(xintercept = 0.05, colour="red")

grid.newpage()
pushViewport(viewport(layout = grid.layout(1, 2)))
print(fig_nt, vp=vplayout(1, 1))
print(fig_nf, vp=vplayout(1, 2))
```

上面左图显示了零假设为真时的P-curve，可以看到在各个P值上的概率密度都大致相等，由于数值模拟的随机性存在一些波动，可以认为当零假设为真时的P值分布为均匀分布，各种P值出现的可能性是均等的。右图显示了零假设为假时的P-curve，这条曲线在P值小的区域有很大的概率密度，说明越小的P值越有可能出现。

如果对一份出现了P-hacking现象的杂志进行统计，P-curve极有可能在$P = 0.05$附近有异常的波动，出现递增的现象，这表明有论文在数据分析中刻意提高了$P <0.05$出现的概率。

```{r p_curvehack, eval=T, echo=F, message=F, warning=F, error=F, fig.height=3, fig.width=4}
x_normal <- seq(0, 0.06, 0.001)
y_normal <- 1 / (x_normal*10)
y_hack <- c(y_normal[x_normal<=0.01], (1/y_normal[x_normal>0.01])*90)

dat <- data.frame(x = x_normal, y_normal = y_normal, y_hack=y_hack)
fig_hack <- ggplot(data=dat) + geom_line(aes(x=x, y=y_hack), colour="red", size=1) + geom_line(aes(x=x, y=y_normal), colour="blue", size=1) + xlab("P.value") + ylab("Frequency")
fig_hack
```

上图就显示了P-hacking发生时P-curve曲线的情况，红色区域出现递增，明显与蓝色理论曲线递减趋势不同，这时就表明在0.05附近发生了P-hacking现象。

\subsubsection{总结}

本文将对P值的认识总结成如下几条，如下：
\begin{enumerate}
\item P值可以用来描述数据与一个特定统计模型（通常是零假设）不相容程度，通常越小的P值说明数据与统计模型不相容程度越高
\item P值不是特定统计模型为真的度量，它本身是对数据与特定模型相容性的度量而非这个统计模型本身
\item 在P值上应用简单的$P<0.05$即显著是错误的，更不能说在这个条件下，特定的统计模型就为假。在对\textbf{假设做统计推断时，需要综合考虑其他因素}
\item P值不能用来描述效应的大小或实验结果的重要性，正如前面的模拟，无论效应多么微小，在保证实验精度和样本容量足够大时，总能给出很小的P值，相反，对于那些实际存在的明显的效应，糟糕的实验精度和小的样本容量同样也能给出一个很大的P值
\item 不应该对P值进行人为选择或操纵，P-hacking应该杜绝
\item 统计推断不应该局限于考虑P值，更多的因素应该被考虑，诸如重复性原则，实验的设计，历史数据等。
\end{enumerate}

除了P值，还有许多可以帮助我们做出统计推断的方法，诸如计算置信区间，可信度或预测区间以及贝叶斯方法。特别是使用贝叶斯方法，能够更好的回答在产生这样的数据的前提下，某个特定的模型是否可靠的问题。即考虑如下问题$\textrm{P}(H|data)$。

\subsubsection{参考文献}

本文的观点和论据来自如下几篇著作和文献：

1. Fisher, R. A. (1954). Statistical methods for research workers, 5th ed. , 21(82), 340-341.

2. Fisher, R. A. (1936). The design of experiments. Social Service Review, 57(1), 183–189.

3. Lew, M. J. (2012). Bad statistical practice in pharmacology (and other basic biomedical disciplines): you probably don't know p. British Journal of Pharmacology, 166(5), 1559-1567.

4. Wasserstein, R. L., & Lazar, N. A. (2016). The asa''s statement on p-values: context, process, and purpose. American Statistician, 70(2), 129-133.

5. Nuzzo, R. (2014). Scientific method: statistical errors. Nature, 506(7487), 150-2.

6. Head, M. L., Holman, L., Lanfear, R., Kahn, A. T., & Jennions, M. D. (2015). The extent and consequences of p-hacking in science. Plos Biology, 13(3), e1002106.