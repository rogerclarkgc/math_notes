---
title: "统计学习读书笔记"
author:
  - Rogerclark
documentclass: ctexart
output:
  rticles::ctex:
    fig_caption: yes
    number_sections: yes
    toc: yes
    toc_depth: 3
classoption: "hyperref,"
---

\newtheorem{Definition}{\hspace{2em}定义}
\newtheorem{theorem}{\hspace{2em}定理}
\newtheorem{lemma}{\hspace{2em}例子}
\newtheorem{Proof}{证明}

# 预备知识

## 范数简介

用实用的方式简单介绍范数，不涉及定义的详细探讨，范数可以认为是对距离概念的一种延拓，统计学习中使用范数，常常是用来度量一个向量在一个超维空间中的大小（类似于长度的概念）

### L-P 范数

L-P范数有着如下的定义：
\begin{equation}
L_p = \Vert \textbf{x} \Vert_p = \sqrt[p]{\sum_{i=1}^n x_i^p}, \quad \textbf{x}=(x_1, x_2, \cdots, x_n) 
\end{equation}

当$p=1$或$p=2$，此时称为$L_1$范数或者$L_2$范数，这是在统计学习中很常用的范数，它们的定义分别如下：

\begin{equation}
\begin{split}
L_1 &= \Vert \textbf{x} \Vert_1 = \sum_{i=1}^n |x_i|\\
L_2 &= \Vert \textbf{x} \Vert_2 = \sqrt{\sum_{i=1}^n x_i^2}
\end{split}
\end{equation}

其中$L_1$范数是$\textbf{x}$向量中元素的绝对值之和，常用的距离描述Manhattan距离就是一种$L_1$范数，$L_2$范数是最常用的范数之一，欧氏距离就是一种$L_2$范数

# 感知机

感知机是二分类的线性模型，学习感知机模型的目的在于找到一个超平面，尽可能的将训练数据分成两类。

## 感知机模型

\begin{Definition}
（感知机）输入空间（特征空间）满足$\mathcal{X}\subseteq \mathbb{R}^n$，输出空间为$\mathcal{Y}=\{+1, -1\}$，输入$x\in\mathcal{X}$表示一个实例的特征向量，对于输入空间的点；输出$y\in\mathcal{Y}$表示实例的类别，由输入空间到输出空间的如下函数：
\begin{equation}
f(x) = \textmd{sign}(w\cdot x + b)
\end{equation}
称为感知机，其中，$w$和$b$是感知机模型的参数，$w \in\mathbb{R}^n$叫权重向量，$b \in \mathbb{R}$叫偏置（bias），$w\cdot x$是两个向量的内积。sign是符号函数，它的定义是：
\begin{equation}
\textmd{sign}(x)=\left\{
\begin{aligned}
&+1, \quad x \geq 0\\
&-1, \quad x < 0
\end{aligned}
\right.
\end{equation}
\end{Definition}

感知机模型是一个线性的分类器，当$w\cdot x + b=0$时，对应了特征空间$\mathbb{R}^n$中的一个超平面$S$，这个超平面将特征空间分成了正负两类。因此超平面$S$称为分离超平面。

## 感知机学习策略

根据感知机的定义，可以给出损失函数的定义，一个直观的选择是用误分类点的总数来表示，但这样定义的函数非连续可导，不便于求解。另一个选择则是用*误分类点到超平面的总距离*来定义。首先，给出输入空间$\mathbb{R}^n$中任一点$x_0$到超平面$S$的距离：

\begin{equation}
\frac{1}{\Vert w \Vert} |w\cdot x_0 + b|
\end{equation}

对误分类点$x_i$，到超平面的距离写为：

\begin{equation}
\label{distance_mislabel}
-\frac{1}{\Vert w \Vert}y_i (w \cdot x_i + b)
\end{equation}

对于所有误分类点集合$M$，那么可以写出所有误分类点到超平面$S$的总距离：
\begin{equation}
\label{distance_mislabel_total}
-\frac{1}{\Vert w \Vert} \sum_{x_i \in M} y_i (w \cdot x_i + b)
\end{equation}

根据上式\eqref{distance_mislabel_total}的定义，感知机的损失函数可以定义为：
\begin{Definition}
\label{perceptron_loss_def}
对给定数据集
\begin{displaymath}
T = {(x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)}
\end{displaymath}
其中，$x_i \in \mathcal{X} = \mathbb{R}^n$，$y_in \in \mathcal{Y}=\{+1, -1\}$，$i=1, 2, \cdots, N$，感知机$\textmd{sign}(w\cdot x +b)$学习的损失函数为：
\begin{equation}
\label{perceptron_loss_function}
L(w, b) = - \sum_{x_i \in M} y_i (w \cdot x_i + b)
\end{equation}
\end{Definition}

这个函数连续可导，方便后续的学习算法进行优化。

## 感知机学习算法

对感知机学习的过程就是确定感知机模型$w$和$b$，使得损失函数$L(w, b)$尽量小的过程，这个问题可以转化成在训练数据集中，求解$L(w, b)$极小值点的最优化问题。常见的方法是*随机梯度下降法*

### 原始形式

输入：训练集$T=\{(x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)\}$，其中$x_i \in \mathcal{X} = \mathbb{R}^n$，$y_i \in \mathcal{Y}=\{-1, +1\}$，$i=1,2, \cdots, N$；学习率$\eta(0<\eta \leq1)$；
输出：$w$, $b$；感知机$f(x)=\text{sign}(w\cdot x + b)$。
\begin{enumerate}
\item 选取初值$w_0$, $b_0$
\item 选取训练集中一对$(x_i, y_i)$
\item 如果误分类，即有$y_i(w\cdot x_i + b) \leq 0$，则让参数分别加一单位学习速率下的梯度
\begin{displaymath}
\begin{split}
&w \leftarrow w + \eta y_i x_i \\
&b \leftarrow b + \eta y_i
\end{split}
\end{displaymath}
\item 回到第2步，只到所有训练数据被正确分类
\end{enumerate}

### 对偶形式

感知机的对偶形式是原始形式的另一种记法，它将$w$和$b$用$x_i$和$y_i$的线性组合来表示，这时感知机模型变成：

\begin{equation}
\label{perceptron_def_an}
f(x) = sign(\sum_{j=1}^N \alpha_j y_j x + b)
\end{equation}

相应的算法中的误分条件也变成，如果$y_i (\sum_{j=1}^N \alpha_j y_j x_j \cdot x_i +b) \leq 0$，则对$\alpha_i$和$b_i$进行更新:
\begin{displaymath}
\begin{split}
&\alpha_i \leftarrow \alpha_i + \eta\\
&b \leftarrow b + \eta y_i
\end{split}
\end{displaymath}

对偶形式的好处在于，式中$x_j\cdot x_i$的形式能预先运算并存在一个矩阵里，这个矩阵就是所谓Gram矩阵:$G=[x_i \ \cdot x_j]_{M\times N}$，以后每次对样本点进行误分条件判断时，只要按照顺序查找这个矩阵中的数字。

## 补充资料

空间中任一点到超平面$S$的距离推导过程，见网页
[链接](https://blog.csdn.net/yutao03081/article/details/76652943)

# $k$近邻算法

$k$近邻算法（k-nearest neighbor, k-NN）是一种分类算法，它十分简单直接，但不像感知机算法，它没有类似梯度下降算法逐渐逼近所求参数的过程，它更像一种直接利用训练样本来对特征向量进行分类的方法，训练样本数据本身就是模型的一部分。

## $k$近邻算法

$k$近邻算法简单说就是给出一个未分类实例，在已分好类的训练集中找出$k$个与该实例的最近的已分类实例，然后再看这$k$个实例中哪个类最多，这个未分类实例就属于这个类。

\begin{Definition}
\label{k-NN-method}
（$k$近邻法）输入：训练数据集
\begin{displaymath}
T=\{(x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)\}
\end{displaymath}
其中， $x_i \in \mathcal{X}\subseteq \mathbb{R}^n$为实例的特征向量，$y_i \in = \{c_1, c_2, \cdots, c_K\}$为实例的类别，$i=1, 2, \cdots, N$为训练数据集大小；未分类实例特征向量记为$x$
\par 输出：实例$x$所属类别$y$
\begin{enumerate}
\item 根据给定距离度量，在训练集$T$中找与$x$最邻近的$k$个点，涵盖这$k$个点的$x$的邻域记为$N_k(x)$;
\item 在$N_k(x)$中根据分类决策规则（如多数表决）决定$x$的类别$y$：
\begin{displaymath}
y = \arg \max_{c_j} \sum_{x_i \in N_k(x)} I(y_i = c_j), i=1, 2, \cdots, N; j = 1, 2, \cdots,K
\end{displaymath}
其中，$I$是指示函数，当$y_i=c_j$时为1，否则为0
\end{enumerate}
\end{Definition}

当$k=1$，未分类实例$x$只将距离其最近的一个实例的类别作为自己的类，这时的算法称为*最邻近算法*

## $k$近邻模型
$k$近邻模型的三个基本要素：距离度量、$k$值、分类决策

### 距离度量

两个实例点间距离是两个实例点间相似程度的反应，常用的距离度量有欧氏距离、$L_p$距离和Minkowsik距离。

设特征空间$\mathcal{X}$是$n$纬实数向量空间$\mathbb{R}^n$，$x_i, x_j \in \mathcal{X}$，$x_i = (x_i^{(1)},x_i^{(2)}, \cdots, x_i^{(n)})^T$，$x_j = (x_j^{(1)},x_j^{(2)}, \cdots, x_j^{(n)})^T$，则$x_i$，$x_j$的$L_p$距离定义为

\begin{equation}
\label{lp_distance_df}
L_p(x_i, x_j) = (\sum_{l=1}^n |x_i^{(l)} - x_j^{(l)}|^p)^{\frac{1}{p}}
\end{equation}

这里$p \geq 1$，当$p=2$，就是常见的欧氏距离；当$p=1$，称为曼哈顿距离（Manhattan distance）。当$p = \infty$时，它是各个坐标距离的最大值，即：
\begin{equation}
L_{\infty}(x_i, x_j) = \max_{l}|x_i^{l} - x_j^{(l)}|
\end{equation}

### $k$值的选择

选择较小的$k$值，相当于用较小的邻域中的训练实例进行预测，这样只有与输入实例较近的训练实例才会对预测结果起作用。但这样的负面影响就是容易发生"过拟合"，学习的估计误差增大，模型对近邻点的变化十分敏感，这样常常会导致模型过于复杂。

选择较大的$k$值，相当于用较大的领域中的训练实例进行预测，这样学习的估计误差较小。但负面影响是学习的近似误差加大，因为距离输入实例较远的点，即不太相似的点也对预测起了作用。$k$值的增大意味着模型变得简单

极端的，让$k=N$，即训练样本全体数量，这样无论输入什么实例，给出的预测结果总是训练实例中占据数量最多的类。这种模型失去了大量信息，过于简单。

一般采取较小的$k$值，用交叉验证法来不断优化。

### 分类决策规则

往往使用多数表决作为分类决策规则，即由输入实例领近的$k$个训练实例中的占据数量多的类来决定输入实例的类。对实例$x\in\mathcal{X}$，其最近领的$k$个训练实例的点构成集合$N_k(x)$，如果$N_k(x)$的类别是$c_j$，这种方法的误分类率是
\begin{equation}
\frac{1}{k} \sum_{x_i \in N_k(x)}I(y_i \neq c_j) = 1 - \frac{1}{k}\sum_{x_i \ \in N_k(x)}I(y_i = c_j)
\end{equation}
要让误分类率最小即经验风险最小，这种多数表决规则等价于经验风险最小化决策

## $k$近邻法的实现：$kd$树

实现$k$近邻法难点在高效的搜索输入实例与周边近邻点，当数据量大时，采用简单的线性扫描效率较低，因此在搜索前，对全体训练数据构建$kd$树存储，能有效减少计算距离的次数。

### 构造$kd$树

$kd$树属于二叉树，表示对$k$维空间的一个划分，构造这个树的过程就是不断的用垂直于坐标轴的超平面将$k$维空间进行划分，最后让子空间已无法继续划分为止。通常用各个维度的坐标轴的中位数点作为切分点，这样得到的是平衡的$kd$树，虽然这种树的搜索效率未必是最优的。

下面是构造$kd$树的算法

输入：$k$维空间数据集合$T={x_1, x_2, \cdots, x_ N}$，其中$x_i = (x_i^{(1)}, x_i^{(2)}, \cdots, x_i^{(k)})^T$，$i=1, 2, \cdots, N$;输出：$kd$树

\begin{enumerate}
\item 从$x^{(1)}$开始划分，以训练实例中$x^{(1)}$的坐标的中位数为第一次切分点，这个点对应的超矩形将空间分成两个子区域，这时产生两个深度为1的左右子节点，将坐标$x^{(1)}$小于这个中位数的点放置在左子节点，将坐标$x^{(1)}$大于中位数的点放置在右子节点。最后将落入切分超平面的实例点保存成根节点
\item 重复：对于深度为$j$的节点，选择$x^{(l)}$作为切分坐标轴的依据为$l=(j \mod k) + 1$，同样，以$x^{(l)}$坐标轴上的中位数为切分点，重复（1）中的步骤，产生深度为$j+1$的两个左右子节点
\item 一直划分，知道两个子节点没有实例时停止，$kd$树完全生成
\end{enumerate}

### 搜索$kd$树

\begin{itemize}
\item 输入：已构造的$kd$树；目标点$x$
\item 输出：$x$的最近邻
\end{itemize}

\begin{enumerate}
\item 找到包含目标点$x$的叶节点：从根出发，递归访问树，若$x$当前维的坐标小于切分点，则移动到左子节点，反之移到右子节点，直到子节点为叶节点
\item 以当前的叶节点为“当前最近点”
\item 递归向上回退，在每个节点上进行如下操作
\begin{enumerate}
\item 如果该节点比当前最近点距离目标点更近，则以这个节点为“当前最近点”
\item 当前最近点一定存在与该节点一个子节点对应区域，检查该子节点的另一个子节点对应区域是否有更近的点，即检查另一子节点对应区域是否与目标点为球心，半径为目标点与“当前距离”距离的超球体相交。若相交，可能在另一个子节点对应区域存在距离目标点更近的点，移动到另一个子节点上接着递归进行最邻近搜索。如果不相交，继续回退。
\end{enumerate}
\item 当回到根节点时，搜索结束，最后保存的“当前最近点”就是$x$的最近邻点
\end{enumerate}

# 朴素贝叶斯法

朴素贝叶斯发基于特征条件独立假设学习输入/输出的联合概率分布；然后基于此模型，对于给定的输入$x$，利用贝叶斯定理求出后验概率最大的输出$y$。

## 朴素贝叶斯法的学习与分类

### 基本方法

设输入空间$\mathcal{X} \subseteq \mathbb{R}^n$为$n$维向量集合，输出空间为类标记集合$\mathcal{Y}=\{c_1, c_2, \cdots, c_K\}$。输入为特征向量$x \in \mathcal{X}$，输出为类标记$y \in \mathcal{Y}$。$X$是定义在输入空间$\mathcal{X}$的随机向量，$Y$是定义在输出空间上的随机变量。$P(X,Y)$是$X$和$Y$的联合概率分布。训练数据为$T=\{(x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)\}$由联合概率分布$P(X, Y)$独立同分布产生。

根据贝叶斯公式，后验概率$P(Y=c_k | X=x)$满足：

\begin{equation}
\label{naive_bayesian_formual}
P(Y=c_k |X=x)= \frac{P(X=x|Y=c_k) P(Y=c_k)}{\sum_k P(X=x|Y=c_k)P(Y=c_k)}
\end{equation}

这个方法的就是要让这个后验概率最大，取能让P(Y=c_k|X=x)最大的$c_k$类作为输入样本$x$的分类结果

由于条件概率$P(X=x|Y=c_k)$中$X$为多维向量，要估计这个概率参数过多，为了简化，朴素贝叶斯对此概率分布作*条件独立性假设*，即：
\begin{equation}
\label{naive_bayesian_assumption}
\begin{split}
P(X=x|Y=c_k) = P(X^{(1)} &= x^{(1)}, \cdots, X^{(n)} = x^{(n)}|Y=c_k)\\ 
&= \prod_{j=1}^{n} P(X^{(j)}=x^{(j)}|Y=c_k)
\end{split}
\end{equation}

这个条件独立假设假定了输入$x$的各个随机分量$x^{(j)}$是独立的，这样计算条件概率时的组合就少了很多。这样式子\ref{naive_bayesian_formual}就变成：

\begin{equation}
P(Y=c_k|X=x) = \frac{P(Y=c_k)\prod_{j} P(X^{(j)}=x^{(j)}|Y=c_k)}{\sum_k P(Y=c_k) \prod_j P(X^{(j)} = x^{(j)}|Y=c_k)}
\end{equation}

显然，上式中分母对所有类$c_k$相同，要得到后验概率最大化需要让分子最大，于是给出朴素贝叶斯分类器：
\begin{equation}
\label{naive_bayesian_classifier}
y=\arg\max_{c_k} P(Y=c_k) \prod_j P(X^{(j)}=x^{(j)}|Y=c_k)
\end{equation}

## 朴素贝叶斯法的参数估计

### 极大似然估计

要学习模型\ref{naive_bayesian_classifier}，意味着需要估计$P(Y=c_k)$和$P(X^{(j)}=x^{(j)}|Y=c_k)$。

先验概率$P(Y=c_k)$的极大似然估计是
\begin{equation}
P(Y=c_k) = \frac{\sum_{i=1}^N I(y_i=c_k)}{N}, \quad k =1, 2, \cdots, K
\end{equation}

设第$j$个特征$x^{(j)}$的取值集合是$\{a_{j1}, a_{j2}, \cdots, a_{jS_j}\}$，则条件概率$P(X^{(j)}=a_{jl}|Y=c_k)$的极大似然估计是

\begin{equation}
\begin{split}
&P(X^{(j)}=a_{jl}|Y=c_k) = \frac{\sum_{i=1}^N I(x_i^{(j)} = a_{jl}, y_i=c_k)}{\sum_{i=1}^N I(y_i = c_k)}\\
&j=1, 2, \cdots, n;\m l=1, 2, \cdots,S_j;\m k = 1,2,\cdots, K
\end{split}
\end{equation}